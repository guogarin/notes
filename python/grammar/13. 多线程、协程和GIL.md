- [一、并发(Concurrency)和并行(Parallelism)](#一并发concurrency和并行parallelism)
  - [1. 并发和并行有何区别？](#1-并发和并行有何区别)
  - [2. 执行单元(Execution unit)](#2-执行单元execution-unit)
  - [2. python的并发模型](#2-python的并发模型)
    - [2.1 Python有哪些并发模型？](#21-python有哪些并发模型)
    - [2.2 这几个并发模型各有何优点？它们各自适合什么样的应用场景？](#22-这几个并发模型各有何优点它们各自适合什么样的应用场景)
- [二、 GIL锁](#二-gil锁)
  - [1 什么是GIL锁？](#1-什么是gil锁)
  - [2 GIL锁 是Python带来的吗？](#2-gil锁-是python带来的吗)
  - [3 为什么需要GIL？](#3-为什么需要gil)
    - [3.1 GIL 为Python解决了什么问题？](#31-gil-为python解决了什么问题)
    - [3.2 为什么选择 全局锁？而不是给每个对象的引用计数单独枷锁？](#32-为什么选择-全局锁而不是给每个对象的引用计数单独枷锁)
    - [3.3 其它原因](#33-其它原因)
    - [3.4 总结](#34-总结)
  - [4 GIL 是如何运作的？](#4-gil-是如何运作的)
    - [4.1 GIL在什么时候会被释放？](#41-gil在什么时候会被释放)
    - [4.2 python如何确保GIL不会一直被一个线程持有不释放？](#42-python如何确保gil不会一直被一个线程持有不释放)
  - [5 为什么不在 Python 3 中删除它？](#5-为什么不在-python-3-中删除它)
  - [6 GIL锁的影响](#6-gil锁的影响)
    - [6.1 GIL会带来什么影响？](#61-gil会带来什么影响)
    - [6.2 多线程编程中，如何最大程度的避免GIL锁带来的影响？](#62-多线程编程中如何最大程度的避免gil锁带来的影响)
  - [7 GIL是否意味着线程安全？](#7-gil是否意味着线程安全)
  - [8 GIL锁的存在是否意味着python多线程一点用也没有了？](#8-gil锁的存在是否意味着python多线程一点用也没有了)
  - [9. 如何通俗的解释GIL？](#9-如何通俗的解释gil)
  - [参考文献](#参考文献)
- [三、 多线程](#三-多线程)
  - [1. 解释器的线程切换间隔是多少？如何获取、修改系统切换线程切换间隔？](#1-解释器的线程切换间隔是多少如何获取修改系统切换线程切换间隔)
  - [2. `thread`和`threading`](#2-thread和threading)
  - [3. `threading`](#3-threading)
    - [3.1 `threading` 和 `multiprocessing`](#31-threading-和-multiprocessing)
    - [3.2 事件对象`Event`](#32-事件对象event)
      - [3.2.1 `Event`的作用](#321-event的作用)
      - [3.2.2 `Event`提供的方法](#322-event提供的方法)
      - [3.2.3 `Event`的使用实例](#323-event的使用实例)
- [四 多进程](#四-多进程)
- [五 `concurrent.futures`](#五-concurrentfutures)
  - [1. `concurrent.futures`的有何用处和优势？](#1-concurrentfutures的有何用处和优势)
  - [2. `concurrent.futures`提供的工具](#2-concurrentfutures提供的工具)
    - [2.1 `concurrent.futures`主要包含哪些实用的工具？](#21-concurrentfutures主要包含哪些实用的工具)
      - [(1) 执行器类](#1-执行器类)
    - [2.2 `concurrent.futures`如何确定进程池(线程池)的个数？](#22-concurrentfutures如何确定进程池线程池的个数)
      - [2.2.1 显示指定池子的大小](#221-显示指定池子的大小)
      - [2.2.2 若不指定池子大小，那池子的大小由什么决定？](#222-若不指定池子大小那池子的大小由什么决定)
        - [(1)  `ThreadPoolExecutor`](#1--threadpoolexecutor)
        - [(2) `ProcessPoolExecutor`](#2-processpoolexecutor)
  - [3. 如何使用`concurrent.futures`执行并发(并行)操作？](#3-如何使用concurrentfutures执行并发并行操作)
    - [3.1 `submit(fn, /, *args, **kwargs)`](#31-submitfn--args-kwargs)
    - [3.2 `map(func, *iterables, timeout=None, chunksize=1)`](#32-mapfunc-iterables-timeoutnone-chunksize1)
    - [3.3 `shutdown(wait=True, *, cancel_futures=False)`](#33-shutdownwaittrue--cancel_futuresfalse)
    - [3.5 `Executor`和上下文管理器](#35-executor和上下文管理器)
    - [3.6 应该使用 `ThreadPoolExecutor`还是`ProcessPoolExecutor`？](#36-应该使用-threadpoolexecutor还是processpoolexecutor)
    - [3.7 `Executor.map(func, *iterables, timeout=None, chunksize=1)`](#37-executormapfunc-iterables-timeoutnone-chunksize1)
      - [3.7.1 `Executor.map()`是阻塞的吗？](#371-executormap是阻塞的吗)
      - [3.7.2 `Executor.map()`的返回值有何特点？](#372-executormap的返回值有何特点)
      - [3.7.3 如果`map()`执行的某个函数发生了异常，会发生什么？](#373-如果map执行的某个函数发生了异常会发生什么)
    - [3.8 使用示例](#38-使用示例)
  - [4 `concurrent.futures.Future`类](#4-concurrentfuturesfuture类)
    - [4.1 它的作用是？它和`asyncio.Future`有何联系？](#41-它的作用是它和asynciofuture有何联系)
    - [4.2 如何使用它？使用时应注意什么？](#42-如何使用它使用时应注意什么)
      - [4.2.1 如何使用？](#421-如何使用)
      - [4.2.2 使用时应注意什么？](#422-使用时应注意什么)
  - [5. `concurrent.futures`的 模块函数(Module Functions)](#5-concurrentfutures的-模块函数module-functions)
    - [5.1 `wait()`](#51-wait)
    - [5.2 `as_completed()`](#52-as_completed)
      - [5.2.1 `as_completed()`的优点是？](#521-as_completed的优点是)
  - [Fluent Python第二版中的国旗下载脚本](#fluent-python第二版中的国旗下载脚本)
    - [顺序下载脚本](#顺序下载脚本)
    - [用`concurrent.futures`重写](#用concurrentfutures重写)
    - [用协程重构](#用协程重构)
- [六、协程(Coroutines)](#六协程coroutines)
  - [1. 什么是协程？](#1-什么是协程)
  - [2. 协程和线程](#2-协程和线程)
    - [2.1 协程是多线程吗？](#21-协程是多线程吗)
    - [2.2 协程和线程的对比](#22-协程和线程的对比)
    - [2.3 协程的优势和应用场景](#23-协程的优势和应用场景)
  - [3. 协程的发展](#3-协程的发展)
    - [3.1 Python和协程相关的关键字和方法的引入过程](#31-python和协程相关的关键字和方法的引入过程)
    - [3.2 几种协程的介绍](#32-几种协程的介绍)
    - [3.3 应该使用哪种协程呢？](#33-应该使用哪种协程呢)
  - [4 原生协程(native coroutines) 介绍](#4-原生协程native-coroutines-介绍)
    - [4.1 什么是原生协程？什么是 非原生协程？](#41-什么是原生协程什么是-非原生协程)
    - [4.2 需要先了解的几个概念](#42-需要先了解的几个概念)
  - [5 原生协程的定义和使用](#5-原生协程的定义和使用)
    - [5.3.1 定义原生协程](#531-定义原生协程)
    - [5.3.2 使用原生协程](#532-使用原生协程)
      - [(1) 和函数不一样的是，简单地调用一个协程并不会使其被调度执行](#1-和函数不一样的是简单地调用一个协程并不会使其被调度执行)
      - [(2) 调用协程的几种方式](#2-调用协程的几种方式)
      - [(3) 获取协程的返回值](#3-获取协程的返回值)
      - [(4) `await`关键字的使用有何限制？](#4-await关键字的使用有何限制)
    - [5.3.3 理解协程的关键点](#533-理解协程的关键点)
    - [5.3.4 一个协程的例子](#534-一个协程的例子)
      - [(1) 源码](#1-源码)
      - [(2) 若把上面`slow()`中的`await asyncio.sleep(3)`换成`time.sleep(3)`，会发生什么？为什么？](#2-若把上面slow中的await-asynciosleep3换成timesleep3会发生什么为什么)
        - [① 会发生什么？](#-会发生什么)
        - [② 为什么会这样？](#-为什么会这样)
      - [(3)](#3)
      - [(4)](#4)
      - [(5)](#5)
  - [6 可等待对象(Awaitables)](#6-可等待对象awaitables)
    - [6.1 什么样的对象是 可等待对象？](#61-什么样的对象是-可等待对象)
    - [6.2 有哪些可等待对象？](#62-有哪些可等待对象)
  - [任务对象(task)](#任务对象task)
    - [TASK何时可以被取消？](#task何时可以被取消)
  - [等待原语(Waiting Primitives)](#等待原语waiting-primitives)
    - [`asyncio.as_completed()`](#asyncioas_completed)
      - [(1) 函数介绍](#1-函数介绍)
      - [(2) `asyncio.as_completed()`的优势](#2-asyncioas_completed的优势)
      - [(3) 实例](#3-实例)
  - [`asyncio.Futures`](#asynciofutures)
  - [事件循环(Event Loop)](#事件循环event-loop)
  - [`asyncio.sleep` 和 `time.sleep`](#asynciosleep-和-timesleep)
    - [`asyncio.sleep` 和 `time.sleep` 有何不同](#asynciosleep-和-timesleep-有何不同)
    - [应该使用哪个？](#应该使用哪个)
    - [`asyncio.sleep(0)`](#asynciosleep0)
  - [asyncio 事件循环(asyncio event loop)](#asyncio-事件循环asyncio-event-loop)
    - [5.4 任务对象`task`](#54-任务对象task)
    - [5.4 绑定回调函数](#54-绑定回调函数)
    - [5.5 多任务](#55-多任务)
    - [5.7 `yield from` 转 `async/await`](#57-yield-from-转-asyncawait)
  - [6 原生协程 和  基于生成器的协程](#6-原生协程-和--基于生成器的协程)
  - [7 `asyncio`](#7-asyncio)
  - [实现协程的几种方式](#实现协程的几种方式)
  - [4 python的经典协程](#4-python的经典协程)
    - [4.1 用作协程的生成器的基本行为](#41-用作协程的生成器的基本行为)
    - [4.2 协程的四种状态](#42-协程的四种状态)
    - [4.3 预激协程(Coroutine Priming)](#43-预激协程coroutine-priming)
      - [4.3.1 预激协程](#431-预激协程)
      - [4.3.2 用装饰器来预激协程](#432-用装饰器来预激协程)
    - [4.5 图解运行流程](#45-图解运行流程)
    - [4.6 终止协程和异常处理(Coroutine Termination and Exception Handling)](#46-终止协程和异常处理coroutine-termination-and-exception-handling)
      - [4.6.1 若不处理协程内部发生的异常，会发生什么？](#461-若不处理协程内部发生的异常会发生什么)
      - [4.6.2 `throw()` 和 `close()`](#462-throw-和-close)
        - [(1)  `throw()` 和 `close()`简介](#1--throw-和-close简介)
        - [(2) `throw()` 和 `close()`的使用示例](#2-throw-和-close的使用示例)
    - [4.7 从协程返回值(Returning a Value from a Coroutine)](#47-从协程返回值returning-a-value-from-a-coroutine)
      - [4.7.1 如何从协程返回值](#471-如何从协程返回值)
      - [4.7.2 如何获取协程返回的值？](#472-如何获取协程返回的值)
    - [4.8 在协程中使用`yield from`](#48-在协程中使用yield-from)
      - [4.8.1 `yield from`在协程中的作用](#481-yield-from在协程中的作用)
      - [4.8.2 在协程中使用`yield from` 需要了解的几个概念](#482-在协程中使用yield-from-需要了解的几个概念)
        - [(1) 几个概念](#1-几个概念)
      - [(2) 委派生成器 的作用是？](#2-委派生成器-的作用是)
      - [(3) 子生成器 如何 终止？](#3-子生成器-如何-终止)
      - [4.8.3 `yield from`使用实例：用 `yield from` 计算平均值并输出统计报告](#483-yield-from使用实例用-yield-from-计算平均值并输出统计报告)
      - [4.8.4 调用方、委派生成器 和 子生成器是如何通信的？](#484-调用方委派生成器-和-子生成器是如何通信的)
      - [4.8.5 如果把`coroaverager3.py`中⓬处的`group.send(None)`会发生什么？](#485-如果把coroaverager3py中处的groupsendnone会发生什么)
        - [(1) `group.send(None)`这行代码的作用是什么？](#1-groupsendnone这行代码的作用是什么)
        - [(2) 注释掉`group.send(None)`会发生什么？](#2-注释掉groupsendnone会发生什么)
    - [4.8.6 用`yield from`连接多个 委派生成器](#486-用yield-from连接多个-委派生成器)
    - [4.8.7 `yield from`在协程中的现状](#487-yield-from在协程中的现状)
    - [4.9 使用`@asyncio.coroutine`来定义协程](#49-使用asynciocoroutine来定义协程)
      - [4.9.1 `@asyncio.coroutine`是什么？](#491-asynciocoroutine是什么)
      - [4.9.2 `@asyncio.coroutine`现状](#492-asynciocoroutine现状)
  - [参考文献](#参考文献-1)






&emsp;
&emsp; 
# 一、并发(Concurrency)和并行(Parallelism)
## 1. 并发和并行有何区别？
并发(Concurrency)：
> &emsp;&emsp; The ability to handle multiple pending tasks, making progress one at a time or in parallel (if possible) so that each of them eventually succeeds or fails. A singlecore CPU is capable of concurrency if it runs an OS scheduler that interleaves the execution of the pending tasks. Also known as multitasking.
> &emsp;&emsp; 
> 
并行(Parallelism)
> &emsp;&emsp; The ability to execute multiple computations at the same time. This requires a multicore CPU, multiple CPUs, a GPU, or multiple computers in a cluster.
> &emsp;&emsp; 
> 

&emsp;
## 2. 执行单元(Execution unit)
> &emsp;&emsp; General term for objects that execute code concurrently, each with independent state and call stack. Python natively supports three kinds of execution units: processes, threads, and coroutines.
> &emsp;&emsp; 
> 

Effective Python 第七章的前言总结的很好。
TODO: 把内容复制过来

&emsp;
## 2. python的并发模型
### 2.1 Python有哪些并发模型？
> ① 多线程
> ② 多进程
> ③ 协程
> 

### 2.2 这几个并发模型各有何优点？它们各自适合什么样的应用场景？
TODO: 




&emsp;
&emsp; 
# 二、 GIL锁
## 1 什么是GIL锁？
&emsp;&emsp; `GIL`全称`Global Interpreter Lock`(全局解释器锁)，缩写GIL）,是计算机程序设计语言解释器 **用于同步线程** 的一种机制，它使得任何时刻仅有一个线程在执行。
&emsp;&emsp; 简单来说，GIL 是一个互斥锁，它规定解释器同一时刻只允许一个线程运行。

&emsp;
## 2 GIL锁 是Python带来的吗？
&emsp;&emsp; 首先需要明确的一点是`GIL`并不是Python的特性，它是**在实现Python解析器(CPython)时**所引入的一个概念。
&emsp;&emsp; 我们知道python有`CPython`，`PyPy`和`JPython`。像其中的`JPython`就没有`GIL`。然而因为`CPython`是大部分环境下默认的Python执行环境。所以在很多人的概念里`CPython`就是Python，也就想当然的把`GIL`归结为Python语言的缺陷。所以这里要先明确一点：`GIL`并不是Python的特性，Python完全可以不依赖于`GIL`。

&emsp;
## 3 为什么需要GIL？
### 3.1 GIL 为Python解决了什么问题？
&emsp;&emsp; 我们知道，Python的内存管理是通过引用计数(reference counting)来完成的。当某对象的引用计数变量的值为 0 时，会将这个对象在内存中占用的资源释放。下面通过一个例子来看看引用计数如何工作：
```python
import sys

a = []
b = a 
print(sys.getrefcount(a))
```
运行结果：
```
3
```
**结果分析：**
&emsp;&emsp; 这`3`个分别是：解释器建立的临时变量`tmp`，变量`a`、`b`
问题是在多个线程试图修改这个 引用计数变量 时会引发竞态条件：
> &emsp;&emsp; 假设有两个线程A和B，它们共享了一个dict对象，在同一时刻，线程A增加了这个dict的引用计数，而线程B减少了这个dict的引用计数，此时就引发了竟态条件，这个dict的引用计数的值很可能就不准确了，这可能造成 内存泄漏 或 内存被提前释放，或者其它莫名其妙的bug。
> 
因此，为防止上述竟态条件的发生，必须给它加锁。

### 3.2 为什么选择 全局锁？而不是给每个对象的引用计数单独枷锁？
&emsp;&emsp; 如果我们给每个创建的对象都加上一把锁，那么很可能会出现一个问题 —— 死锁。
&emsp;&emsp; 另外，给每个对象的引用计数都加锁的话，会因为频繁的获取/释放锁而降低程序的性能。
因此，不能给每个对象的引用计数单独枷锁，只能用全局锁。

### 3.3 其它原因
&emsp;&emsp; 在 Python 刚诞生那会儿，操作系统甚至还没有线程这个概念。Python 的设计初衷：易学易用，加快开发者们工作的进度。
&emsp;&emsp; 另外GIL 比较容易实现，添加到 Python 中也很方便。因为只需要管理一个锁，提高了单线程程序的性能。

### 3.4 总结
&emsp;&emsp; Python的内存管理是通过引用计数来完成的，但问题是这个引用计数并不是线程安全的，多个线程在修改引用计数时可能会引发的竟态条件，所以必须加锁，此时我们有两种方案：
> ① 给每个对象的引用计数单独枷锁
> ② 加一个全局锁
> 
但问题是给每个对象的引用计数单独枷锁会导致死锁，而且频繁的加解锁也会降低程序性能，所以只能加一个全局锁(即GIL)来保护 引用计数。

&emsp;
## 4 GIL 是如何运作的？
### 4.1 GIL在什么时候会被释放？
GIL会在如下两种情况下被释放：
> &emsp;&emsp; ① 属于该线程的时间片到了(默认是5毫秒)；
> &emsp;&emsp; ② 当线程遇到IO密集型任务时，会主动释放GIL，如 磁盘IO、网络IO和`sleep()`等。
> 

### 4.2 python如何确保GIL不会一直被一个线程持有不释放？
《fluent python》第二版的原文：
> &emsp;&emsp; To prevent a Python thread from holding the GIL indefinitely, Python’s bytecode interpreter pauses the current Python thread every 5ms by default,releasing the GIL. The thread can then try to reacquire the GIL, but if there are other threads waiting for it, the OS scheduler may pick one of them to proceed.
> &emsp;&emsp; 为防止GIL一直被一个线程持有，python解释器默认每`5ms`进行线程切换，切换时会把GIL释放。该线程开继续尝试获取GIL，但是如果此时有其它线程也请求获取GIL，解释器可能会从这些请求GIL的线程选一个，然后把GIL给它，然后这个得到GIL的线程就能继续运行了。
> 

&emsp;
## 5 为什么不在 Python 3 中删除它？
**(1) 去掉GIL会降低单线程性能**
&emsp;&emsp; Python3 确实有机会从头开始，包括重写现存的一些 C 扩展。但 GIL并不是一无是处，它的存在使python的单线程的性能非常强大，曾经有人实现过不带GIL的解释器，但代价却是单线程性能却下降了好几倍。
**(2) 对于那些必须高效利用CPU核心的应用，可以寻求其它替代方案**
&emsp;&emsp; 对于IO密集型引用，GIL影响很小；而对于CPU密集型应用，可以使用其它替代方案(如多进程)。

&emsp;
## 6 GIL锁的影响
### 6.1 GIL会带来什么影响？
&emsp; GIL主要会影响多线程程序：
> &emsp;&emsp; 尽管Python完全支持多线程编程，但是解释器的C语言实现部分在完全并行执行时并不是线程安全的。 实际上，解释器被一个全局解释器锁保护着，它确保任何时候都只有一个Python线程执行。
> 
带来的问题是：
> &emsp;&emsp; Python的多线程程序并不能利用多核CPU的优势，对于一个使用了多个线程的程序，同一时间只能有一个线程被执行，即使其它几个CPU都是空闲状态。
> 

### 6.2 多线程编程中，如何最大程度的避免GIL锁带来的影响？
&emsp; 我们都知道，程序大概可以分成两类
> &emsp;&emsp; ① IO密集型
> &emsp;&emsp; ② CPU密集型
> 
&emsp;&emsp;对于 **IO密集型** 的多线程程序，`GIL` 对 的很友好，可以大大提高其性能。因为它们大部分时间都在等待，当一个线程等待 I/O 的时候，GIL让前者睡眠，然后启动另外的线程。
&emsp;&emsp;而对于 **CPU密集型** 的多线程程序，如果GIL 的存在给程序性能造成了影响，可以尝试一下的解决方案：
> &emsp;&emsp; ① 使用多进程；
> &emsp;&emsp; ② 换个解释器

&emsp;
## 7 GIL是否意味着线程安全？
&emsp;&emsp; 不是

&emsp;
## 8 GIL锁的存在是否意味着python多线程一点用也没有了？
&emsp;&emsp; 不是，GIL只对CPU密集型应用影响比较大，IO密集型应用影响有限。

&emsp;
## 9. 如何通俗的解释GIL？
&emsp;&emsp; GIL有点像一个通行证，但这个通行证在每个进程里只有一个，得到GIL的线程就能运行，没有得到GIL的线程们被挂起并排队等待GIL。


&emsp;
## 参考文献
1. [翻译：什么是全局解释器锁GIL？](https://zhuanlan.zhihu.com/p/67349209)
2. [What Is the Python Global Interpreter Lock (GIL)?](https://realpython.com/python-gil/)
3. [CPython有GIL是因为当年设计CPython的人偷懒吗？](https://www.zhihu.com/question/439920631/answer/1685766305)
4. [为什么 Python的GIL问题一直让人诟病，Python社区却不解决？](https://www.zhihu.com/question/323812020/answer/2219586213)
5. [面试官：你如何破解 Python的 GIL 的？这才是完美的回答](https://zhuanlan.zhihu.com/p/407236410)
6. [深入理解Python中的GIL](https://zhuanlan.zhihu.com/p/75780308)
7. [python cookbook]()
8. [python的GIL锁](https://blog.csdn.net/qq_43517875/article/details/109131380)






&emsp;
&emsp; 
# 三、 多线程
## 1. 解释器的线程切换间隔是多少？如何获取、修改系统切换线程切换间隔？
&emsp;&emsp; 默认情况下，python解释器 **每`5ms`** 进行线程切换，切换时会把GIL释放。我们可以通过下面的方法来获取、修改线程切换的间隔：
> ① `sys.getswitchinterval()`  ： 获取解释器的线程切换间隔
> ② `sys.setswitchinterval(s)` :  修改解释器的线程切换间隔，参数`s`的单位是 秒
> 
```python
import sys

# (1) 获取线程切换的间隔
print(f"Default switching interval: {sys.getswitchinterval()}")

# (2) 修改线程切换的间隔
sys.setswitchinterval(1)
print(f"Current switching interval: {sys.getswitchinterval()}")
```
运行结果：
```
Default switching interval: 0.005
Current switching interval: 1.0
```

&emsp;
## 2. `thread`和`threading`
&emsp;&emsp; `thread` 模块已被废弃。用户可以使用 `threading` 模块代替。所以，在 Python3 中不能再使用`thread` 模块。为了兼容性，Python3 将 `thread` 重命名为 `_thread` 。

&emsp;
## 3. `threading`
### 3.1 `threading` 和 `multiprocessing`
&emsp;&emsp; `multiprocessing` 是一个支持使用与 `threading` 模块类似的 API 来产生进程的包。但是它们之间区别也不小。

### 3.2 事件对象`Event`
#### 3.2.1 `Event`的作用
&emsp;&emsp; `Event`是事件对象，**是一种线程间的通信机制**，可以说是线程之间通信的最简单机制之一：一个线程发出事件信号，而其他线程等待该信号。
&emsp;&emsp; 一个事件对象管理一个内部标识，调用 `set()` 方法可将其设置为 `true` ，调用 `clear()` 方法可将其设置为 `false` ，调用 `wait()` 方法将进入阻塞直到标识为 `true` 。
&emsp;&emsp; `Event`是一个实现事件对象的类。每个事件对象管理着一个内部标识，调用 `set()` 方法可将其设置为`true`。调用 `clear()` 方法可将其设置为 `false` 。调用 `wait()` 方法将进入阻塞直到标识为`true`。这个标识初始时为 `false` 。

#### 3.2.2 `Event`提供的方法
`Event`类包含以下方法：
(1) `is_set()`
> &emsp;&emsp; 当且仅当内部标识为 true 时返回 `True`。
> &emsp;&emsp; `isSet` 方法是此方法的已弃用别名。
> 

(2) `set()`
> &emsp;&emsp; 将内部标识设置为 `true`。
> 

(3) `clear()`
> &emsp;&emsp; 于清除标志位，使之为`false`。
> 

(4) `wait(timeout=None)`
> &emsp;&emsp; 默认情况下(即`timeout`参数为`None`)，阻塞线程直到内部变量为`true`；若调用时内部标识为 `true`，将立即返回。
> &emsp;&emsp; 当提供了`timeout`参数且不是 `None` 时，它应该是一个浮点数，代表操作的超时时间，以秒为单位（可以为小数）。
> 当内部变量为`true`，`wait()`将返回`true`；若`timeout`参数设置的等待时间到了，内部标识还未变为`true`，则`wait()`将返回`false`。
> 
#### 3.2.3 `Event`的使用实例
《Fluent Python》第二版的一个例子：
```python
import itertools
import time
from threading import Thread, Event


def spin(msg: str, done: Event) -> None:
    for char in itertools.cycle(r'\|/-'):
        status = f'\r{char} {msg}'
        print(status, end='', flush=True)
        if done.wait(0.1):
            break
    blanks = ' ' * len(status)
    print(f'\r{blanks}\r', end='')


def slow() -> int:
    time.sleep(3)
    return 42


def supervisor() -> int:
    done = Event()
    spinner = Thread(target=spin, args=('thinking!', done))
    print(f'spinner object: {spinner}')
    spinner.start()
    result = slow()

    # 将Event对象done的内部标识设为true，此时 spin()里面的 done.wait(0.1)将返回true
    done.set() 

    spinner.join()
    return result


def main() -> None:
    result = supervisor()
    print(f'Answer: {result}')


if __name__ == '__main__':
    main()
```







&emsp;
&emsp; 
# 四 多进程






&emsp;
&emsp; 
# 五 `concurrent.futures`
&emsp;&emsp; 截止到python3.10，`concurrent.futures`是里面唯一的模块(module )。

## 1. `concurrent.futures`的有何用处和优势？
标准库里面是这么说的:
> &emsp;&emsp; The `concurrent.futures` module provides a high-level interface for asynchronously executing callables.
> &emsp;&emsp; `concurrent.futures`提供了为 异步调用 提供了高级别的接口(其实就是对`threading`类和`multiprocessing`类进行了封装，简化了这两个库的使用)。
> &emsp;&emsp; The asynchronous execution can be performed with threads, using `ThreadPoolExecutor`, or separate processes, using `ProcessPoolExecutor`. Both implement the same interface, which is defined by the abstract `Executor` class.
> &emsp;&emsp; 我们可以通过 线程(`ThreadPoolExecutor`) 和 进程(`ProcessPoolExecutor`)来异步执行代码，它俩通过(继承)`Executor`类实现一套类似的API(这降低了我们的学习成本)。
> 
`concurrent.futures`更贴近应用层面，而无需关心底层并发的控制逻辑（并发锁、数据共享等）。用起来更加便捷，不过这也牺牲了对进程的部分控制能力。

&emsp;
## 2. `concurrent.futures`提供的工具
### 2.1 `concurrent.futures`主要包含哪些实用的工具？
#### (1) 执行器类
&emsp;&emsp; 库中包含了若干 3个执行器对象类`Executor Objects`，以及1个`Future`类：
**(1) `Executor`类**
> &emsp;&emsp; An abstract class that provides methods to execute calls asynchronously. It should not be used directly, but through its concrete subclasses.
> &emsp;&emsp; `Executor`是一个抽象类，它提供了一些异步执行的方法，它不应该被直接使用(实例化)，而应该使用它的子类(指的是`ThreadPoolExecutor`和`ProcessPoolExecutor`)。
> 

**(2) `ThreadPoolExecutor`类**
> &emsp;&emsp; `ThreadPoolExecutor` is an `Executor` subclass that uses a pool of threads to execute calls asynchronously.
> &emsp;&emsp; `ThreadPoolExecutor`是`Executor`的子类，它内部用了一个线程池来执行异步调用
> 

**(3) `ProcessPoolExecutor`类**
> &emsp;&emsp; The `ProcessPoolExecutor` class is an `Executor` subclass that uses a pool of processes to execute calls asynchronously. `ProcessPoolExecutor` uses the `multiprocessing` module, which allows it to side-step the Global Interpreter Lock but also means that only `picklable` objects can be executed and returned.
> &emsp;&emsp; `ProcessPoolExecutor`是`Executor`的子类，它内部用了一个进程池来执行异步调用。因为它内部使用的是`multiprocessing`模块，因此它可以避开GIL的限制，但这意味着`ProcessPoolExecutor`只能执行和返回 `picklable`对象。
> 

**(4) `Future`类**
> &emsp;&emsp; The `Future` class encapsulates the asynchronous execution of a callable. Future instances are created by `Executor.submit()`.
> &emsp;&emsp; `Future`类封装了一个可异步执行的可调用对象，它由`Executor.submit()`创建。
> 

**⑤ `Exception classes`**
> &emsp;&emsp; 封装了一些在`concurrent.futures`定义的异常。
> 

### 2.2 `concurrent.futures`如何确定进程池(线程池)的个数？
#### 2.2.1 显示指定池子的大小
`ThreadPoolExecutor`和`ProcessPoolExecutor`都可以通过`max_workers`来指定池子的大小。
#### 2.2.2 若不指定池子大小，那池子的大小由什么决定？
##### (1)  `ThreadPoolExecutor`
让我们来看看`ThreadPoolExecutor`的源码：
```python
class ThreadPoolExecutor(_base.Executor):
    def __init__(self, max_workers=None, thread_name_prefix='',
                 initializer=None, initargs=()):
        if max_workers is None:
            # ThreadPoolExecutor is often used to:
            # * CPU bound task which releases GIL
            # * I/O bound task (which releases GIL, of course)
            #
            # We use cpu_count + 4 for both types of tasks.
            # But we limit it to 32 to avoid consuming surprisingly large resource
            # on many core machine.
            max_workers = min(32, (os.cpu_count() or 1) + 4)
        if max_workers <= 0:
            raise ValueError("max_workers must be greater than 0")
```
`(os.cpu_count() or 1)`返回的肯定是`os.cpu_count()`，因此默认情况下，线程执行器的线程池个数为：
```python
max_workers = min(32, os.cpu_count() + 4)
```

##### (2) `ProcessPoolExecutor`
&emsp;&emsp; **进程池默认和CPU内核个数一致**，让我们来看看`ProcessPoolExecutor`的源码：
```python
class ProcessPoolExecutor(_base.Executor):
    def __init__(self, max_workers=None, mp_context=None,
        if max_workers is None:
            self._max_workers = os.cpu_count() or 1
            if sys.platform == 'win32':
                self._max_workers = min(_MAX_WINDOWS_WORKERS,
                                        self._max_workers)
        else:
            if max_workers <= 0:
                raise ValueError("max_workers must be greater than 0")
            elif (sys.platform == 'win32' and
                max_workers > _MAX_WINDOWS_WORKERS):
                raise ValueError(
                    f"max_workers must be <= {_MAX_WINDOWS_WORKERS}")

            self._max_workers = max_workers
```
&emsp;&emsp; 显然，`ProcessPoolExecutor`的进程池的个数为和 CPU内核个数`os.cpu_count()`一致。
&emsp;&emsp; 唯一一个例外就是如果在`win32`环境下，取 CPU内核个数 和 内核允许的最大进程数`_MAX_WINDOWS_WORKERS` 两者的最小值。

&emsp;
## 3. 如何使用`concurrent.futures`执行并发(并行)操作？
使用`ThreadPoolExecutor`和`ProcessPoolExecutor`：
> (1) `ThreadPoolExecutor(max_workers=None, thread_name_prefix='', initializer=None, initargs=())`
> 
> (2) `ProcessPoolExecutor(max_workers=None, mp_context=None, initializer=None, initargs=())`
> 
它们俩都是`Executor`的子类，而且它们对外提供的API类似：
### 3.1 `submit(fn, /, *args, **kwargs)`
> &emsp;&emsp; Schedules the callable, `fn`, to be executed as `fn(*args, **kwargs)` and returns a `Future` object representing the execution of the callable.
> &emsp;&emsp; 将可调用对象`fn`加入池中并以`fn(*args, **kwargs)`方式运行，然后返回一个代表 `fn`的`Future`对象。
> 


### 3.2 `map(func, *iterables, timeout=None, chunksize=1)`
> &emsp;&emsp; 类似于内建函数`map()`，但不同的是，可调用对象`func`会被并发(并行)执行；
> &emsp;&emsp; 它将返回一个生成器，里面包含着每个`func`的返回值；
> 

### 3.3 `shutdown(wait=True, *, cancel_futures=False)`
> &emsp;&emsp; Signal the executor that it should free any resources that it is using when the currently pending futures are done executing. Calls to `Executor.submit()` and `Executor.map()` made after shutdown will raise `RuntimeError`.
> &emsp;&emsp; 在处于`pending`状态的 `future`对象完成执行后 向执行器(executor)发送信号，使其释放正在使用的任何资源。 另外，在进行`Executor.shutdown()`操作后调用 `Executor.submit()` 和 `Executor.map()` 将会引发 `RuntimeError`。
> **`wait`参数：**
> &emsp;&emsp; 若`wait`为`True`，则在处于`pending`状态的`future`对象完成执行并释放相关资源前，`shutdown()`不会返回。
> &emsp;&emsp; 若`wait`为` False`，则`shutdown()`将立即返回，相关资源将在将在所有`pending`的 `future` 对象完成执行后进行释放;
> **`cancel_futures`参数**
> &emsp;&emsp; If cancel_futures is True, this method will cancel all pending futures that the executor has not started running. Any futures that are completed or running won’t be cancelled, regardless of the value of cancel_futures.
> &emsp;&emsp; If both cancel_futures and wait are True, all futures that the executor has started running will be completed prior to this method returning. The remaining futures are cancelled.
> 
**另外，如果不想调用`shutdown()`，可以改用`with`语句(上下文管理器)，详情见下面的介绍**

### 3.5 `Executor`和上下文管理器
`Executor`类实现了上下文管理器：
> &emsp;&emsp; `executor.__exit__` method will call `executor.shutdown(wait=True)`, which will block until all threads are done.
> &emsp;&emsp; `executor.__exit__`方法将会调用`executor.shutdown(wait=True)`，这会造成主线程阻塞，直到所有子线程(进程)完成。
> 

### 3.6 应该使用 `ThreadPoolExecutor`还是`ProcessPoolExecutor`？
> &emsp;&emsp; Processes use more memory and take longer to start than threads, so the real value of ProcessPoolExecutor is in CPU-intensive jobs
> 
**CPU密集型使用多进程，IO密集型使用多线程。**

### 3.7 `Executor.map(func, *iterables, timeout=None, chunksize=1)`
#### 3.7.1 `Executor.map()`是阻塞的吗？
&emsp;&emsp; `Executor.map()`不是阻塞的，

#### 3.7.2 `Executor.map()`的返回值有何特点？
&emsp;&emsp; `map()`的返回值是按进程(线程)的调用顺序排列的，这个特性可能有用，也可能没用， 具体情况取决于需求： 
> `Executor.map`函数返回结果的顺序与调用开始的顺序一致。 如果第一个调用生成结果用时 `10` 秒， 而其他调用只用 `1` 秒， 代码会阻塞 `10` 秒， 获取 `map` 方法返回的生成器产出的第一个结果。 在此之后， 获取后续结果时不会阻塞， 因为后续的调用已经结束。 
> 
如果必须等到获取所有结果后再处理， 这种行为没问题；不过， 通常更可取的方式是：
> 不管提交的顺序， 只要有结果就获取。 为此， 要把 `Executor.submit` 方法和`futures.as_completed` 函数结合起来使用。
> 

#### 3.7.3 如果`map()`执行的某个函数发生了异常，会发生什么？
官方文档如下：
> &emsp;&emsp; If a `func` call raises an exception, then that exception will be raised when its value is retrieved from the iterator.
> &emsp;&emsp; 如果调用的`func`函数抛出了异常，则在从迭代器（指的是`map()`的返回值）检索其值时将引发该异常。
> 
但是有一点需要注意：
> &emsp;&emsp; 如果未处理的异常(不限于`StopIteration`)由生成器函数引发或通过，则异常以通常的方式传递给调用者，并随后尝试恢复生成器函数引发停止迭代。换句话说，未处理的异常会终止生成器的使用寿命。
> 
来看一个例子：
```python
import concurrent.futures

def test_func(val):
    raise Exception(val)        

with concurrent.futures.ThreadPoolExecutor() as executor:   
    results = executor.map(test_func,[1,2,3,4,5])
    for r in results:
        try:
            print (r)
        except Exception as exc:
            print ('generated an exception: %s' % (exc))
```
运行结果：
```
Traceback (most recent call last):
  File "d:\code_practice\practice.py", line 67, in <module>
    for r in results:
  File "C:\Python310\lib\concurrent\futures\_base.py", line 608, in result_iterator
    yield fs.pop().result()
  File "C:\Python310\lib\concurrent\futures\_base.py", line 438, in result
    return self.__get_result()
  File "C:\Python310\lib\concurrent\futures\_base.py", line 390, in __get_result
    raise self._exception
  File "C:\Python310\lib\concurrent\futures\thread.py", line 52, in run
    result = self.fn(*self.args, **self.kwargs)
  File "d:\code_practice\practice.py", line 63, in test_func
    raise Exception(val)        
Exception: 1
```
**可以看到的是，我们只捕获了第一个异常，然后程序就终止了**
如果想把所有异常都打出来，可以这么做：
```python
import concurrent.futures

def test_func(val):
    raise Exception(val)        

future_list = []
with concurrent.futures.ThreadPoolExecutor() as executor:
    for arg in range(10):
        future = executor.submit(test_func, arg)
        future_list.append(future)

for future in future_list:
    try:
        print(future.result())
    except Exception as e:
        print(e)
```
运行结果：
```
0
1
2
3
4
5
6
7
8
9
```

### 3.8 使用示例
```python
from concurrent.futures import ProcessPoolExecutor


def cube(name):
    return name**3


# 不使用上下文管理器
def exec_no_with(ls):
    # (1) 新建一个执行器
    executor = ProcessPoolExecutor()
    ret =[]
    # (2) 通过 for循环 + submit() 来达到多线程执行的效果
    for item in ls:
        f = executor.submit(cube, item) # submit()返回一个Future对象
        ret.append((item, f.result()))    
    executor.shutdown(wait=True)
    return ret


# 使用上下文管理器
def exec_use_with(ls):
    ret = []
    with ProcessPoolExecutor() as executor: # (1) 新建一个执行器
        # (2) 通过 for循环 + submit() 来达到多线程执行的效果
        for item in ls:
            f = executor.submit(cube, item)
            ret.append((item, f.result()))
        # (3) 离开with语句时，executor.__exit__方法将替我们调
        #    用executor.shutdown(wait=True)，以等待所有子进程完成并释放资源
    return ret


# 上下文管理器 + Executor.map()
def exec_use_with_map(ls):
    with ProcessPoolExecutor() as executor:
        ret = executor.map(cube, ls)
    return ret


if __name__ == "__main__":
    print(f"exec_no_with()      :  {exec_no_with(range(0, 15, 3))}")
    print(f"exec_use_with()     :  {exec_use_with(range(0, 15, 3))}")
    # map()返回的是一个生成器
    print(f"exec_use_with_map() :  {list(exec_use_with_map(range(0, 15, 3)))}")
```
运行结果：
```
exec_no_with()      :  [(0, 0), (3, 27), (6, 216), (9, 729), (12, 1728)]
exec_use_with()     :  [(0, 0), (3, 27), (6, 216), (9, 729), (12, 1728)]
exec_use_with_map() :  [0, 27, 216, 729, 1728]
```

&emsp;
## 4 `concurrent.futures.Future`类
### 4.1 它的作用是？它和`asyncio.Future`有何联系？
来看看《Fluent Python》第二版是怎么说的：
> &emsp; Since Python 3.4, there are two classes named `Future` in the standard library: `concurrent.futures.Future` and `asyncio.Future`. They serve the same purpose: an instance of either `Future` class represents a deferred computation that may or may not have completed. This is somewhat similar to the `Deferred` class in Twisted, the `Future` class in Tornado, and `Promise` in modern JavaScript. 
> &emsp; 自 Python 3.4后，标准中有两个名为`Future`的类：
> > &emsp;&emsp; ① `concurrent.futures.Future`
> > &emsp;&emsp; ② `asyncio.Future`
> 
> 它俩的作用是一样的：它们任何一个类的对象都代表一个 可能完成，也可能不完成的 延迟计算。作用和 Twisted的 `Deferred`类、Tornado的`Future` 类以及现代JS的`Promise` 类似。
> 
> &emsp; Futures encapsulate pending operations so that we can put them in queues, check whether they are done, and retrieve results (or exceptions) when they become available. 
> &emsp; 
> 

### 4.2 如何使用它？使用时应注意什么？
#### 4.2.1 如何使用？
&emsp;&emsp; `Future`对象由`Executor.submit()` 创建并返回。

#### 4.2.2 使用时应注意什么？
《Fluent Python》第二版原文如下：
> &emsp;&emsp; An important thing to know about `futures` is that you and I should not create them: they are meant to be instantiated exclusively by the concurrency framework, be it `concurrent.futures` or `asyncio`. Here is why: a `Future` represents something that will eventually run, therefore it must be scheduled to run, and that’s the job of the framework. In particular, `concurrent.futures.Future` instances are created only as the result of submitting a callable for execution with a `concurrent.futures.Executor` subclass. For example, the `Executor.submit()` method takes a callable, schedules it to run, and returns a `Future`.
> &emsp;&emsp; 对于`Future`，有一点很重要：我们不应该自己创建`Future`对象：它们只应该由并发框架来实例化（如`concurrent.futures` 和 `asyncio`），原因如下：一个`Future`对象表示的是 一个待执行的任务，因此它应该由框架来规划(执行)。特别是，`Future`实例只能通过传一个可调用对象给 `concurrent.futures.Executor`的子类(其实就是`ThreadPoolExecutor`和`ProcessPoolExecutor`)来获取，例如`Executor.submit()`方法接收一个可调用对象并规划其运行，并返回一个`Future`对象。
> &emsp;&emsp; Application code is not supposed to change the state of a future: the concurrency framework changes the state of a future when the computation it represents is done, and we can’t control when that happens.
> &emsp;&emsp; 应用代码不应该修改`Future`实例的状态，而应该由框架来修改。
> 
上面的内容总结一下就是：
① 不要自己创建`Future`对象；
② 不要自己修改`Future`的状态；

&emsp;
## 5. `concurrent.futures`的 模块函数(Module Functions)
### 5.1 `wait()`

### 5.2 `as_completed()`
#### 5.2.1 `as_completed()`的优点是？
&emsp;&emsp; 相比于比 `executor.map`，`executor.submit` 和 `futures.as_completed`的组合有如下几个优点：
> &emsp;&emsp; ① 和`Executor.map()`不一样的是，它们不管提交的顺序，只要有结果就获取。
> &emsp;&emsp; ② 更灵活，因为`submit`方法能处理不同的可调用对象和参数，而`executor.map` 只能处理参数不同的同一个可调用对象。此外，传给`futures.as_completed` 函数的`Futere`集合可以来自多个 `Executor`实例，例如一些由`ThreadPoolExecutor`实例创建，另一些由`ProcessPoolExecutor`实例创建。
> 
来看看实例：
```python

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor,as_completed
import urllib.request

URLS = ['http://www.foxnews.com/',
        'http://www.jd.com/',
        'http://www.baidu.com/',
        'https://www.zhihu.com/explore',
        'http://some-made-up-domain.com/']

# Retrieve a single page and report the URL and contents
def load_url(url, timeout):
    with urllib.request.urlopen(url, timeout=timeout) as conn:
        return conn.read()


if __name__ == "__main__":
    # We can use a with statement to ensure threads are cleaned up promptly
    with ThreadPoolExecutor() as thread_exc, ProcessPoolExecutor() as process_exc:
        # 我们可以将ThreadPoolExecutor和ProcessPoolExecutor创建的`Futere`集合
        ## 加入到as_completed()中，这是map()做不到的
        future_to_url = {thread_exc.submit(load_url, url, 60): url for url in URLS[:2]}
        future_to_url.update({process_exc.submit(load_url, url, 60): url for url in URLS[2:]})
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                data = future.result()
            except Exception as exc:
                print('%r generated an exception: %s' % (url, exc))
            else:
                print('%r page is %d bytes' % (url, len(data)))
```
运行结果：
```
'http://www.jd.com/' page is 150665 bytes
'http://www.baidu.com/' page is 354705 bytes
'http://some-made-up-domain.com/' generated an exception: cannot pickle '_io.BufferedReader' object
'https://www.zhihu.com/explore' page is 300928 bytes
'http://www.foxnews.com/' page is 289079 bytes
```
从输出结果很容易看出：
> &emsp;&emsp; **通过`as_completed()`获取的结果和线程(进程)运行的顺序不一致，谁先完成那么就先获取谁的结果。**
> 

&emsp;
## Fluent Python第二版中的国旗下载脚本
### 顺序下载脚本
```python
import time
from pathlib import Path
from typing import Callable

import httpx # httpx 不是标准库，按规范应空一行写在标准库下面

POP20_CC = ('CN IN US ID BR PK NG BD RU JP '
            'MX PH VN ET EG DE IR TR CD FR').split()

BASE_URL = 'https://www.fluentpython.com/data/flags'
DEST_DIR = Path('downloaded')


def save_flag(img: bytes, filename: str) -> None:
    (DEST_DIR / filename).write_bytes(img)


def get_flag(cc: str) -> bytes:
    url = f'{BASE_URL}/{cc}/{cc}.gif'.lower()
    # 在进行网络请求时显式提供超时时间是个好习惯，可以避免长时间阻塞
    resp = httpx.get(url, timeout=6.1,
                        follow_redirects=True) # By default, HTTPX does not follow redirect
    resp.raise_for_status()
    return resp.content


def download_many(cc_list: list[str]) -> int:
    for cc in sorted(cc_list):
        image = get_flag(cc)
        save_flag(image, f'{cc}.gif')
        print(cc, end=' ', flush=True)
    return len(cc_list)


def main(downloader: Callable[[list[str]], int]) -> None:
    DEST_DIR.mkdir(exist_ok=True)
    t0 = time.perf_counter()
    count = downloader(POP20_CC)
    elapsed = time.perf_counter() - t0
    print(f'\n{count} downloads in {elapsed:.2f}s')


if __name__ == '__main__':
    main(download_many)
```
运行结果：
```
BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN
20 downloads in 142.45s
```

### 用`concurrent.futures`重写
```python
import time
from pathlib import Path
from typing import Callable
from concurrent import futures


import httpx # httpx 不是标准库，按规范应空一行写在标准库下面

POP20_CC = ('CN IN US ID BR PK NG BD RU JP '
            'MX PH VN ET EG DE IR TR CD FR').split()

BASE_URL = 'https://www.fluentpython.com/data/flags'
DEST_DIR = Path('downloaded')


def save_flag(img: bytes, filename: str) -> None:
    (DEST_DIR / filename).write_bytes(img)


def get_flag(cc: str) -> bytes:
    url = f'{BASE_URL}/{cc}/{cc}.gif'.lower()
    # 在进行网络请求时显式提供超时时间是个好习惯，可以避免长时间阻塞
    resp = httpx.get(url, timeout=6.1,
                        follow_redirects=True) # By default, HTTPX does not follow redirect
    resp.raise_for_status()
    return resp.content


# 工作线程
def download_one(cc: str):
    image = get_flag(cc)
    save_flag(image, f'{cc}.gif')
    print(cc, end=' ', flush=True)
    return cc


def download_many(cc_list: list[str]) -> int:
    with futures.ThreadPoolExecutor() as executor:
        res = executor.map(download_one, sorted(cc_list))
    return len(list(res))


def main(downloader: Callable[[list[str]], int]) -> None:
    DEST_DIR.mkdir(exist_ok=True)
    t0 = time.perf_counter()
    count = downloader(POP20_CC)
    elapsed = time.perf_counter() - t0
    print(f'\n{count} downloads in {elapsed:.2f}s')


if __name__ == '__main__':
    main(download_many)
```
运行结果：
```
ID ET MX DE PH NG FR JP BR PK IN EG CN IR RU TR US CD BD VN 
20 downloads in 9.04s
```
可以看到的是，用多线程下载明显更快；

### 用协程重构



&emsp;
&emsp; 
# 六、协程(Coroutines)
## 1. 什么是协程？
&emsp;&emsp; 协程，又称微线程，纤程，是一种用户态的轻量级线程。

&emsp;
## 2. 协程和线程
### 2.1 协程是多线程吗？
&emsp;&emsp; 协程本质上是单线程，拥有自己的寄存器上下文和栈。所以能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态。
TODO:
### 2.2 协程和线程的对比

### 2.3 协程的优势和应用场景

&emsp;
## 3. 协程的发展
### 3.1 Python和协程相关的关键字和方法的引入过程
阶段一：基于生成器的协程：
(1) Python 2.2  `yield`
(2) Python 2.5  `send()` `throw()` `close()`，标志着python开始支持协程；
(3) Python 3.3  `yield from`，使python协程语法简化了；
阶段二：原生协程
(4) Python 3.5  `async` 和 `await`，标志着python支持 原生协程(native coroutine)

### 3.2 几种协程的介绍
**(1) Native coroutine(原生协程)**
> &emsp;&emsp; A coroutine function defined with `async def`. You can delegate from a native coroutine to another native coroutine using the `await` keyword, similar to how classic coroutines use `yield from`. The `async def` statement always defines a native coroutine, even if the `await` keyword is not used in its body. The `await` keyword cannot be used outside of a native coroutine.
> 
**(2) Classic coroutine(经典协程)**
> &emsp;&emsp; A generator function that consumes data sent to it via `my_coro.send(data)` calls, and reads that data by using `yield` in an expression. Classic coroutines can delegate to other classic coroutines using `yield` from. Classic coroutines cannot be driven by `await`, and are no longer supported by `asyncio`.
> 
**(3) Generator-based coroutine(基于生成器的协程)**
> &emsp;&emsp; A generator function decorated with `@types.coroutine`—introduced in Python 3.5. That decorator makes the generator compatible with the new `await` keyword.
> 

### 3.3 应该使用哪种协程呢？
&emsp;&emsp; 应该使用原生协程，因为基于生成器的协程的支持已被 弃用 并将在 Python 3.11 中被移除。

&emsp;
## 4 原生协程(native coroutines) 介绍
### 4.1 什么是原生协程？什么是 非原生协程？
&emsp;&emsp; 在python3.5之前，python的协程都是 基于生成器的协程(generator based coroutines)，换句话说就是 非原生协程。
&emsp;&emsp; 在python3.5中加入了`async`/`await`，使用这两个关键字定义的协程叫 原生协程(native coroutines)。

### 4.2 需要先了解的几个概念
① `coroutine` 
> &emsp;&emsp; 协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。
> 
② `event_loop` 
> &emsp;&emsp; 事件循环，程序开启一个无限的循环，程序员会把一些函数注册到事件循环上。当满足事件发生的时候，调用相应的协程函数。
> 
③ `task` 
> &emsp;&emsp; 任务，是对协程进一步封装，其中包含任务的各种状态。
> 
④ `future`
> &emsp;&emsp; 代表将来执行或没有执行的任务的结果。它和task上没有本质的区别
> 

&emsp;
## 5 原生协程的定义和使用
### 5.3.1 定义原生协程
&emsp;&emsp; 在`def`前加上`async`的声明，就完成了一个协程函数的定义：
```python
import asyncio

async def fun(a): # 定义协程函数
    print(a)

asyncio.run(fun("Hello World!"))
```
### 5.3.2 使用原生协程
#### (1) 和函数不一样的是，简单地调用一个协程并不会使其被调度执行
&emsp;&emsp; 和之前的 非原生协程 一样，简单地调用一个协程并不会使其被调度执行：
```python
>>> import asyncio

>>> async def main():
...     print('hello')
...     await asyncio.sleep(1)
...     print('world')

>>> main() # 注意：和函数不一样的是，简单地调用一个协程并不会使其被调度执行
<coroutine object main at 0x1053bb7c8>

>>> asyncio.run(main()) # 运行一个协程
hello
world
```

#### (2) 调用协程的几种方式
要真正运行一个协程，`asyncio`提供了**三种**主要机制:
| 机制                    | 使用场景                      | 是否阻塞                   |
| ----------------------- | ----------------------------- | -------------------------- |
| `asyncio.run()`         | 用来调用最外层的入口`main()`  | 是，直到运行的协程返回     |
| `await`                 | `await`关键字用来等待一个协程 | 是，直到等待的协程返回     |
| `asyncio.create_task()` | 用来并发运行多个协程          | 否，将立即返回一个Task对象 |

**另外需要注意的是**，通过`task1 = asyncio.create_task()`添加的任务不会立即运行，而是处于`pending`状态，可以通过一下两种方式获得运行权：
> ① `await task1`
> ② 系统的调度，比如有某个子例程通过`asyncio.sleep(n)`主动将运行权交出，此时`asyncio`事件循环(asyncio event loop)将得到运行权，它会负责调度那些处于`pending`状态的协程。
> 

**① `asyncio.run()`**
&emsp;&emsp; `asyncio.run()`函数用来运行最高层级的入口点 `main()` 函数，例子见上一小节。 

**② `await`**                 
&emsp;&emsp; `await`用来等待一个协程。下面的代码段会在等待`1`秒后打印 "hello"，然后 再次 等待 `2` 秒后打印 "world":
```python
import asyncio
from datetime import datetime


async def say_after(delay, what):
    await asyncio.sleep(delay)
    print(f"{what:>10}")


async def main():
    start_time = datetime.now()
    print(f"started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    await say_after(1, 'hello')
    await say_after(2, 'world')

    end_time = datetime.now()
    print(f"finished at {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total time: {end_time-start_time}s")


if __name__ =="__main__":
    # 正如上面所说，asyncio.run()函数用来运行最高层级的入口点main()函数
    asyncio.run(main())
```
运行结果：
```
started at 2022-08-23 13:56:40
     hello
     world
finished at 2022-08-23 13:56:43
Total time: 0:00:03.026763s
```

**③ `asyncio.create_task()`**
&emsp;&emsp; 让我们修改上面`await`中的示例，并发运行两个 `say_after` 协程:
```python
import asyncio
from datetime import datetime


async def say_after(delay, what):
    await asyncio.sleep(delay)
    print(f"{what:>10}")


async def main():
    # 这里改为了使用 asyncio.create_task()来创建 task
    task1 = asyncio.create_task(say_after(1, 'hello'))
    task2 = asyncio.create_task(say_after(2, 'world'))

    start_time = datetime.now()
    print(f"started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # 这里await也改为了 前面新建的task对象
    await task1
    await task2

    end_time = datetime.now()
    print(f"finished at {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total time: {end_time-start_time}s")


if __name__ =="__main__":
    # 正如上面所说，asyncio.run()函数用来运行最高层级的入口点main()函数
    asyncio.run(main())
```
运行结果：
```
started at 2022-08-23 14:01:19
     hello
     world
finished at 2022-08-23 14:01:21
Total time: 0:00:02.009229s
```
**注意，预期的输出显示代码段的运行时间比之前快了 `1` 秒**，这是因为`asyncio.create_task()`让两个协程并发执行了。

#### (3) 获取协程的返回值
对于协程的三种运行方式，`await`和 `asyncio.run()`都会返回它们运行的协程的返回值:
```python
import asyncio


async def nested(s):
    return s


async def coro_func(num):
    ret = await nested(num) # await会 返回 它所等待的协程的返回值
    print(f"await nested(s) 的返回值：    {ret}")
    return ret+1


# run()函数会返回它运行的协程的返回值
ret = asyncio.run(coro_func(99)) 
print(f"asyncio.run(coro_func(99))  的返回值： {ret}")
```
运行结果：
```
await nested(s) 的返回值：    99
asyncio.run(coro_func(99))  的返回值： 100
```

#### (4) `await`关键字的使用有何限制？
`await` 只能在 带有`async def`的 协程函数 中使用(即只能用于原生协程)，要不然会报错：
```python
async def coro_func():
    print("In coro_func()")


await coro_func()
```
运行结果：
```
  File "d:\code_practice\practice.py", line 5
    await coro_func()
    ^^^^^^^^^^^^^^^^^
SyntaxError: 'await' outside function
```
把`await`用协程函数封装一下，然后用`asyncio.run()`调用就一切正常了：
```python
import asyncio


async def coro_func():
    print("In coro_func()")


async def func():
    await coro_func()


asyncio.run(func())
```
运行结果：
```
In coro_func()
```

### 5.3.3 理解协程的关键点
&emsp;&emsp; 协程其实就是用来在单线程中实现并发编程的一种操作。协程由用户决定，在哪些地方交出控制权，切换到下一个任务，在协程中会导致运行权交出的几个操作：
> ① `await coro_func()` 将运行权交给了 `coro_func()`；
> ② `asyncio.sleep(n)` 会将运行权交出，此时`asyncio`事件循环(asyncio event loop)将得到运行权，它会负责调度那些处于`pending`状态的协程。
> 

### 5.3.4 一个协程的例子
这个例子来自于《Fluent Python》第二版，通过它可以更好的理解协程的运行流程。
#### (1) 源码
```python
import itertools
import time
import asyncio


# 1 main()函数是此程序中唯一的普通函数，其它都是协程；
def main() -> None: 
    # 2.1 正如前面所说，asyncio.run()函数适合用在最外层的入口点中；
    # 2.2 main()函数将被一直被阻塞，直到它调用的supervisor()函数返回；
    # 2.3 supervisor()的返回值将成为asyncio.run()的返回值 赋予result变量；
    result = asyncio.run(supervisor()) 
    print(f'Answer: {result}')


async def supervisor() -> int: 
    # 3 create_task()不会阻塞，它会立即返回一个 asyncio.Task对象，此时的 spin('thinking!')
    #  处于 pending状态，等待获取 运行权
    spinner = asyncio.create_task(spin('thinking!')) 
    print(f'spinner object: {spinner}') 

    # 4 await关键字 将调用slow()，并将slow()的返回值赋予result对象
    result = await slow() 

    # 5 Task.cancel()方法 将在 spin()中引发 CancelledError异常；
    spinner.cancel() 
    return result


async def spin(msg: str) -> None: # 
    for char in itertools.cycle(r'\|/-'):
        status = f'\r{char} {msg}'
        print(status, flush=True, end='')
        try:
            # 注意，这里用的是 asyncio.sleep()
            await asyncio.sleep(.1) 
        # CancelledError将在 控制该协程的Task 调用cancel()时被触发
        except asyncio.CancelledError: 
            break
    blanks = ' ' * len(status)
    print(f'\r{blanks}\r', end='')


async def slow() -> int:
    await asyncio.sleep(3) # 11
    return 42


if __name__ == '__main__':
    main()
```
运行结果：
```
spinner object: <Task pending name='Task-2' coro=<spin() running at d:\test.py:19>>
     spin() is starting!
/ thinking! # 这里只会维持3秒，最后会消失；
Answer: 42
```
**上面的代码的运行流程：**
> ① &emsp;&emsp; `main()`函数中的`asyncio.run(supervisor())`将代码的运行权给到了协程`supervisor()`，此时`main()`函数阻塞；
> ② &emsp;&emsp; 此时代码运行到了协程`supervisor()`，`asyncio.create_task(spin('thinking!'))`将`spin('thinking!')`加到了事件循环中等待执行；
> ③ &emsp;&emsp; 协程`supervisor()`中的代码`await slow()`将程序的运行权给到了 协程`slow()`，协程`supervisor()`被阻塞；
> ④ &emsp;&emsp; 协程`slow()`中的`await asyncio.sleep(3)`会导致睡眠3秒，此时 协程`slow()`自动将运行权交出；
> ⑤ &emsp;&emsp; 此时 Task对象 中的任务 `spin('thinking!')` 得到了运行权。
> ⑥ 其它的就比较简单了，略...
> 

#### (2) 若把上面`slow()`中的`await asyncio.sleep(3)`换成`time.sleep(3)`，会发生什么？为什么？
##### ① 会发生什么？
将`slow()`中的`await asyncio.sleep(3)`换成`time.sleep(3)后，运行结果如下：
```
spinner object: <Task pending name='Task-2' coro=<spin() running at d:\test.py:19>>
Answer: 42
```
可以看到的是，下面这个语句不会再出现了
```
/ thinking! # 这里只会维持3秒，最后会消失；
```
##### ② 为什么会这样？
&emsp;&emsp; 这是因为`time.sleep(3)`会导致整个线程被阻塞，这使得处于`pending`状态的协程`spinner`没有得到运行：
```python
async def supervisor() -> int:
    # 1 spinner任务被创建
    spinner = asyncio.create_task(spin('thinking!'))
    # 2 print的结果显示 spinner任务 处于 pending状态
    print(f'spinner object: {spinner}')
    # 3 await把运行权交给了协程slow()
    result = await slow()

    # 5 slow()返回后，处于pending状态的spinner就被cancel了，这就导致协程spinner没有被运行
    spinner.cancel()
    return result


async def slow() -> int:
    # 4 time.sleep(3)将整个主线程阻塞了，OS此时去做其他事了，至少3秒后才会回来
    time.sleep(3)
    # await asyncio.sleep(3)
    return 42
```


#### (3) 

#### (4) 

#### (5) 





&emsp;
## 6 可等待对象(Awaitables)
&emsp;&emsp; The `for` keyword works with iterables. The `await` keyword works with awaitables.
### 6.1 什么样的对象是 可等待对象？
&emsp;&emsp; 如果一个对象可以在 `await` 语句中使用，那么它就是 可等待(awaitable) 对象。

### 6.2 有哪些可等待对象？
可等待 对象有三种主要类型: 
> ① **协程**：
> &emsp;&emsp; 原生协程函数: 定义形式为 `async def` 的函数;
> &emsp;&emsp; 原生协程对象: 调用 协程函数 所返回的对象。
> 
> ② **任务(`asyncio.Task`)** : 
> &emsp;&emsp; 一般由`asyncio.create_task()`创建；
> 
> ③ **future** : 
> 

&emsp;
## 任务对象(task)
Fluent Python 里的原文：
>  You don’t instantiate Task objects yourself, you get them by passing a coroutine to `asyncio.create_task(…)`.
> • When asyncio.create_task(…) returns a Task object, it is already scheduled to run
> 

### TASK何时可以被取消？
Fluent Python 里的原文：
> by definition, a coroutine can only be cancelled when it’s suspended at an await expression, so you can perform cleanup by handling the Cancelled Error exception.
> 


## 等待原语(Waiting Primitives)
### `asyncio.as_completed()`
#### (1) 函数介绍
`asyncio.as_completed(aws, *, timeout=None)`
形参：
> ① `aws`: `aws`应该是一个可迭代对象，该迭代器里面包含的是 可等待(`awaitable`)对象；
> ② `timeout` ： 超时时间，如果在所有 `Future` 对象完成前发生超时则将引发 `asyncio.TimeoutError`。
> ③ 返回值 ： 返回一个 包含协程的迭代器。
> 
作用：
> 类似于`futures.as_completed()`，在对`asyncio.as_completed()`的返回值进行迭代时，它将按 协程的完成顺序返回协程(而不是按协程的提交顺序)。
> 

#### (2) `asyncio.as_completed()`的优势
&emsp;&emsp; `asyncio.as_completed()`的优势就是 在运行多个协程时，可以按 协程的完成的顺序 获取结果，无需按协程提交的顺序获取结果，这和`futures.as_completed()`类似。

#### (3) 实例
```python
import asyncio
import socket
from keyword import kwlist

MAX_KEYWORD_LEN = 4


async def probe(domain: str) -> tuple[str, bool]:
    loop = asyncio.get_running_loop()
    try:
        # socket.getaddrinfo()的异步版本
        await loop.getaddrinfo(domain, None)
    except socket.gaierror:
        return (domain, False)
    return (domain, True)


async def main() -> None:
    names = (kw for kw in kwlist if len(kw) <= MAX_KEYWORD_LEN)
    domains = (f'{name}.dev'.lower() for name in names)

    # 注意，此时coros列表里的协程还没被运行
    coros = [probe(domain) for domain in domains]
    print(coros[:3])

    # as_completed()会并发的运行coros里面的协程，
    ## 另外，as_completed()会按 协程的完成顺序返回
    for coro in asyncio.as_completed(coros):
        domain, found = await coro
        mark = '+' if found else ' '
        print(f'{mark} {domain}')


if __name__ == '__main__':
    asyncio.run(main())
```
运行结果：
```
[<coroutine object probe at 0x00000258AA0F84A0>, <coroutine object probe at 0x00000258AA0FA9D0>, <coroutine object probe at 0x00000258AA0FAA40>]
+ and.dev
+ del.dev
+ in.dev
  pass.dev
  or.dev
+ try.dev
+ as.dev
+ not.dev
+ def.dev
+ true.dev
  else.dev
+ from.dev
  with.dev
  is.dev
  none.dev
  if.dev
+ elif.dev
  for.dev
```
可以看到的是，协程的返回顺序和它们的完成的顺序是一致的。

&emsp;
## `asyncio.Futures`
I removed several paragraphs about asyncio.Futures, which is now considered part of the low-level asyncio APIs

&emsp;
## 事件循环(Event Loop)
[事件循环的官方文档](https://docs.python.org/3/library/asyncio-eventloop.html)

## `asyncio.sleep` 和 `time.sleep` 
### `asyncio.sleep` 和 `time.sleep` 有何不同
它俩都会导致睡眠(被挂起)，它们的区别在于 睡眠(被挂起) 的影响范围。
① `asyncio.sleep`：
> &emsp;&emsp; `asyncio.sleep` 是 协程级 的睡眠，它只会导致调用`asyncio.sleep`的那个协程睡眠(被挂起)，此时其它处于`pending`状态的协程可以获得运行权；
> 
② `time.sleep`：
> &emsp;&emsp; 而`time.sleep`是 系统级 的睡眠，他会导致整个线程睡眠(被挂起)，即使此时程序中有其它处于`pending`状态的协程，这些`pending`的协程也不会获取运行权，因为整个线程(进程)都 睡眠(被挂起)了。
> 
### 应该使用哪个？

> &emsp;&emsp; Never use time.sleep(…) in asyncio coroutines unless you want to pause your whole program. If a coroutine needs to spend some time doing nothing, it should await asyncio.sleep(DELAY). This yields control back to the asyncio event loop, which can drive other pending coroutines.
> &emsp;&emsp; 永远不要在`asyncio`协程中使用`time.sleep(…)`，除非你真想让整个程序阻塞。
> &emsp;&emsp; 若协程想阻塞一会，应该使用`asyncio.sleep(DELAY)`，它会把控制权交给`asyncio`事件循环(asyncio event loop)，此时其它处于`pending`状态的协程就能得到运行权。
> 

### `asyncio.sleep(0)`
&emsp;&emsp; `asyncio.sleep(0)`会使代码运行变慢，能不用就不用。



## asyncio 事件循环(asyncio event loop)



```python
import asyncio


async def fun(a):    # 定义协程函数
    print("\nInside of async def fun()")
    print(a)


# 调用协程函数，生成一个协程对象，此时协程函数并未执行
coroutine = fun('hello world')

# 创建事件循环
loop = asyncio.get_event_loop()

# 将协程函数添加到事件循环，并启动
loop.run_until_complete(coroutine)
```
运行结果：
```
Inside of async def fun()
hello world
```
### 5.4 任务对象`task`
&emsp;&emsp; 协程对象不能直接运行，在注册事件循环的时候，其实是`run_until_complete()`方法将协程包装成为了一个任务（`task`）对象。我们也可以显式实现它，我们有两种方法可以创建`task`对象：
> ① `create_task()`
> ② `asyncio.ensure_future()`
> 
```python
import asyncio


async def fun(a):
    print("\nInside of async def fun()")
    print(a)
    return a


# 方法一： 使用create_task()创建task，并将coroutine对象转化成task对象
coroutine = fun('hello world')
loop = asyncio.get_event_loop()
task = loop.create_task(coroutine)
print(f'task: {task}\n')
loop.run_until_complete(task)
print(f'task: {task}\n')
print(f'result: {task.result()}\n')


# 方法二：使用asyncio.ensure_future() 创建task，并将coroutine对象转化成task对象
coroutine = fun('hello world')
task = asyncio.ensure_future(coroutine)
loop = asyncio.get_event_loop()
print(f'task: {task}\n')
loop.run_until_complete(task)
print(f'task: {task}\n')
```
运行结果：
```
task: <Task pending name='Task-1' coro=<fun() running at d:\code_practice\practice.py:4>>


Inside of async def fun()
hello world
task: <Task finished name='Task-1' coro=<fun() done, defined at d:\code_practice\practice.py:4> result='hello world'>

result: hello world

d:\code_practice\practice.py:22: DeprecationWarning: There is no current event loop
  task = asyncio.ensure_future(coroutine)
d:\code_practice\practice.py:23: DeprecationWarning: There is no current event loop
  loop = asyncio.get_event_loop()
task: <Task pending name='Task-2' coro=<fun() running at d:\code_practice\practice.py:4>>


Inside of async def fun()
hello world
task: <Task finished name='Task-2' coro=<fun() done, defined at d:\code_practice\practice.py:4> result='hello world'>
```

### 5.4 绑定回调函数
&emsp;&emsp; 如果需要在任务(`task`)执行完毕后对结果进行处理，可以通过给`task`对象绑定回调函数完成，回调的最后一个参数是`future`对象（如`task`对象）。
```python
import asyncio


async def fun(a):
    print("\nInside of async def fun()")
    print(a)
    return a


# 回调函数，打印task的返回值
def callback(task): 
    print("\nInside of callback()")
    print(f'result: {task.result()}\n')


coroutine = fun('hello world')
loop = asyncio.get_event_loop()
task = loop.create_task(coroutine)

#绑定回调函数
task.add_done_callback(callback)

print(f'task1: {task}\n')
loop.run_until_complete(task)
print(f'task2: {task}')
```
运行结果：
```
task1: <Task pending name='Task-1' coro=<fun() running at d:\code_practice\practice.py:4> cb=[callback() at d:\code_practice\practice.py:11]>


Inside of async def fun()
hello world

Inside of callback()
result: hello world

task2: <Task finished name='Task-1' coro=<fun() done, defined at d:\code_practice\practice.py:4> result='hello world'> 
```

### 5.5 多任务
&emsp;&emsp; 当需要执行多个任务时，可以定义一个任务列表，并将需要完成的协程任务都加进去，然后将原本的`loop.run_until_complete(tasks)`改为`loop.run_until_complete(asyncio.wait(tasks))`。
&emsp;&emsp; 如果执行的是多个耗时的任务，如网络请求、文件读取等。此时就`await`就派上用场了：
> &emsp;&emsp; `await`可以针对耗时的操作进行挂起，就像生成器里的`yield`一样，函数让出控制权。协程遇到`await`，事件循环将会挂起该协程，执行别的协程，直到其他的协程也挂起或者执行完毕，再进行下一个协程的执行。
> 
```python
import time
import asyncio


async def taskIO_1():   
    print('开始运行IO任务1...')
    await asyncio.sleep(2)  
    print('IO任务1已完成，耗时2s')
    return taskIO_1.__name__


async def taskIO_2():       
    print('开始运行IO任务2...')
    await asyncio.sleep(3)  
    print('IO任务2已完成，耗时3s')
    return taskIO_2.__name__


if __name__ == '__main__':
    start = time.time()
    loop = asyncio.get_event_loop() 
    
    # 建立任务列表
    tasks = [taskIO_1(), taskIO_2()]

    # 完成事件循环，直到最后一个任务结束
    loop.run_until_complete(asyncio.wait(tasks)) 

    
    print(f'所有IO任务总耗时{float(time.time()-start)}秒')
```
运行结果：
```
开始运行IO任务2...
开始运行IO任务1...
IO任务1已完成，耗时2s
IO任务2已完成，耗时3s
所有IO任务总耗时3.008939266204834秒
```
可以看出，原本需要`5`秒，现在执行只需要`3`秒多。

### 5.7 `yield from` 转 `async/await`
&emsp;&emsp; 由于async/await与yield from风格的协程底层实现方式相同。因此，从yield from风格改为async/await风格非常容易。只需：
> ① 把`@asyncio.coroutine`去掉，在协程函数的`def`前面加上`async`；
> ② 把`yield from`替换为`await`。
> 
`async/await`风格的代码隐藏了装饰器、`yield from`语法，方便了人们的理解，同时也让代码更加简洁。
&emsp;&emsp; 对于前面那个通过`@asyncio.coroutine`实现的协程`display_date()`，我们可以用`async`/`await`将其改为：
```python
import asyncio
import datetime
import random

# 差异1：没了装饰器
async def display_date(num, loop): # 差异二：def 前面多了 async
    end_time = loop.time() + 50.0
    while True:
        print('Loop: {} Time: {}'.format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) >= end_time:
            break
        await asyncio.sleep(random.randint(0, 5)) # 差异三：await替代了 yield from 


loop = asyncio.get_event_loop()
asyncio.ensure_future(display_date(1, loop))
asyncio.ensure_future(display_date(2, loop))
loop.run_forever()
```
运行结果：
```
d:\code_practice\practice.py:15: DeprecationWarning: There is no current event loop
  loop = asyncio.get_event_loop()
d:\code_practice\practice.py:16: DeprecationWarning: There is no current event loop
  asyncio.ensure_future(display_date(1, loop))
d:\code_practice\practice.py:17: DeprecationWarning: There is no current event loop
  asyncio.ensure_future(display_date(2, loop))
Loop: 1 Time: 2022-08-15 16:57:13.514031
Loop: 2 Time: 2022-08-15 16:57:13.514031
Loop: 2 Time: 2022-08-15 16:57:15.517992
Loop: 1 Time: 2022-08-15 16:57:16.519159
Loop: 2 Time: 2022-08-15 16:57:16.520156
```
可以看到的是，除了不再提醒`DeprecationWarning`，其它的都一样。




&emsp;
## 6 原生协程 和  基于生成器的协程
&emsp;&emsp; 实际上，除了语法之外 原生协程(`async/await`)和基于生成器的协程(`@asyncio.coroutine/yield from`)并没有功能上的区别。但是注意，这两种写法不能混用，就是说你不能在`generator based coroutines`里使用`await`，或者在`naive coroutines`里头使用`yield`或者`yield from`。
&emsp;&emsp; 除此之外，两种写法是互通的，我们可以同时使用，比如我们可以在原生协程里`await`一个基于生成器的协程，也可以在基于生成器的协程里`yield from`一个使用`async`定义的原生协程。
比如我们同时在一个时间循环里使用两种协程:
```python
import asyncio
import datetime
import random
import types


@types.coroutine
def my_sleep_func():
    yield from asyncio.sleep(random.randint(0, 5))    # 注意这里就不能用 await


async def display_date(num, loop):
    end_time = loop.time() + 50.0
    while True:
        print('Loop: {} Time: {}'.format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) >= end_time:
            break
        yield from my_sleep_func()    # 注意这里就不能用 yield from


loop = asyncio.get_event_loop()
asyncio.ensure_future(display_date(1, loop))
asyncio.ensure_future(display_date(2, loop))
loop.run_forever()
```


&emsp;
## 7 `asyncio`
&emsp;&emsp; `asyncio`是Python 3.4 试验性引入的异步`I/O`框架，提供了基于协程做异步I/O编写单线程并发代码的基础设施。其核心组件有
> 事件循环（Event Loop）
> 协程(Coroutine）
> 任务(Task)
> 未来对象(Future)
> 其他一些扩充和辅助性质的模块
> 





## 实现协程的几种方式
① greenlet。
② yield 关键字
③ asyncio 装饰器（py3.4之后引入）
④ async、await关键字（py3.5之后引入）【推荐】

## 4 python的经典协程
&emsp;&emsp; 在Python2.5引入`send()` `throw()` `close()`后，标志着python支持协程了。最开始的协程是通过生成器（generator）实现的。
### 4.1 用作协程的生成器的基本行为
```python
>>> def simple_coroutine(): # ➊
...     print('-> coroutine started')
...     x = yield # ➋
...     print('-> coroutine received:', x)
...

>>> my_coro = simple_coroutine()
>>> my_coro # ➌
<generator object simple_coroutine at 0x100c2be10>

>>> next(my_coro) # ➍
-> coroutine started

>>> my_coro.send(42) # ➎
-> coroutine received: 42
Traceback (most recent call last): # ➏
  ...
StopIteration
```
> ❶ 协程使用生成器函数定义： 定义体中有 `yield` 关键字。
> ❷ `yield` 在表达式中使用； 如果协程只需从客户那里接收数据， 那么产出的值是 `None`——这个值是隐式指定的， 因为 `yield` 关键字右边没有表达式。
> ❸ 与创建生成器的方式一样， 调用函数得到生成器对象。
> ❹ 首先要调用 `next(...)` 函数， 因为生成器还没启动， 没在 `yield` 语句处暂停， 所以一开始无法发送数据。
> ❺ 调用这个方法后， 协程定义体中的 `yield` 表达式会计算出 `42`； 现在， 协程会恢复， 一直运行到下一个`yield` 表达式， 或者终止。
> ❻ 这里， 控制权流动到协程定义体的末尾， 导致生成器像往常一样抛出 `StopIteration` 异常。

### 4.2 协程的四种状态
&emsp;&emsp; 协程可以身处四个状态中的一个。当前状态可以使用 `inspect.getgeneratorstate(...)`函数确定， 该函数会返回下述字符串中的一个：
> ① `GEN_CREATED` 协程生成器创建完成，等待开始执行。
> ② `GEN_RUNNING` 解释器正在执行。
> ③ `GEN_SUSPENDED` 在`yield`表达式处暂停。
> ④ `GEN_CLOSED`  执行结束。
> 

### 4.3 预激协程(Coroutine Priming)
#### 4.3.1 预激协程
&emsp;&emsp; 如果不预激，那么协程没什么用。调用 `my_coro.send(x)` 之前，记住一定要调用 `next(my_coro)`。

#### 4.3.2 用装饰器来预激协程
为了简化协程的用法，有时会使用一个预激装饰器，比如：
```python
from functools import wraps
```
下面的代码在命令行运行：
```python
def coroutine(func):
    """Decorator: primes `func` by advancing to first `yield`"""
    @wraps(func)
    def primer(*args,**kwargs):
        gen = func(*args,**kwargs)
        next(gen)  #  预激生成器
        return gen # 返回预激的生成器
    return primer


@coroutine # ➎
def averager(): # ➏
    total = 0.0
    count = 0
    average = None
    while True:
        term = yield average
        total += term
        count += 1
        average = total/count
```
在命令行运行一下上面的代码：
```python
'''
    关于 @coroutine装饰器 和 averager()的定义省略...
'''

>>> coro_avg = averager()

>>> coro_avg.send(40) # ➊
40.0

>>> coro_avg.send(50)
45.0

>>> coro_avg.send('spam') # ➋  
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 8, in averager
TypeError: unsupported operand type(s) for +=: 'float' and 'str'

>>> coro_avg.send(60) # ➌
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
```
> ❶ 调用 `averager()` 函数创建一个生成器对象， 在 `coroutine` 装饰器的 `primer` 函数中已经预激了这个生成器。
> ❷ `getgeneratorstate` 函数指明， 处于 `GEN_SUSPENDED` 状态， 因此这个协程已经准备好， 可以接收值了。
> ❸ 可以立即开始把值发给 `coro_avg`——这正是 `coroutine` 装饰器的目的。
> 


### 4.5 图解运行流程
```python
>>> def simple_coro2(a):
...     print('-> Started: a =', a)
...     b = yield a
...     print('-> Received: b =', b)
...     c = yield a + b
...     print('-> Received: c =', c)
...
>>> my_coro2 = simple_coro2(14)
>>> from inspect import getgeneratorstate
>>> getgeneratorstate(my_coro2) # ➊ 
'GEN_CREATED'

>>> next(my_coro2) # ➋
-> Started: a = 14
14

>>> getgeneratorstate(my_coro2) # ➌
'GEN_SUSPENDED'

>>> my_coro2.send(28) # ➍
-> Received: b = 28
42

>>> my_coro2.send(99) # ➎
-> Received: c = 99
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
StopIteration

>>> getgeneratorstate(my_coro2) # ➏
'GEN_CLOSED'
```
> ❶ `inspect.getgeneratorstate` 函数指明， 处于 `GEN_CREATED` 状态（即协程未启动） 。
> ❷ 向前执行协程到第一个 `yield` 表达式， 打印 `-> Started: a = 14` 消息， 然后产出 `a` 的值， 并且暂停， 等待为 `b` 赋值。
> ❸ `getgeneratorstate` 函数指明， 处于 `GEN_SUSPENDED` 状态（即协程在 `yield` 表达式处暂停） 。
> ❹ 把数字 `28` 发给暂停的协程； 计算 `yield` 表达式， 得到 `28`， 然后把那个数绑定给 `b`。 打印 `-> Received: b = 28` 消息， 产出 `a + b` 的值（42） ， 然后协程暂停， 等待为 `c` 赋值。
> ❺ 把数字 `99` 发给暂停的协程； 计算 `yield` 表达式， 得到 `99`， 然后把那个数绑定给 `c`。 打印 `-> Received: c = 99` 消息， 然后协程终止， 导致生成器对象抛出 `StopIteration` 异常。
> ❻ `getgeneratorstate` 函数指明， 处于 `GEN_CLOSED` 状态（即协程执行结束） 。
> 
&emsp;&emsp; 关键的一点是， 协程在 `yield` 关键字所在的位置暂停执行。 前面说过， 在赋值语句中， `=` 右边的代码在赋值之前执行。 因此， 对于 `b = yield a` 这行代码来说， 等到客户端代码再激活协程时才会设定 `b` 的值。这种行为要花点时间才能习惯， 不过一定要理解， 这样才能弄懂异步编程中 yield 的作用（后文探讨） `。simple_coro2` 协程的执行过程分为 `3` 个阶段， 如图下图所示：
> (1) 调用 `next(my_coro2)`， 打印第一个消息， 然后执行 yield a， 产出数字 14。
> (2) 调用 `my_coro2.send(28)`， 把 `28` 赋值给 `b`， 打印第二个消息， 然后执行 `yield a + b`， 产出数字`42`。
> (3) 调用 `my_coro2.send(99)`， 把 `99` 赋值给 `c`， 打印第三个消息， 协程终止。
> 
<div align="center"><img src="./pic/the execution of the simple_coro2 coroutine.png"></div>
&emsp;&emsp; 图：执行 simple_coro2 协程的 3 个阶段（注意， 各个阶段都在 yield 表达式中结束， 而且下一个阶段都从那一行代码开始， 然后再把 yield 表达式的值赋给变量）

**总结一下：每次遇到`yield`就返回到调用者这边**

### 4.6 终止协程和异常处理(Coroutine Termination and Exception Handling)
&emsp;&emsp; 协程中未处理的异常会向上冒泡，传给 `next()` 函数或 `send()` 方法的调用方（即触发协程的对象） 
#### 4.6.1 若不处理协程内部发生的异常，会发生什么？
未处理的异常 **会导致协程终止**：
```python
'''
    关于 @coroutine装饰器 和 averager()的定义省略...
'''

>>> coro_avg = averager()

>>> coro_avg.send(40) # ➊
40.0

>>> coro_avg.send(50)
45.0

>>> coro_avg.send('spam') # ➋
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 8, in averager
TypeError: unsupported operand type(s) for +=: 'float' and 'str'

>>> coro_avg.send(60) # ➌
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
```
> ❶ 使用 `@coroutine` 装饰器装饰的 `averager` 协程， 可以立即开始发送值。
> ❷ 发送的值不是数字， 导致协程内部有异常抛出。
> ❸ 由于在协程内没有处理异常， 协程会终止。 如果试图重新激活协程， 会抛出 StopIteration 异常。
> 

#### 4.6.2 `throw()` 和 `close()`
##### (1)  `throw()` 和 `close()`简介
```python
generator.throw(exc_type[, exc_value[, traceback]])
```
&emsp;&emsp; 致使生成器在暂停的 `yield` 表达式处抛出指定的异常：
> &emsp;&emsp; ① 如果生成器处理了抛出的异常，代码会向前执行到下一个 `yield` 表达式， 而产出的值会成为`generator.throw()`的返回值；
> &emsp;&emsp; ② 如果生成器没有处理抛出的异常，则异常会向上冒泡，传到调用方的上下文中。
> 
```python
generator.close()
```
&emsp;&emsp; 致使生成器在暂停的 yield 表达式处抛出 GeneratorExit 异常。 如果生成器没有处理这个异常， 或者抛出了 StopIteration 异常（通常是指运行到结尾） ， 调用方不会报错。 如果收到 GeneratorExit 异常， 生成器一定不能产出值， 否则解释器会抛出 RuntimeError 异常。 生成器抛出的其他异常会向上冒泡， 传给调用方。
##### (2) `throw()` 和 `close()`的使用示例
```python
class DemoException(Exception):
    """An exception type for the demonstration."""

def demo_exc_handling():
    print('-> coroutine started')
    while True:
        try:
            x = yield
        except DemoException: # 特别处理 DemoException 异常。
            print('*** DemoException handled. Continuing...')
        else: # 如果没有异常， 那么显示接收到的值。
            print('-> coroutine received: {!r}'.format(x))
    raise RuntimeError('This line should never run.')
```
**① 在不使用异常的情况下关闭协程**
```python
>>> exc_coro = demo_exc_handling()
>>> next(exc_coro)
-> coroutine started
>>> exc_coro.send(11)
-> coroutine received: 11
>>> exc_coro.send(22)
-> coroutine received: 22
>>> exc_coro.close()
>>> from inspect import getgeneratorstate
>>> getgeneratorstate(exc_coro)
'GEN_CLOSED'
```

**② 如果异常在协程内部被处理了，则不会导致该协程被中止**
```python
>>> exc_coro = demo_exc_handling()
>>>  next(exc_coro)
  File "<stdin>", line 1
    next(exc_coro)
IndentationError: unexpected indent
>>> next(exc_coro)
-> coroutine started
>>> exc_coro.send(11)
-> coroutine received: 11
>>> exc_coro.throw(DemoException)
*** DemoException handled. Continuing...
>>> getgeneratorstate(exc_coro)
'GEN_SUSPENDED'
```
**③ 如果无法处理传入的异常， 协程会终止**
```python
>>> exc_coro = demo_exc_handling()
>>> next(exc_coro)
-> coroutine started
>>> exc_coro.send(11)
-> coroutine received: 11
>>> exc_coro.throw(ZeroDivisionError)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 5, in demo_exc_handling
ZeroDivisionError
>>> getgeneratorstate(exc_coro)
'GEN_CLOSED'
```
**④ 使用 `try/finally` 块在协程终止时执行操作**
```python
class DemoException(Exception):
    """An exception type for the demonstration."""

def demo_finally():
    print('-> coroutine started')
    try:
        while True:
            try:
                x = yield
            except DemoException:
                print('*** DemoException handled. Continuing...')
            else:
                print('-> coroutine received: {!r}'.format(x))
    finally:
        print('-> coroutine ending')

exc_coro = demo_finally()
next(exc_coro)
exc_coro.send(11)

print("\n" + "*" * 30 + "\n")

exc_coro.throw(ZeroDivisionError)
```
运行结果：
```
-> coroutine started
-> coroutine received: 11

******************************

-> coroutine ending
Traceback (most recent call last):
  File "d:\code_practice\practice.py", line 23, in <module>
    exc_coro.throw(ZeroDivisionError)
  File "d:\code_practice\practice.py", line 9, in demo_finally
    x = yield
ZeroDivisionError
```
可以看到的是，在异常上冒的时候，`finnally`子句被运行了。

### 4.7 从协程返回值(Returning a Value from a Coroutine)
#### 4.7.1 如何从协程返回值
下面是协程`averager`的不同版本，此版会返回结果：
```python
from collections import namedtuple

Result = namedtuple('Result', 'count average')

def averager():
    total = 0.0
    count = 0
    average = None
    while True:
        term = yield
        if term is None:
            break # ➊
        total += term
        count += 1
        average = total/count
    return Result(count, average) # ➋

>>> coro_avg = averager()
>>> next(coro_avg)
>>> coro_avg.send(10)  # ❸
>>> coro_avg.send(30)
>>> coro_avg.send(6.5)
>>> coro_avg.send(None) # ❹
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration: Result(count=3, average=15.5)
```
> ❶ 为了返回值， 协程必须正常终止； 因此， 这一版 `averager` 中有个条件判断， 以便退出累计循环。
> ❷ 返回一个 `namedtuple`， 包含 `count` 和 `average` 两个字段。 注意，在 Python3.3 之前， 如果生成器返回值， 解释器会报句法错误
> ❸ 这一版不产出值。
> ❹ 发送 `None` 会终止循环， 导致协程结束， 返回结果。 一如既往， 生成器对象会抛出 `StopIteration` 异常。 异常对象的 `value` 属性保存着返回的值。
> 
**总结：**
&emsp;&emsp; 从协程中返回值和普通函数一样，直接`return`就行，但是注意， `return` 表达式的值是偷偷传给调用方的：
> 返回值被赋值给 `StopIteration` 异常的一个属性。 
> 
这样做有点不合常理， 但是能保留生成器对象的常规行为——耗尽时抛出 `StopIteration` 异常。那应该怎么正常获取`return`的值呢？

#### 4.7.2 如何获取协程返回的值？
很简单，使用`try/catch`表达式：
```python
"以下代码在命令行执行："
>>> coro_avg = averager()
>>> next(coro_avg)
>>> coro_avg.send(10)  # 
>>> coro_avg.send(30)
>>> coro_avg.send(6.5) # 
>>> try:
...     coro_avg.send(None)
... except StopIteration as exc:
...     result = exc.value
...
>>> result
Result(count=3, average=15.5)
```
**在yield from**被引入前，我们只能使用这种方法获取协程的返回值，但`yield from`被引入后，就不需要这么麻烦了：
> `yield from`结构会在内部自动捕获 `StopIteration` 异常。 这种处理方式与 `for` 循环处理`StopIteration `异常的方式一样： 循环机制使用用户易于理解的方式处理异常。
> 对 `yield from` 结构来说， 解释器不仅会捕获 `StopIteration` 异常， 还会把 `value` 属性的值变成 `yield from` 表达式的值。
> 

&emsp;
### 4.8 在协程中使用`yield from`
####  4.8.1 `yield from`在协程中的作用
**作用一：打开双向通道：**
&emsp;&emsp; 在协程中，`yield from` 的主要功能是打开双向通道， 把最外层的调用方与最内层的子生成器连接起来， 这样二者可以直接发送和产出值， 还可以直接传入异常， 而不用在位于中间的协程中添加大量处理异常的样板代码。有了这个结构， 协程可以通过以前不可能的方式委托职责。
**作用二：以一种更简化的方式获取协程的返回值：**
&emsp;&emsp; 另外，当需要通过 协程返回值时，我们需要通过 异常处理（即使用`try/catch`表达式）来获取协程的返回值，但`yield from` 会捕获 `StopIteration` 异常， 而且还会把 `value` 属性的值变成 `yield from` 表达式的值，换句话说，用`yield from`可以简化代码（不用处理`StopIteration` 异常）。

#### 4.8.2 在协程中使用`yield from` 需要了解的几个概念
##### (1) 几个概念
① **委派生成器(delegating generator)**
> 包含 `yield from <iterable>` 表达式的生成器函数。
> 

② **子生成器(subgenerator)**
> `yield from <iterable>` 表达式中的 `<iterable>` 
> 

③ **调用方(caller)**

#### (2) 委派生成器 的作用是？
主要有两个作用：
> ① 作为“管道” **连接 调用方 和 子生成器**；
> ② 当子生成器结束后，**接收子生成器的返回值**
> 

#### (3) 子生成器 如何 终止？
&emsp;&emsp; 由 调用方 来控制，一般是由调用放`send(None)`，子生成器 收到`None`主动终止，然后返回。

#### 4.8.3 `yield from`使用实例：用 `yield from` 计算平均值并输出统计报告
```python
'coroaverager3.py'

from collections import namedtuple

Result = namedtuple('Result', 'count average')

# 子生成器
def averager(): # ➊
    total = 0.0
    count = 0
    average = None
    while True:
        term = yield # ➋
        if term is None: # ➌
            break
        total += term
        count += 1
        average = total/count
    return Result(count, average) # ➍


# 委派生成器
def grouper(results, key): # ➎
    while True: # ➏
        results[key] = yield from averager() # ➐


# 客户端代码， 即调用方
def main(data): # ➑
    results = {}
    for key, values in data.items():
        group = grouper(results, key) # ➒
        next(group) # ➓
        for value in values:
            group.send(value) # ⓫
        group.send(None) # 重要！ ⓬
    print(results) # 如果要调试， 去掉注释
    report(results)
    

# 输出报告
def report(results):
    for key, result in sorted(results.items()):
        group, unit = key.split(';')
        print('{:2} {:5} averaging {:.2f}{}'.format(
                result.count, group, result.average, unit))

data = {
    'girls;kg':
        [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5],
    'girls;m':
        [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43],
    'boys;kg':
        [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3],
    'boys;m':
        [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],
} 

if __name__ == '__main__':
    main(data)
```
运行结果：
```
 9 boys  averaging 40.42kg
 9 boys  averaging 1.39m
10 girls averaging 42.04kg
10 girls averaging 1.43m
```
**代码解析：**
> ❶ 与示例 16-13 中的 averager 协程一样。 这里作为子生成器使用。
> ❷ main 函数中的客户代码发送的各个值绑定到这里的 term 变量上。
> ❸ 至关重要的终止条件。 如果不这么做， 使用 yield from 调用这个协程的生成器会永远阻塞。
> ❹ 返回的 Result 会成为 grouper 函数中 yield from 表达式的值。
> ❺ grouper 是委派生成器。
> ❻ 这个循环每次迭代时会新建一个 averager 实例； 每个实例都是作为协程使用的生成器对象。
> ❼ grouper 发送的每个值都会经由 yield from 处理， 通过管道传给 averager 实例。 grouper 会在`yield from` 表达式处暂停， 等待 averager 实例处理客户端发来的值。 averager 实例运行完毕后， 返回的值绑定到 results[key] 上。 while 循环会不断创建 averager 实例， 处理更多的值。
> ❽ main 函数是客户端代码， 用 PEP 380 定义的术语来说， 是“调用方”。 这是驱动一切的函数。
> ❾ group 是调用 grouper 函数得到的生成器对象， 传给 grouper 函数的第一个参数是 results， 用于收集结果； 第二个参数是某个键。 group 作为协程使用。
> ❿ 预激 group 协程。
> ⓫ 把各个 value 传给 grouper。 传入的值最终到达 averager 函数中 term = yield 那一行； grouper永远不知道传入的值是什么。
> ⓬ 把 None 传入 grouper， 导致当前的 averager 实例终止， 也让 grouper 继续运行， 再创建一个averager 实例， 处理下一组值。
> 
代码中最后一个标号前面有个注释——“重要！ ”， 强调这行代码（group.send(None)） 至关重要：终止当前的 averager 实例， 开始执行下一个。 如果注释掉那一行， 这个脚本不会输出任何报告。 此时，把 main 函数靠近末尾的 print(results) 那行的注释去掉， 你会发现， results 字典是空的。


#### 4.8.4 调用方、委派生成器 和 子生成器是如何通信的？
&emsp;&emsp; 在每次调用`send(value)`时，`value`不是传递给委派生成器，而是借助`yield from`将`value`传递给了子生成器的`yield`。
&emsp;&emsp; 对于上面用 `yield from` 计算平均值并输出统计报告的脚本`coroaverager3.py`，它的通信示意图如下所示：
<div align="center"><img src="./pic/yield from in coroutine.png"></div>

#### 4.8.5 如果把`coroaverager3.py`中⓬处的`group.send(None)`会发生什么？
##### (1) `group.send(None)`这行代码的作用是什么？
&emsp;&emsp; `group.send(None)`相当于一个终止器，它的作用是 让子生成器终止。
##### (2) 注释掉`group.send(None)`会发生什么？
&emsp;&emsp; 如果注释掉了这行，那子生成器就不会终止，委派生成器会在 `yield from` 表达式处永远暂停。 如果是这样 程序不会向前执行， 因为 `yield from`（与 `yield` 一样） 把控制权转交给客户代码（即， 委派生成器的调用方）了。显然， 肯定有任务无法完成。
让我们把`group.send(None)`注释掉，运行结果如下：
```
{}
```
可以看到的是，程序返回了一个空字典，让我们来分析一下代码的执行过程：
> &emsp;&emsp; 外层 `for` 循环每次迭代会新建一个 `grouper` 实例， 赋值给 `group` 变量； `group` 是委派生成器。
> &emsp;&emsp; 调用 `next(group)`， 预激委派生成器 `grouper`， 此时进入 `while True` 循环， 调用子生成器`averager` 后， 在 `yield from` 表达式处暂停。
> &emsp;&emsp; 内层 `for` 循环调用 `group.send(value)`， 直接把值传给子生成器 `averager`。 同时， 当前的 `grouper`实例（`group`） 在 `yield from` 表达式处暂停。
> &emsp;&emsp; 内层循环结束后， group 实例依旧在 `yield from` 表达式处暂停， 因此， `grouper` 函数定义体中为`results[key]` 赋值的语句还没有执行。
> &emsp;&emsp; 如果外层 `for` 循环的末尾没有 `group.send(None)`， 那么 `averager` 子生成器永远不会终止， 委派生成器 `group` 永远不会再次激活， 因此永远不会为 `results[key]` 赋值。
> &emsp;&emsp; 外层 `for` 循环重新迭代时会新建一个 `grouper` 实例， 然后绑定到 `group` 变量上。 前一个 `grouper` 实例（以及它创建的尚未终止的 `averager` 子生成器实例） 被垃圾回收程序回收。
> 

### 4.8.6 用`yield from`连接多个 委派生成器
&emsp;&emsp; `coroaverager3.py`展示了 `yield from` 结构最简单的用法，只有一个委派生成器和一个子生成器。因为委派生成器相当于管道， 所以可以把任意数量个委派生成器连接在一起： 一个委派生成器使用 `yield from` 调用一个子生成器，而那个子生成器本身也是委派生成器，使用 `yield from` 调用另一个子生成器， 以此类推。最终， 这个链条要以一个只使用 yield 表达式的简单生成器结束； 不过， 也能以任何可迭代的对象结束。
&emsp;&emsp; 另外，任何 `yield from` 链条都必须由客户驱动， 在最外层委派生成器上调用 `next(...)` 函数或 `.send(...)` 方法。 可以隐式调用， 例如使用 `for` 循环。

### 4.8.7 `yield from`在协程中的现状
来看看《Fluent python》第二版的原文：
> &emsp;&emsp; With the advent of native coroutines in Python 3.5, the Python core developers are gradually phasing out support for classic coroutines in asyncio. But the underlying mechanisms are very similar. The async def syntax makes native coroutines easier to spot in code, which is a great benefit. Inside, native coroutines use await instead of yield from to delegate to other coroutines.
> &emsp;&emsp; 随着原生协程(native coroutines)在python3.5的加入，python核心开发人员逐渐不再支持经典协程(classic coroutines)了，但原生协程和经典协程的底层原理其实是一样的。原生协程的语法使得协程更容易被区分出来(这里指的是容易和生成器函数区分开来)。在原生协程中，不再使用`yield from`来委派协程，取而代之的是`await`关键字。
> 
简而言之，在Python3.5后，已经很少使用`yield from`来委派协程了，取而代之的是`await`关键字。

&emsp;
### 4.9 使用`@asyncio.coroutine`来定义协程
#### 4.9.1 `@asyncio.coroutine`是什么？
&emsp;&emsp; `asyncio`是Python 3.4 试验性引入的异步`I/O`框架，`@asyncio.coroutine`是里面一个用来简化定义协程的一个装饰器。

#### 4.9.2 `@asyncio.coroutine`现状
现在已经不推荐使用`@asyncio.coroutine`来定义协程了，在python3.8以后，如果在代码中使用`@asyncio.coroutine`来定义协程，解释器会予以警告：
```python
import asyncio
import datetime
import random


@asyncio.coroutine
def display_date(num, loop):
    end_time = loop.time() + 50.0
    while True:
        print('Loop: {} Time: {}'.format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) >= end_time:
            break
        yield from asyncio.sleep(random.randint(0, 5))


loop = asyncio.get_event_loop()
asyncio.ensure_future(display_date(1, loop))
asyncio.ensure_future(display_date(2, loop))
loop.run_forever()
```
运行结果：
```
d:\code_practice\practice.py:7: DeprecationWarning: "@coroutine" decorator is deprecated since Python 3.8, use "async def" instead
  def display_date(num, loop):
d:\code_practice\practice.py:16: DeprecationWarning: There is no current event loop
  loop = asyncio.get_event_loop()
d:\code_practice\practice.py:17: DeprecationWarning: There is no current event loop
  asyncio.ensure_future(display_date(1, loop))
d:\code_practice\practice.py:18: DeprecationWarning: There is no current event loop
  asyncio.ensure_future(display_date(2, loop))
Loop: 1 Time: 2022-08-15 16:51:52.090880
Loop: 2 Time: 2022-08-15 16:51:52.093877
Loop: 1 Time: 2022-08-15 16:51:52.097863
Loop: 1 Time: 2022-08-15 16:51:53.100349
Loop: 2 Time: 2022-08-15 16:51:54.108288
Loop: 1 Time: 2022-08-15 16:51:54.109292
Traceback (most recent call last): 
    # 此处是CTRL+c将程序中断，要不然就会一直运行
```
代码是在python3.10.0上运行的，可以看到的是，解释器给出了警告：
> DeprecationWarning: "@coroutine" decorator is deprecated since Python 3.8, use "async def" instead
> 自python3.8开始，"@asyncio.coroutine"装饰器就被抵制使用了，而应该使用 "async def"来定义(协程)。
> 


## 参考文献
1. [谈谈Python协程技术的演进](https://zhuanlan.zhihu.com/p/30275154)
2. [Python generators, coroutines, native coroutines and async/await](https://zhuanlan.zhihu.com/p/28334506)
3. [python协程是什么？](https://www.zhihu.com/question/35139020)
4. [出于什么样的原因，诞生了「协程」这一概念？](https://www.zhihu.com/question/50185085/answer/183463734)
5. [深入理解协程（三）：async/await实现异步协程](https://zhuanlan.zhihu.com/p/103315778)
6. [【asyncio专题】生成器(yield与yield from)](https://zhuanlan.zhihu.com/p/452223418)

