- [1. RAII](#1-raii)
  - [用在什么地方了？](#用在什么地方了)
- [2. local copy](#2-local-copy)
- [3.  对象池、弱回调](#3--对象池弱回调)
- [4. 分布式系统中，一般推荐用什么 进程通信 方法? 为什么？](#4-分布式系统中一般推荐用什么-进程通信-方法-为什么)
  - [4.1  用什么？](#41--用什么)
  - [4.2  为什么？](#42--为什么)
- [5. muduo库中为什么不推荐使用 递归mutex?](#5-muduo库中为什么不推荐使用-递归mutex)
  - [5.1 因为 非递归mutex性能好？](#51-因为-非递归mutex性能好)
  - [5.2 那是为什么呢？](#52-那是为什么呢)
  - [5.3 如何解决上面代码中的死锁问题？](#53-如何解决上面代码中的死锁问题)
- [6. 死锁](#6-死锁)
  - [6.2 自己写的时候犯的错误](#62-自己写的时候犯的错误)
- [7 条件变量](#7-条件变量)
  - [7.1 使用建议](#71-使用建议)
  - [7.2 如何确定，`pthread_cond_signal()`和 `pthread_cond_broadcast()`应该使用哪个？](#72-如何确定pthread_cond_signal和-pthread_cond_broadcast应该使用哪个)
- [8. 不要用 读写锁 和 信号量](#8-不要用-读写锁-和-信号量)
  - [8.1 为什么不要用 读写锁？](#81-为什么不要用-读写锁)
  - [8.2 为什么不要用 信号量？](#82-为什么不要用-信号量)
- [9 用RAII封装`mutex`类](#9-用raii封装mutex类)
- [9 封装`Condition`类](#9-封装condition类)
  - [9.2 同时使用`mutex`和`condition`时需要注意什么？](#92-同时使用mutex和condition时需要注意什么)
- [10. 线程安全的`Singleton`实现](#10-线程安全的singleton实现)
- [11. sleep(3) 不是同步原语](#11-sleep3-不是同步原语)
- [12. 借`shared_ptr`实现`copy-on-write (COW)`](#12-借shared_ptr实现copy-on-write-cow)
  - [12.1 如何用 `shared_ptr`实现`copy-on-write`？](#121-如何用-shared_ptr实现copy-on-write)
  - [12.2 用`shared_ptr`实现COW时要注意什么呢？](#122-用shared_ptr实现cow时要注意什么呢)
  - [12.3 copy-on-write 应用：用普通`mutex`替换读写锁的一个例子](#123-copy-on-write-应用用普通mutex替换读写锁的一个例子)
- [13. one loop per thread](#13-one-loop-per-thread)
- [14 如何确定线程池的大小？](#14-如何确定线程池的大小)
- [15. 进程间通信方式的选择](#15-进程间通信方式的选择)
  - [15.1 IPC工具](#151-ipc工具)
  - [15.2 通信协议](#152-通信协议)
  - [15.3 长连接还是短连接？](#153-长连接还是短连接)
- [16. 多线程服务器的适用场合](#16-多线程服务器的适用场合)
  - [16.1 多线程可用的模式 和 优缺点](#161-多线程可用的模式-和-优缺点)
  - [16.2 为什么说 多线程 并不是 万灵丹（silver bullet） ？](#162-为什么说-多线程-并不是-万灵丹silver-bullet-)
  - [16.3 那么究竟什么时候该用 多线程？什么时候该用单线程？](#163-那么究竟什么时候该用-多线程什么时候该用单线程)
    - [16.3.1 何时使用 单线程](#1631-何时使用-单线程)
      - [16.3.2.1 必须使用 单线程的场合](#16321-必须使用-单线程的场合)
      - [16.3.2.2 单线程程序的优缺点](#16322-单线程程序的优缺点)
    - [16.3.2 何时使用 多线程？](#1632-何时使用-多线程)
      - [16.3.2.1 多线程程序是否有性能优势？](#16321-多线程程序是否有性能优势)
      - [16.3.2.2 适用多线程程序的场景](#16322-适用多线程程序的场景)
      - [16.3.2.3 总结](#16323-总结)
  - [16.4 一个多线程程序中，它的线程可以分为哪几类？](#164-一个多线程程序中它的线程可以分为哪几类)
  - [16.5 为什么说，在这个时代，多线程是不可避免的？](#165-为什么说在这个时代多线程是不可避免的)
- [17. Linux上的线程标识](#17-linux上的线程标识)
  - [17.1 Linux有哪些方法可以用来表示一个线程？](#171-linux有哪些方法可以用来表示一个线程)
  - [17.2 书中建议如何标识一个线程？为什么？](#172-书中建议如何标识一个线程为什么)
    - [17.2.1 建议的做法](#1721-建议的做法)
    - [17.2.2 原因](#1722-原因)
    - [17.2.3 如何保证性能？](#1723-如何保证性能)
- [参考文献](#参考文献)





# 1. RAII
## 用在什么地方了？
加锁TODO:





&emsp;
&emsp; 
# 2. local copy






&emsp;
&emsp; 
# 3.  对象池、弱回调
TODO: 后面自己实现一遍





&emsp;
&emsp; 
# 4. 分布式系统中，一般推荐用什么 进程通信 方法? 为什么？
## 4.1  用什么？
用 消息传递(message passing)。

## 4.2  为什么？
&emsp;&emsp; 共享内存只能用于同一个机器上的进程间通信，而消息传递不但可以用于同一机器中的进程间通信，还可以用于不同机器间的通信。在单机上， 我们也可以照搬message passing作为多个进程的并发模型。 这样整个分布式系统的架构的一致性很强， 扩容（scale out） 起来也较容易。





&emsp;
&emsp; 
# 5. muduo库中为什么不推荐使用 递归mutex?
## 5.1 因为 非递归mutex性能好？
&emsp;&emsp; 不是为了性能，而是为了体现设计意图。non-recursive和recursive的性能差别其实不大， 因为少用一个计数器，前者略快一点点而已。

## 5.2 那是为什么呢？
&emsp;&emsp; 毫无疑问recursive mutex使用起来要方便一些， 因为不用考虑一个线程会自己把自己给锁死了， 我猜这也是Java和Windows默认提供recursive mutex的原因。 
&emsp;&emsp; 正因为它方便，recursive mutex可能会隐藏代码里的一些问题。典型情况是你以为拿到一个锁就能修改对象了，没想到外层代码已经拿到了锁，正在修改（或读取） 同一个对象呢。 来看一个具体的例子：
```cpp
class Foo{
public:
    void doit() const;
};

MutexLock mutex;
std::vector<Foo> foos;

void post(const Foo& f){
    MutexLockGuard lock(mutex);
    foos.push_back(f); // 
}

void traverse()
{
    MutexLockGuard lock(mutex);
    for (std::vector<Foo>::const_iterator it = foos.begin(); it != foos.end(); ++it){
        it->doit();
    }
}

void Foo::doit() const{
    Foo f;
    post(f);
}

int main()
{
    Foo f;
    post(f);
    traverse();
}
```
`post()`加锁， 然后修改`foos`对象； `traverse()`加锁， 然后遍历`foos`向量。 这些都是正确的。
将来有一天， `Foo::doit()`间接调用了`post()`， 那么会很有戏剧性的结果：
> (1) mutex是非递归的， 于是死锁了。
> (2) mutex是递归的， 由于`push_back()`可能（但不总是）导致`vector迭代器`失效（`push_back()`导致内存重新分配后，迭代器会失效），程序偶尔会crash。
> 
这时候就能体现non-recursive的优越性：把程序的逻辑错误暴露出来。死锁比较容易debug，把各个线程的调用栈打出来，只要每个函数不是特别长，很容易看出来是怎么死的， 见§2.1.2的例子 9。或者可以用`PTHREAD_MUTEX_ERRORCHECK`一下子就能找到错误（前提是MutexLock带debug选项）。程序反正要死，不如死得有意义一点，留个“全尸”，让验尸（post-mortem）更容易些。
&emsp;&emsp; **换句话来说就是：使用非递归mutex可以尽早的将问题暴露出来，这样不容易在生产环境出问题。**

## 5.3 如何解决上面代码中的死锁问题？
可以，用`shared_ptr`实现copy-on-write，这样就能解决，代码如下：
```cpp
class Foo
{
public:
    void doit() const;
};

typedef std::vector<Foo> FooList;
typedef boost::shared_ptr<FooList> FooListPtr;
FooListPtr g_foos;
MutexLock mutex;

void post(const Foo& f)
{
    printf("post\n");
    MutexLockGuard lock(mutex);

    // 理论上 全局变量 g_foos 应该只有一个拥有者，如果不只一个，
    // 那么应该就是在 traverse()中正在遍历
    if (!g_foos.unique())
    {
        // 因为有其它线程在遍历 g_foos，为了不破坏 g_foos的迭代器
        // 直接拷贝一份
        g_foos.reset(new FooList(*g_foos));
        printf("copy the whole list\n");
    }

    // 对拷贝后的 g_foos 进行修改，不会影响 traverse()
    assert(g_foos.unique());
    g_foos->push_back(f);
}

void traverse()
{
    FooListPtr foos;
    {
        // 锁是加在这个局部作用域的，出了花括号的范围就解锁了，
        // 后面的遍历是没有锁的
        MutexLockGuard lock(mutex);
        foos = g_foos;
        assert(!g_foos.unique());
    }

    // assert(!foos.unique()); this may not hold

    for (std::vector<Foo>::const_iterator it = foos->begin();
        it != foos->end(); ++it)
    {
        it->doit();
    }
}

void Foo::doit() const
{
    Foo f;
    post(f);
}

int main()
{
    g_foos.reset(new FooList);
    Foo f;
    post(f);
    traverse();
}
```






&emsp;
&emsp; 
# 6. 死锁
TODO: 只是自己实现了一下第一个程序，后面的调试涉及到`gdb`，后面学了之后再回来！
```cpp
pthread_mutex_t mtx;
pthread_mutexattr_t mtxAttr;

class Request{
public:
    void process(){
        int ret = pthread_mutex_lock(&mtx);
        print();
        cout << "process() : " << ret << endl;
        ret = pthread_mutex_unlock(&mtx);
    }

    void print(){
        int ret = pthread_mutex_lock(&mtx);
        ret = pthread_mutex_unlock(&mtx);
        cout << "The end of print() :" << ret << endl;
    }
};


int main()
{
    int s = pthread_mutexattr_init(&mtxAttr);
    // 不依赖默认属性，直接指定mutex类型为 PTHREAD_MUTEX_NORMAL
    s = pthread_mutexattr_settype(&mtxAttr, PTHREAD_MUTEX_NORMAL);
    s = pthread_mutex_init(&mtx, &mtxAttr);
    Request obj;
    obj.process();
}
```

## 6.2 自己写的时候犯的错误
(1) 编译的时候忘记加 `-pthread`选项
正确的应该是：
```shell
g++ -o test.o test.cpp --std=c++11 -pthread
```





&emsp;
&emsp; 
# 7 条件变量
## 7.1 使用建议
**对于wait端：**
> (1) 必须与mutex一起使用， 该布尔表达式的读写需受此mutex保护。
> (2) 在mutex已上锁的时候才能调用wait()。
> (3) 把判断布尔条件和wait()放到while循环中。
>
对于signal/broadcast端：
> (1) 不一定要在mutex已上锁的情况下调用signal（理论上） 。
> (2) 在signal之前一般要修改布尔表达式。
> (3) 修改布尔表达式通常要用mutex保护（至少用作full memory barrier） 。
> (4) 注意区分signal与broadcast：“broadcast通常用于表明状态变化， signal通常用于表示资源可用。
> 

## 7.2 如何确定，`pthread_cond_signal()`和 `pthread_cond_broadcast()`应该使用哪个？
(1) 下面情况适合用`pthread_cond_broadcast`：
> 一个生产者多消费者，生产者能一次产生多个产品的情况。
> 多生产者多消费者
> 读写锁实现（写入之后，通知所有读者）
> 
(2) 下面情况适合`pthread_cond_signal`的情况
> 单一生产者，生产者一次生产一个产品的情况，最好一个消费者
> 
**在muduo库2.2小节中，有这样一段话：**
> 注意区分signal与broadcast： “broadcast通常用于表明状态变化， signal通常用于表示资源可用。（broadcast should generally be used to indicate state change rather than resource availability。 ）
> 
对于这段话其实可以这么理解：
> 资源被释放时，一般只能同时供一个线程使用，因此用signal通知一个线程即
>






&emsp;
&emsp; 
# 8. 不要用 读写锁 和 信号量
## 8.1 为什么不要用 读写锁？
&emsp;&emsp; 
**从正确性方面来说**，一种典型的易犯错误是在持有read lock的时候修改了共享数据。 这通常发生在程序的维护阶段， 为了新增功能， 程序员不小心在原来read lock保护的函数中调用了会修改状态的函数。 这种错误的后果跟无保护并发读写共享数据是一样的。
**从性能方面来说**，读写锁不见得比普通mutex更高效。 无论如何reader lock加锁的开销不会比mutex lock小，因为它要更新当前reader的数目。如果临界区很小，锁竞争不激烈，那么mutex往往会更快。

## 8.2 为什么不要用 信号量？
&emsp;&emsp; 笔者说：我没有遇到过需要使用信号量的情况，无从谈及个人经验。我认为信号量不是必备的同步原语，因为条件变量配合互斥器可以完全替代其功能，而且更不易用错。除了[RWC]指出的“semaphore has no notion of ownership”之外，信号量的另一个问题在于它有自己的计数值，而通常我们自己的数据结构也有长度值，这就造成了同样的信息存了两份，需要时刻保持一致，这增加了程序员的负担和出错的可能。如果要控制并发度，可以考虑用`muduo::ThreadPool`。






&emsp;
&emsp; 
# 9 用RAII封装`mutex`类
有几点要注意：
(1) 不应该使用系统默认的互斥锁类型（即`PTHREAD_MUTEX_DEFAULT`），而应该用`mutexattr`来显示指定mutex的类型（这里是`PTHREAD_MUTEX_NORMAL`）。（在本文前面关于 [死锁]()的介绍中就是这么做的 ）






&emsp;
&emsp; 
# 9 封装`Condition`类
TODO: 代码分析

## 9.2 同时使用`mutex`和`condition`时需要注意什么？
&emsp;&emsp; 注意它们的声明顺序和初始化顺序， mutex_应先于condition_构造， 并作为后者的构造参数。
&emsp;&emsp; 因为成员变量在类中的声明次序 就是 其在初始化列表中的初始化顺序，与其在初始化列表中的先后次序无关。






&emsp;
&emsp; 
# 10. 线程安全的`Singleton`实现







&emsp;
&emsp; 
# 11. sleep(3) 不是同步原语
&emsp;&emsp; `sleep()/usleep()/nanosleep()`只能出现在测试代码中 比如写单元测试的时候；或者用于有意延长临界区，加速复现死锁的情况。







&emsp;
&emsp; 
# 12. 借`shared_ptr`实现`copy-on-write (COW)`
## 12.1 如何用 `shared_ptr`实现`copy-on-write`？
概括的来说，**COW技术的精髓是**：
> (1) 如果你是数据的唯一拥有者，那么你可以直接修改数据。
> (2) 如果你不是数据的唯一拥有者，那么你拷贝它之后再修改。
> 
用`shared_ptr`来实现COW时，主要考虑两点：
> (1) 读数据
> (2) 写数据
> 
&emsp;&emsp; `shared_ptr`拥有对对象的引用计数，在对对象进行读写操作时，这个计数是`1`，当读数据时，我们 创建一个新的智能指针 指向 原智能指针指向的对象，这个时候引用计数加`1`（变为`2`），这么做其实就是为了提醒其它线程，这个对象还有其他人持有，你别乱改。
&emsp;&emsp; **概括的来说**，就是通过新建一个 局部`shared_ptr` 指向共享的那个 全局`shared_ptr`所指向的对象，让其引用计数增加，这样其它线程就能知道你在使用该对象，其它线程就会 “自觉地” 拷贝后再修改：
```cpp
//假设g_ptr是一个全局的shared_ptr<Foo>并且已经初始化。
void read()
{
    shared_ptr<Foo> tmpptr; // 新建一个 局部shared_ptr
    {
        lock(); // 加锁
        /* 此时引用计数为2，这样写端就能知道有其它线程持有了g_ptr，它就不会直接修改g_ptr了
         而是会复制一份，然后修改复制的那一份，具体可以看后面 写端的实现*/
        // 引用计数加1是为了提醒其它线程，这个对象还有其他人持有，你别乱改
        tmpptr=g_ptr; // 局部智能指针tmpptr 和 全局智能指针g_ptr指向同一对象，它俩的引用计数都是2
    }
    // 注意 tmpptr指向的对象的引用计数还是为2哦，因为tmpptr还没被释放
    //访问tmpptr
    //...

} // 离开read()，tmpptr被释放，那么引用计数就减1
```
这部分是shared_ptr最基本的用法，还是很好理解的，`read()`函数调用结束，`tmpptr`作为栈上变量离开作用域，自然析构，原数据对象的引用计数也变为1。
&emsp;&emsp; 写数据就复杂一些。根据COW的准则，当你是唯一拥有者（对应对象的引用计数是1）时，那么你直接修改数据，这样没有问题，当你不是唯一拥有者，则需要拷贝数据再去修改，这就需要用到一些shared_ptr的编程技法了：
```cpp
void write()
{
    // 写端要在一开始就加锁，要不然多个线程并发write()时会导致，某些线程写入失效。
    lock() 
    if(!g_ptr.unique())
    {
        g_ptr.reset(new Foo(*g_ptr));
    }
    assert(g_ptr.unique());
    // write
    // 
}
```
解释一下代码：
`shared_ptr::unique()`：当引用计数为1时返回true，否则false。
那么当引用计数不为1的时候，说明有别的线程正在读，受shread_ptr::reset()中example的误导，一直以为，reset后，原对象被析构，这样不就会影响正在读的线程了吗？
**解释一下`write()`中的`g_ptr.reset(new Foo(*g_ptr));`**
&emsp;&emsp; 假设在同一时刻，有一个读线程A，一个写线程B，当写线程B进入到`if`循环中时，原对象的引用计数为2（写线程A也持有了`g_ptr`指向的数据，导致引用计数加1），分别为`tmpptr`（线程B）和`g_ptr`，此时`reset()`函数将原对象的引用计数减1，并且`g_ptr`已经指向了新的对象（用原对象构造），这样就完成了数据的拷贝，并且原对象还在，只是引用计数变成了1，只有在读端线程A释放了`tmpptr`后才会导致旧的数据被释放。

## 12.2 用`shared_ptr`实现COW时要注意什么呢？
read端：
> 锁只需在拷贝的时候加，最好使用local copy，或者新建一个函数来完成拷贝也是可以的（就像后面的`CustomerData::query`的第一行代码那样）。
> 
write端：
> 锁需要在一开始就加上，因为我们是需要修改共享的资源的，所以我们需要对整个函数加锁。
> 

## 12.3 copy-on-write 应用：用普通`mutex`替换读写锁的一个例子
**场景**： 一个多线程的C++程序，24h x 5.5d运行。有几个工作线程ThreadWorker{0, 1, 2, 3}， 处理客户发过来的交易请求； 另外有一个背景线程ThreadBackground， 不定期更新程序内部的参考数据。 这些线程都跟一个hash表打交道， 工作线程只读， 背景线程读写， 必然要用到一些同步机制， 防止数据损坏。 这里的示例代码用`std::map`代替`hash`表， 意思是一样的：
```cpp
typedef std::pair<string, int> Entry;
typedef std::vector<Entry> EntryList;
typedef std::map<string, EntryList> Map;
```
`Map`的`key`是用户名， `value`是一个`vector`， 里边存的是不同`stock`的最小交易间隔， `vector`已经排好序， 可以用二分查找。
&emsp;&emsp; 我们的系统要求工作线程的延迟尽可能小，可以容忍背景线程的延迟略大。一天之内，背景线程对数据更新的次数屈指可数，最多一小时一次，更新的数据来自于网络，所以对更新的及时性不敏感。Map的数据量也不大，大约一千多条数据。
&emsp;&emsp; 最简单的同步办法是用读写锁：工作线程加读锁，背景线程加写锁。但是读写锁的开销比普通mutex要大，而且是写锁优先，会阻塞后面的读锁。如果工作线程能用最普通的非重入`mutex`实现同步，就不必用读写锁，这能降低工作线程延迟。我们借助`shared_ptr`做到了这一点： 
```cpp
class CustomerData : boost::noncopyable
{
public:
    CustomerData()
        : data_(new Map)
    { }

    int query(const string& customer, const string& stock) const;

private:
    typedef std::pair<string, int> Entry;
    typedef std::vector<Entry> EntryList;
    typedef std::map<string, EntryList> Map;
    typedef boost::shared_ptr<Map> MapPtr;
    void update(const string& customer, const EntryList& entries);
    void update(const string& message);

    static int findEntry(const EntryList& entries, const string& stock);
    static MapPtr parseData(const string& message);

    MapPtr getData() const
    {
        muduo::MutexLockGuard lock(mutex_);
        return data_;
    }

    mutable muduo::MutexLock mutex_;
    MapPtr data_;
};
```
`CustomerData::query()`相当于读端，用了前面说的引用计数加`1`的办法， 用局部`MapPtr data`变量来持有`Map`，防止并发修改：
```cpp
int CustomerData::query(const string& customer, const string& stock) const
{
    // getData() 返回 data_的拷贝，这样管理data_的shared_ptr的引用计数就
    // 不再为1，而是2，如果写端(这里是背景线程)同时在操作data_，
    // 它就能知道有人持有 _data，它会复制一份再修改。
    MapPtr data = getData();

    Map::const_iterator entries = data->find(customer);
    if (entries != data->end())
        return findEntry(entries->second, stock);
    else
        return -1;
}
```
&emsp;&emsp; 关键看`CustomerData::update()`（写端）怎么写。既然要更新数据，那肯定得加锁，如果这时候其他线程正在读， 那么不能在原来的数据上修改， 得创建一个副本，在副本上修改，修改完了再替换。如果没有用户在读， 那么就能直接修改，节约一次Map拷贝。
&emsp;&emsp; 注意其中用了`shared_ptr::unique()`来判断是不是有人在读， 如果有人在读， 那么我们不能直接修改， 因为`query()`并没有全程加锁， 只在`getData()`内部有锁。`shared_ptr::swap()`把`data_`替换为新副本， 而且我们还在锁里，不会有别的线程来读， 可以放心地更新。如果别的reader线程已经刚刚通过`getData()`拿到了`MapPtr`，它会读到稍旧的数据。这不是问题，因为数据更新来自网络，如果网络稍有延迟， 反正reader线程也会读到旧的数据。
```cpp
// 每次收到一个 customer 的数据更新
void CustomerData::update(const string& customer, const EntryList& entries)
{
    // 注意，写端要在最外层加锁，要不然多个线程同时update()会导致某些线程修改失败
    muduo::MutexLockGuard lock(mutex_); //
    if (!data_.unique()) // 如果本线程不是 data_ 的唯一持有者，那就复制一份再修改
    {
        // 复制一份
        MapPtr newData(new Map(*data_)); 
        data_.swap(newData);
    }
    assert(data_.unique());
    (*data_)[customer] = entries;
}

void CustomerData::update(const string& message)
{
    MapPtr newData = parseData(message);
    if (newData)
    {
        muduo::MutexLockGuard lock(mutex_);
        data_.swap(newData);
    }
}
```





&emsp;
&emsp;
&emsp; 
# 13. one loop per thread 
这是muduo库使用的 C++多线程服务端编程模式， `one (event) loop per thread `+ `thread pool`
> `event loop`（也叫IO loop） 用作IO multiplexing，配合 non-blocking IO和定时器。
> `thread pool` 用来做计算， 具体可以是任务队列或生产者消费者队列。
> 

.TODO:不太明白这到底是是指啥，学了源码再回来看






&emsp;
&emsp;
&emsp; 
# 14 如何确定线程池的大小？
&emsp;&emsp; 程序里具体用几个loop、线程池的大小等参数需要根据应用来设定，基本的原则是“**阻抗匹配**”，使得CPU和IO都能高效地运作。
&emsp;&emsp; 如果池中线程在执行任务时，密集计算所占的时间比重为`P（0＜P≤1）`，而系统一共有`C`个CPU，为了让这`C`个CPU跑满而又不过载，线程池大小的经验公式`T＝C/P`。`T`是个hint，考虑到`P`值的估计不是很准确，`T`的最佳值可以上下浮动50％.这个经验公式的原理很简单，`T`个线程，每个线程占用`P`的CPU时间， 如果刚好占满`C`个CPU，那么必有`T×P＝C`。下面验证一下边界条件的正确性。
&emsp;&emsp; 假设`C＝8， P＝1.0`，线程池的任务完全是密集计算，那么`T＝8`。只要8个活动线程就能让8个CPU饱和，再多也没用，因为CPU资源已经耗光了。
&emsp;&emsp; 假设`C＝8， P＝0.5`，线程池的任务有一半是计算，有一半等在IO上，那么`T＝16`。考虑操作系统能灵活、合理地调度sleeping/writing/running线程，那么大概16个“50％繁忙的线程”能让8个CPU忙个不停。启动更多的线程并不能提高吞吐量，反而因为增加上下文切换的开销而降低性能。
&emsp;&emsp; 如果`P＜0.2`，这个公式就不适用了，`T`可以取一个固定值，比如`5×C`。另外，公式里的`C`不一定是CPU总数，可以是“分配给这项任务的CPU数目”，比如在8核机器上分出4个核来做一项任务，那么`C＝4`。
&emsp;&emsp; 此外，程序里或许还有个别执行特殊任务的线程，比如logging，这对应用程序来说基本是不可见的，但是在分配资源（CPU和IO）的时候要算进去，以免高估了系统的容量。






&emsp;
&emsp;
&emsp; 
# 15. 进程间通信方式的选择
## 15.1 IPC工具
&emsp;&emsp; 进程间通信我首选socket（主要指TCP），其最大的好处在于：可以跨主机，具有伸缩性。反正都是多进程了，如果一台机器的处理能力不够，很自然地就能用多台机器来处理。把进程分散到同一局域网的多台机器上，程序改改`host:port`配置就能继续用。相反，其他IPC都不能跨机器，这就限制了scalability。

## 15.2 通信协议
使用TCP协议，它有如下好处：
> ① 没有资源泄露的风险：TCP port由一个进程独占，且操作系统会自动回收（listening port和已建立连接的TCP socket都是文件描述符， 在进程结束时操作系统会关闭所有文件描述符） 。 这说明， 即使程序意外退出， 也不会给系统留下垃圾， 程序重启之后能比较容易地恢复， 而不需要重启操作系统（用跨进程的mutex就有这个风险）
> ② 既然port是独占的，那么可以防止程序重复启动，后面那个进程抢不到port，自然就没法初始化了，避免造成意料之外的结果。
> ③ 方便分析：tcpdump和Wireshark是解决两个进程间协议和状态争端的好帮手， 也是性能（吞吐量、 延迟） 分析的利器。 我们可以借此编写分布式程序的自动化回归测试。 也可以用tcpcopy 14之类的工具进行压力测试
> ④ TCP还能跨语言， 服务端和客户端不必使用同一种语言。 试想如果用共享内存作为IPC， C++程序如何与Java通信， 难道用JNI吗？
> 

## 15.3 长连接还是短连接？
建议使用长连接，它有如下好处：
> ① 容易定位分布式系统中的服务之间的依赖关系。 只要在机器上运`行netstat -tpna | grep :port`就能立刻列出用到某服务的客户端地址（Foreign列） ， 然后在客户端的机器上用`netstat`或`lsof`命令找出是哪个进程发起的连接。 这样在迁移服务的时候能有效地防止出现outage。 TCP短连接和UDP则不具备这一特性。 
> ② 通过接收和发送队列的长度也较容易定位网络或程序故障。在正常运行的时候，`netstat`打印的`Recv-Q`和`Send-Q`都应该接近0，或者在0附近摆动。 如果`Recv-Q`保持不变或持续增加，则通常意味着服务进程的处理速度变慢，可能发生了死锁或阻塞。如果`Send-Q`保持不变或持续增加，有可能是对方服务器太忙、来不及处理，也有可能是网络中间某个路由器或交换机故障造成丢包，甚至对方服务器掉线，这些因素都可能表现为数据发送不出去。通过持续监控`Recv-Q`和`Send-Q`就能及早预警性能或可用性故障。 
> 






&emsp;
&emsp;
&emsp; 
# 16. 多线程服务器的适用场合
## 16.1 多线程可用的模式 和 优缺点
如果要在一台多核机器上提供一种服务或执行一个任务，可用的模式有： 
> (1) 运行 一个 单线程的进程；（启动一个进程A，该进程只含有一个线程）
> (2) 运行 一个 多线程的进程；（启动一个进程A，该进程中包含多个线程）
> (3) 运行 多个 单线程的进程；(启动多个进程，A B C...，这些进程各自都只包含一个线程)
> &emsp;&emsp; a 简单地把模式(1)中的进程运行多份 
> &emsp;&emsp; b 主进程+woker进程，如果必须绑定到一个TCP port，比如httpd+fastcgi
> (4) 运行 多个 多线程的进程。(启动多个进程，A B C...，而且这些进程都包含多个线程)
> 
模式(1)是不可伸缩的（scalable） ， 不能发挥多核机器的计算能力。
模式(2)是被很多人所鄙视的， 认为多线程程序难写， 而且与模式(3)相比并没有什么优势。
模式(3)是目前公认的主流模式。 它有以下两种子模式：
> a 简单地把模式(1)中的进程运行多份 
> b 主进程+woker进程， 如果必须绑定到一个TCP port， 比如httpd+fastcgi
> 
模式(4)更是千夫所指， 它不但没有结合2和3的优点， 反而汇聚了二者的缺点

## 16.2 为什么说 多线程 并不是 万灵丹（silver bullet） ？
&emsp;&emsp; Paul E. McKenney在《Is Parallel Programming Hard, And, If So, What Can You Do About It?》 17第3.5节指出：
> As a rough rule of thumb, use the simplest tool that will get the job done.
> rule of thumb：(根据实际经验的)粗略估算;经验之谈; 经验法则
> 
比方说，使用速率为50MB/s的数据压缩库、在进程创建销毁的开销是800μs、线程创建销毁的开销是50μs的前提下， 考虑如何执行压缩任务：
* &emsp;&emsp; 如果要偶尔压缩1GB的文本文件，预计运行时间是20s，那么起一个进程去做是合理的，因为进程启动和销毁的开销远远小于实际任务的耗时。
* &emsp;&emsp; 如果要经常压缩500kB的文本数据，预计运行时间是10ms，那么每次都起进程似乎有点浪费了，可以每次单独起一个线程去做。
* &emsp;&emsp; 如果要频繁压缩10kB的文本数据，预计运行时间是200μs，那么每次起线程似乎也很浪费，不如直接在当前线程搞定。也可以用一个线程池，每次把压缩任务交给线程池，避免阻塞当前线程（特别要避免阻塞IO线程）。

由此可见，多线程并不是万灵丹（silver bullet），它有适用的场合，也有不适合的场合。

## 16.3 那么究竟什么时候该用 多线程？什么时候该用单线程？
### 16.3.1 何时使用 单线程
#### 16.3.2.1 必须使用 单线程的场合
一般来说，有两种场合必须使用单线程：
> (1) 程序可能会`fork()`,多线程程序不是不能调用fork(2)， 而是这么做会遇到很多麻烦；
> (2) 限制程序的CPU占用率。
> 
<span style="color:red; font-size:18px; font-weight:bold"> (1) 程序可能会fork() </span>

多线程程序不是不能调用`fork()`， 而是这么做会遇到很多麻烦；一个程序`fork()`之后一般有两种行为：
> ① 立刻执行`exec()`，变身为另一个程序。例如`shell`和`inetd`；又比如`lighttpd` `fork()`出子进程， 然后运行`fastcgi`程序。 或者集群中运行在计算节点上的负责启动`job`的守护进程（即我所谓的“看门狗进程”） 。
> ② 不调用`exec()`，继续运行当前程序。要么通过共享的文件描述符与父进程通信， 协同完成任务；要么接过父进程传来的文件描述符，独立完成工作，例如20世纪80年代的Web服务器`NCSA httpd`。
>
这些行为中，我认为只有“看门狗进程”必须坚持单线程，其他的均可替换为多线程程序（从功能上讲）。
&emsp;
<span style="color:red; font-size:18px; font-weight:bold"> (2) 限制程序的CPU占用率 </span>

&emsp;&emsp; 这个很容易理解，比如在一个8核的服务器上，一个单线程程序即便发生busy-wait（无论是因为bug，还是因为overload），占满1个core，其CPU使用率也只有`12.5％`。在这种最坏的情况下，系统还是有`87.5％`的计算资源可供其他服务进程使用。
&emsp;&emsp; 因此对于一些辅助性的程序，如果它必须和主要服务进程运行在同一台机器的话（比如它要监控其他服务进程的状态），那么**做成单线程的能限制该程序的CPU占用频率，避免其过分抢夺系统的计算资源**。比方说：
> &emsp;&emsp; 如果要把生产服务器上的日志文件压缩后备份到NFS上，那么应该使用普通单线程压缩工具（gzip/bzip2）。它们对系统造成的影响较小，在8核服务器上最多占满1个core。如果有人为了“提高速度”，开启了多线程压缩或者同时起多个进程来压缩多个日志文件， 有可能造成的结果是非关键任务耗尽了CPU资源，正常客户的请求响应变慢。这是我们不愿意看到的。

#### 16.3.2.2 单线程程序的优缺点
<span style="color:red; font-size:18px; font-weight:bold"> 优点： </span>

&emsp;&emsp; 从编码的角度，单线程程序的优势无须赘言： 编写起来简单。比如说event loop

<span style="color:red; font-size:18px; font-weight:bold"> 缺点： </span>

&emsp;&emsp; event loop 有一个明显的缺点，它是非抢占的（non-preemptive）。假设事件a的优先级高于事件b，处理事件a需要1ms，处理事件b需要10ms。如果事件b稍早于a发生，那么当事件a到来时，程序已经离开了`poll()`调用，并开始处理事件b。事件a要等上10ms才有机会被处理，总的响应时间为11ms。这等于发生了优先级反转。这个缺点可以用多线程来克服，这也是多线程的主要优势。

### 16.3.2 何时使用 多线程？
#### 16.3.2.1 多线程程序是否有性能优势？
&emsp;&emsp; 无论是`IO bound`还是`CPU bound`的服务，多线程都 没有绝对意义上的 性能优势。这句话是说：
> **如果用很少的CPU负载就能让IO跑满，或者用很少的IO流量就能让CPU跑满，那么多线程没啥用处。**
> 
举例来说：
* &emsp;&emsp; 对于静态Web服务器，或者FTP服务器，CPU的负载较轻，主要瓶颈在磁盘IO和网络IO方面。这时候往往一个单线程的程序（模式1） 就能撑满IO。 用多线程并不能提高吞吐量， 因为IO硬件容量已经饱和了。 同理， 这时增加CPU数目也不能提高吞吐量。
* &emsp;&emsp; CPU跑满的情况比较少见，这里我只好虚构一个例子。假设有一个服务，它的输入是n个整数，问能否从中选出m个整数，使其和为0（这里n＜100, m＞0）。这是著名的subset sum问题，是NP-Complete的。对于这样一个“服务”，哪怕很小的n值也会让CPU算死。比如n＝30，一次的输入不过200字节（32-bit整数），CPU的运算时间却能长达几分钟。 对于这种应用， 模式3a是最适合的，能发挥多核的优势， 程序也简单。
* 
也就是说， 无论任何一方早早地先到达瓶颈， 多线程程序都没啥优势。

#### 16.3.2.2 适用多线程程序的场景
&emsp;&emsp; **多线程的适用场景是**：提高响应速度，让IO和“计算”相互重叠，降低latency。虽然多线程不能提高绝对性能，但能提高平均响应性能。
一个程序要做成多线程的， 大致要满足：
  * (1) 有多个CPU可用。单核机器上多线程没有性能优势（但或许能简化并发业务逻辑的实现） 。
  * (2) 线程间有共享数据，即内存中的全局状态。如果没有共享数据，用模型`(3)b`就行。 虽然我们应该把线程间的共享数据降到最低，但不代表没有。
  * (3) 共享的数据是可以修改的， 而不是静态的常量表。 如果数据不能修改， 那么可以在进程间用shared memory， 模式(3)就能胜任。
  * (4) 提供非均质的服务。即，事件的响应有优先级差异，我们可以用专门的线程来处理优先级高的事件。防止优先级反转。
  * (4) latency和throughput同样重要，不是逻辑简单的IO bound或CPU bound程序。换言之，程序要有相当的计算量。
  * (5) 利用异步操作。比如logging。无论往磁盘写log file，还是往log server发送消息都不应该阻塞critical path。
  * (6) 能scale up。一个好的多线程程序应该能享受增加CPU数目带来的好处，目前主流是8核，很快就会用到16核的机器了。
  * (7) 具有可预测的性能。随着负载增加，性能缓慢下降，超过某个临界点之后会急速下降。线程数目一般不随负载变化。
  * (8) 多线程能有效地划分责任与功能，让每个线程的逻辑比较简单，任务单一，便于编码。而不是把所有逻辑都塞到一个event loop里，不同类别的事件之间相互影响。
  
上面这些条件比较抽象，这里举两个具体的（虽然是虚构的） 例子
<span style="color:red;  font-weight:bold"> 例一：Linux服务器机群管理程序 </span>

&emsp;&emsp; 假设要管理一个Linux服务器机群， 这个机群里有8个计算节点，1个控制节点。机器的配置都是一样的：双路四核CPU，千兆网互联。现在需要编写一个简单的机群管理软件（参考LLNL的SLURM 20），这个软件由3个程序组成：
> (1) 运行在控制节点上的master，这个程序监视并控制整个机群的状态。
> (2) 运行在每个计算节点上的slave，负责启动和终止job，并监控本机的资源。 
> (3) 供最终用户使用的client命令行工具，用于提交job。
>
根据前面的分析，slave是个“看门狗进程”，它会启动别的job进程，因此必须是个单线程程序。另外它不应该占用太多的CPU资源，这也适合单线程模型。master应该是个模式(2)的多线程程序：
> (1)  它独占一台8核的机器， 如果用模型(1)，等于浪费了87.5％的CPU资源。
> (2)  整个机群的状态应该能完全放在内存中，这些状态是共享且可变的。如果用模式(3)，那么进程之间的状态同步会成大问题。而如果大量使用共享内存，则等于是掩耳盗铃，是披着多进程外衣的多线程程序。因为一个进程一旦在临界区内阻塞或crash， 其他进程会全部死锁。
> (3)  master的主要性能指标不是throughput， 而是latency， 即尽快地响应各种事件。 它几乎不会出现把IO或CPU跑满的情况。
> (4)  master监控的事件有优先级区别，一个程序正常运行结束和异常崩溃的处理优先级不同， 计算节点的磁盘满了和机箱温度过高这两种报警条件的优先级也不同。 如果用单线程， 则可能会出现优先级反转。
> (4)  假设master和每个slave之间用一个TCP连接，那么master采用2个或4个IO线程来处理8个TCPconnections能有效地降低延迟。
> (5)  master要异步地往本地硬盘写log，这要求logging library有自己的IO线程。
> (6)  master有可能要读写数据库，那么数据库连接这个第三方library可能有自己的线程， 并回调master的代码。
> (7)  master要服务于多个clients，用多线程也能降低客户响应时间。也就是说它可以再用2个IO线程专门处理和clients的通信。
> (8)  master还可以提供一个monitor接口， 用来广播推送（pushing）机群的状态，这样用户不用主动轮询（polling）。这个功能如果用单独的线程来做，会比较容易实现， 不会搞乱其他主要功能。
> 
综上所述，master一共开了10个线程：
> 4个用于和slaves通信的IO线程。
> 1个logging线程。
> 1个数据库IO线程。
> 2个和clients通信的IO线程。
> 1个主线程， 用于做些背景工作， 比如job调度。
> 1个pushing线程， 用于主动广播机群的状态。
> 
虽然线程数目略多于core数目，但是这些线程很多时候都是空闲的，可以依赖OS的进程调度来保证可控的延迟。

<span style="color:red; font-weight:bold"> 例二：TCP聊天服务器 </span>

&emsp;&emsp; 注意，这里的“聊天”不完全指人与人聊天，也可能是机器与机器“聊天”。这种服务的特点是并发连接之间有数据交换（如果多个连接分属不同的进程，则数据交互会很不方便），从一个连接收到的数据要转发给其他多个连接。因此我们不能按模式(3)的做法，把多个连接分到多个进程中分别处理（因为并发连接之间有数据交换，这会带来复杂的进程间通信），而只能用模式(1)或者模式(2)。如果纯粹只有数据交换，那么我想模式(1)也能工作得很好，因为现在的CPU足够快， 单线程应付几百个连接不在话下。
&emsp;&emsp; 如果功能进一步复杂化，加上关键字过滤、黑名单、防灌水等等功能，甚至要给聊天内容自动加上相关连接，每一项功能都会占用CPU资源。这时就要考虑模式(2)了，因为单个CPU的处理能力显得捉襟见肘，顺序处理导致消息转发的延迟增加。这时我们考虑把空闲的多个CPU利用起来，自然的做法是把连接分散
到多个线程上，例如按round-robin的方式把1000个客户连接分配到4个IO线程上。这样充分利用多核加速。具体的例子见 *书中6.6* 的方案9，以及*书中7.12* 的实现。
**上面都是书中原话，总结一下就是：**
> 模式(1)（即，运行 一个 单线程的进程） 不适合是因为，聊天程序可能需要处理成千上万的用户连接，因为单线程只能占用一个核，无法使用多核加速，这样不能充分使用CPU资源，会造成消息转发有明显的延时。
> 模式(3) （即，运行 多个 单线程的进程）可以充分发挥多核性能，但是因为该模式运行了多个进程，而在聊天程序中，不同的连接需要进行数据交互（毕竟是多个用户(一个用户可以看做一个连接)，而多进程之间信息交换比较麻烦，因此也不合适
> 模式(2)(即，运行 一个 多线程的进程) 充分发挥多核性(因为有多个线程)，而且通信很方便(这些线程都属于一个线程)，因此使用这个模式最合理。
> 
#### 16.3.2.3 总结
&emsp;&emsp; 首先要明确的是，使用多线程程序的目的是为了提高程序性能，如何多线程可以提高程序性能，那么就该使用多线程，比如前面的TCP聊天服务器的例子。
&emsp;&emsp; 但有的时候，多线程程序并不一定有性能优势，比如：
* &emsp;&emsp; 对于静态Web服务器，或者FTP服务器，CPU的负载较轻，主要瓶颈在磁盘IO和网络IO方面。这时候往往一个单线程的程序（模式1） 就能撑满IO。 用多线程并不能提高吞吐量， 因为IO硬件容量已经饱和了。 同理， 这时增加CPU数目也不能提高吞吐量。
* &emsp;&emsp; CPU跑满的情况比较少见，这里我只好虚构一个例子。假设有一个服务，它的输入是n个整数，问能否从中选出m个整数，使其和为0（这里n＜100, m＞0）。这是著名的subset sum问题，是NP-Complete的。对于这样一个“服务”，哪怕很小的n值也会让CPU算死。比如n＝30，一次的输入不过200字节（32-bit整数），CPU的运算时间却能长达几分钟。 对于这种应用， 模式3a是最适合的，能发挥多核的优势， 程序也简单。

&emsp;&emsp; **总的来说**，就是如果多线程能提高程序性能，那就使用多线程；如果多线程不能提高程序性能，那就别用，因为做了也没啥效果，反而增加程序的复杂程度。

## 16.4 一个多线程程序中，它的线程可以分为哪几类？
据作者的经验， 一个多线程服务程序中的线程大致可分为3类：
**(1) IO线程**
&emsp;&emsp; 这类线程的主循环是IO multiplexing，阻塞地等在`select/poll/epoll_wait`系统调用上。这类线程也处理定时事件。当然它的功能不止IO， 有些简单计算也可以放入其中， 比如消息的编码或解码。
**(2) 计算线程**
&emsp;&emsp; 这类线程的主循环是blockingqueue，阻塞地等在`conditionvariable`上。 这类线程一般位于`thread pool`中。 这种线程通常不涉及IO， 一般要避免任何阻塞操作。
**(3) 第三方库所用的线程**
&emsp;&emsp; 比如`logging`， 又比如`database connection`。

## 16.5 为什么说，在这个时代，多线程是不可避免的？
&emsp;&emsp; 因为现在是多核时代，要想充分发挥CPU性能，多线程编程是不可避免的。






&emsp;
&emsp;
&emsp; 
# 17. Linux上的线程标识
## 17.1 Linux有哪些方法可以用来表示一个线程？
一共有两种方法：
&emsp; (1) Pthreads 自带的函数`pthread_self()`的返回值；
&emsp; (2) 系统调用 `gettid()`的返回值；

## 17.2 书中建议如何标识一个线程？为什么？
### 17.2.1 建议的做法
&emsp;&emsp; 书中建议使用系统调用 `gettid()`的返回值来表示一个线程；
### 17.2.2 原因
<span style="color:red; font-size:18px; font-weight:bold"> (1) 为什么不使用`pthread_self()`的返回值 </span>

之所以不使用`pthread_self()`的返回值 来标识一个线程，主要有两个原因：
**原因一：`pthread_self()`的返回值 的类型不够明确：**
&emsp;&emsp; 因为 `pthread_t` 不一定是一个数值类型（整数或指针），也有可能是一个结构体，因此`Pthreads`专门提供了`pthread_equal()`函数用于对比两个线程标识符是否相等。 这就带来一系列问题， 包括：
> ① 无法打印输出`pthread_t`， 因为不知道其确切类型。 也就没法在日志中用它表示当前线程的id。
> ② 无法比较`pthread_t`的大小或计算其hash值， 因此无法用作关联容器的key。
> ③ 无法定义一个非法的`pthread_t`值，用来表示绝对不可能存在的线程id，因此`MutexLock class`没有办法有效判断当前线程是否已经持有本锁。
> ④ pthread_t值只在进程内有意义，与操作系统的任务调度之间无法建立有效关联。比方说在/proc文件系统中找不到pthread_t对应的task。

**原因二：`pthread_self()`返回的线程ID并不绝对唯一**
&emsp;&emsp; `glibc`的`Pthreads`实现实际上把`pthread_t`用作一个结构体指针（它的类型是`unsigned long`），指向一块动态分配的内存，而且这块内存是反复使用的。这就造成`pthread_t`的值很容易重复。 `Pthreads`只保证同一进程之内，同一时刻的各个线程的id不同；不能保证同一进程先后多个线程具有不同的id， 更不要说一台机器上多个进程之间的id唯一性了。
&emsp;&emsp; 例如下面这段代码中先后两个线程的标识符是相同的：
```cpp
void* thread_func(void *arg) {   }

int main()
{
    pthread_t t1, t2;
    pthread_create(&t1, NULL, thread_func, NULL);
    cout << t1 << endl;
    pthread_join(t1, NULL);

    pthread_create(&t2, NULL, thread_func, NULL);
    cout << t2 << endl; 
    pthread_join(t2, NULL);
}
```
运行结果如下：
```
140553174914816
140553174914816
```
<span style="color:green; font-weight:bold"> 综上所述，`pthread_t`并不适合用作程序中对线程的标识符 </span>

<span style="color:red; font-size:18px; font-weight:bold"> (2) 为什么使用 系统调用`gettid()`的返回值 </span>

使用`gettid()`系统调用的返回值作为线程id， 这么做的好处有：
> ① 它的类型是`pid_t`， 其值通常是一个小整数，便于在日志中输出。
> ② 在现代Linux中， 它直接表示内核的任务调度`id`，因此在`/proc`文件系统中可以轻易找到对应项： `/proc/tid`或`/prod/pid/task/tid`。
> ③ 方便用Linux的其他系统工具定位到具体某一个线程，例如：在`top()中`我们可以按线程列出任务， 然后找出CPU使用率最高的线程id，再根据程序日志判断到底哪一个线程在耗用CPU
> ④ 任何时刻都是全局唯一的，并且由于Linux分配新`pid`采用递增轮回办法，短时间内启动的多个线程也会具有不同的线程`id`。
> ⑤ `0`是非法值， 因为操作系统第一个进程init的pid是`1`
> 

### 17.2.3 如何保证性能？
&emsp;&emsp; 每次都执行一次系统调用`gettid()`似乎有些浪费，如何才能做到更高效呢？
> 用`__thread变量`来缓存`gettid()`的返回值，这样只有在本线程第一次调用的时候才进行系统调用， 以后都是直接从`thread local`缓存的线程id拿到结果，效率无忧。多线程程序在打日志的时候可以在每一条日志消息中包含当前线程的id， 不必担心有效率损失。
> 


&emsp;
&emsp;
&emsp; 
# 参考文献
1. [理解 shared_ptr实现copy-on-write（COW）](https://blog.csdn.net/zhangxiao93/article/details/52792888)