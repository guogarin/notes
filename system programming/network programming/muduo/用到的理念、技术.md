- [1. RAII](#1-raii)
  - [用在什么地方了？](#用在什么地方了)
- [2. local copy](#2-local-copy)
- [3.  对象池、弱回调](#3--对象池弱回调)
- [4. 分布式系统中，一般推荐用什么 进程通信 方法? 为什么？](#4-分布式系统中一般推荐用什么-进程通信-方法-为什么)
  - [4.1  用什么？](#41--用什么)
  - [4.2  为什么？](#42--为什么)
- [5. muduo库中为什么不推荐使用 递归mutex?](#5-muduo库中为什么不推荐使用-递归mutex)
  - [5.1 因为 非递归mutex性能好？](#51-因为-非递归mutex性能好)
  - [5.2 那是为什么呢？](#52-那是为什么呢)
  - [5.3 如何解决上面代码中的死锁问题？](#53-如何解决上面代码中的死锁问题)
- [6. 死锁](#6-死锁)
  - [6.2 自己写的时候犯的错误](#62-自己写的时候犯的错误)
- [7 条件变量](#7-条件变量)
  - [7.1 使用建议](#71-使用建议)
  - [7.2 如何确定，`pthread_cond_signal()`和 `pthread_cond_broadcast()`应该使用哪个？](#72-如何确定pthread_cond_signal和-pthread_cond_broadcast应该使用哪个)
- [8. 不要用 读写锁 和 信号量](#8-不要用-读写锁-和-信号量)
  - [8.1 为什么不要用 读写锁？](#81-为什么不要用-读写锁)
  - [8.2 为什么不要用 信号量？](#82-为什么不要用-信号量)
- [9 用RAII封装`mutex`类](#9-用raii封装mutex类)
- [9 封装`Condition`类](#9-封装condition类)
  - [9.2 同时使用`mutex`和`condition`时需要注意什么？](#92-同时使用mutex和condition时需要注意什么)
- [10. 线程安全的`Singleton`实现](#10-线程安全的singleton实现)
- [11. sleep(3) 不是同步原语](#11-sleep3-不是同步原语)
- [12. 借`shared_ptr`实现`copy-on-write (COW)`](#12-借shared_ptr实现copy-on-write-cow)
  - [12.1 如何用 `shared_ptr`实现`copy-on-write`？](#121-如何用-shared_ptr实现copy-on-write)
  - [12.2 用`shared_ptr`实现COW时要注意什么呢？](#122-用shared_ptr实现cow时要注意什么呢)
  - [12.3 copy-on-write 应用：用普通`mutex`替换读写锁的一个例子](#123-copy-on-write-应用用普通mutex替换读写锁的一个例子)
- [13. one loop per thread](#13-one-loop-per-thread)
- [14 如何确定线程池的大小？](#14-如何确定线程池的大小)
- [15. 进程间通信方式的选择](#15-进程间通信方式的选择)
  - [15.1 IPC工具](#151-ipc工具)
  - [15.2 通信协议](#152-通信协议)
  - [15.3 长连接还是短连接？](#153-长连接还是短连接)
- [16. 多线程服务器的适用场合](#16-多线程服务器的适用场合)
  - [16.1 多线程可用的模式 和 优缺点](#161-多线程可用的模式-和-优缺点)
  - [16.2 为什么说 多线程 并不是 万灵丹（silver bullet） ？](#162-为什么说-多线程-并不是-万灵丹silver-bullet-)
  - [16.3 那么究竟什么时候该用 多线程？什么时候该用单线程？](#163-那么究竟什么时候该用-多线程什么时候该用单线程)
    - [16.3.1 何时使用 单线程](#1631-何时使用-单线程)
      - [16.3.2.1 必须使用 单线程的场合](#16321-必须使用-单线程的场合)
      - [16.3.2.2 单线程程序的优缺点](#16322-单线程程序的优缺点)
    - [16.3.2 何时使用 多线程？](#1632-何时使用-多线程)
      - [16.3.2.1 多线程程序是否有性能优势？](#16321-多线程程序是否有性能优势)
      - [16.3.2.2 适用多线程程序的场景](#16322-适用多线程程序的场景)
      - [16.3.2.3 总结](#16323-总结)
  - [16.4 一个多线程程序中，它的线程可以分为哪几类？](#164-一个多线程程序中它的线程可以分为哪几类)
  - [16.5 为什么说，在这个时代，多线程是不可避免的？](#165-为什么说在这个时代多线程是不可避免的)
- [17. Linux上的线程标识](#17-linux上的线程标识)
  - [17.1 Linux有哪些方法可以用来表示一个线程？](#171-linux有哪些方法可以用来表示一个线程)
  - [17.2 书中建议如何标识一个线程？为什么？](#172-书中建议如何标识一个线程为什么)
    - [17.2.1 建议的做法](#1721-建议的做法)
    - [17.2.2 原因](#1722-原因)
    - [17.2.3 如何保证性能？](#1723-如何保证性能)
- [18. 线程的创建与销毁的守](#18-线程的创建与销毁的守)
  - [18.1 线程的创建](#181-线程的创建)
    - [18.1.1 线程的创建需要遵守的原则是？为什么？](#1811-线程的创建需要遵守的原则是为什么)
  - [18.1 线程的销毁](#181-线程的销毁)
    - [18.1.1 线程的销毁有几种方式？](#1811-线程的销毁有几种方式)
    - [18.1.2 `exit()`是否线程安全？为什么？](#1812-exit是否线程安全为什么)
- [19. 善用`__thread`关键字](#19-善用__thread关键字)
  - [19.1 `__thread`是什么？](#191-__thread是什么)
  - [19.2 线程局部存储有何优势？](#192-线程局部存储有何优势)
- [20. 多线程与IO](#20-多线程与io)
  - [20.1 能否多个线程同时读写同一个socket文件描述符？](#201-能否多个线程同时读写同一个socket文件描述符)
  - [20.2 对于多线程程序，应该如何操作文件描述符呢？](#202-对于多线程程序应该如何操作文件描述符呢)
  - [20.3 对于多线程程序，应该如何操作`epoll`?](#203-对于多线程程序应该如何操作epoll)
  - [20.4 多线程IO的几个例外](#204-多线程io的几个例外)
- [21. 在多线程程序中管理文件描述符](#21-在多线程程序中管理文件描述符)
  - [21.1 为什么说 多线程读写 文件描述符 容易“串话”？](#211-为什么说-多线程读写-文件描述符-容易串话)
  - [21.2 如何解决上面说的“串话”的问题？](#212-如何解决上面说的串话的问题)
  - [21.3 服务端是否应该关闭 标准输出（fd＝1） 和标准错误（fd＝2）？ 不是不应该，正确做法是怎样的？](#213-服务端是否应该关闭-标准输出fd1-和标准错误fd2-不是不应该正确做法是怎样的)
  - [21.4 多线程程序中应该如何管理 并发连接？](#214-多线程程序中应该如何管理-并发连接)
  - [21.5 总结](#215-总结)
- [22. RAII与`fork()`](#22-raii与fork)
- [23. 多线程与fork()](#23-多线程与fork)
  - [23.1 多线程和fork()的协作性如何？为什么？](#231-多线程和fork的协作性如何为什么)
  - [23.2 当多线程进程 调用`fork()`会发生什么？会引发什么问题？](#232-当多线程进程-调用fork会发生什么会引发什么问题)
    - [23.2.1 会发生什么？](#2321-会发生什么)
    - [23.2.2 会引发什么问题？](#2322-会引发什么问题)
  - [23.3 鉴于多线程进程中调用`fork()`会引发这么多问题，我们应该怎么做呢？](#233-鉴于多线程进程中调用fork会引发这么多问题我们应该怎么做呢)
- [24. 多线程与signal](#24-多线程与signal)
- [26. 编写多线程C++程序需遵循的原则](#26-编写多线程c程序需遵循的原则)
- [27. 日志(logging)](#27-日志logging)
  - [27.1 在服务端编程中， 日志一般要记录哪些内容？为什么要记录这些内容？](#271-在服务端编程中-日志一般要记录哪些内容为什么要记录这些内容)
  - [27.2 日志级别有什么用？](#272-日志级别有什么用)
  - [27.3 日志的归档应该由谁来完成？为什么？](#273-日志的归档应该由谁来完成为什么)
  - [27.4 磁盘空间满了之后是否应该将之前的日志清空吗？](#274-磁盘空间满了之后是否应该将之前的日志清空吗)
  - [27.5 在程序奔溃时，如何保证最后几条日志不会丢失？](#275-在程序奔溃时如何保证最后几条日志不会丢失)
  - [27.6 日志的格式应该满足什么要求？](#276-日志的格式应该满足什么要求)
  - [27.7 日志的性能](#277-日志的性能)
    - [27.7.1 日志的性能重要吗？](#2771-日志的性能重要吗)
    - [27.7.2 日志的高效性](#2772-日志的高效性)
      - [27.7.2.1 日志的高效性体现在哪几个方面？](#27721-日志的高效性体现在哪几个方面)
      - [27.7.2.2 为什么需要保证程序可以 1秒写100万条 日志消息呢？](#27722-为什么需要保证程序可以-1秒写100万条-日志消息呢)
      - [27.7.2.3 每次打印日志时都要打印时间戳，如何提高性能呢？](#27723-每次打印日志时都要打印时间戳如何提高性能呢)
    - [27.7.3 多线程 异步 日志](#2773-多线程-异步-日志)
      - [27.7.3.1 多线程日志程序如何处理 不同线程的日志消息会出现交织 的情况？](#27731-多线程日志程序如何处理-不同线程的日志消息会出现交织-的情况)
      - [27.7.3.2 什么是异步日志？它有何优势？](#27732-什么是异步日志它有何优势)
      - [27.7.3.3 异步日志 如何保证 消息不会出现交织？](#27733-异步日志-如何保证-消息不会出现交织)
      - [27.7.3.4 什么是双缓冲技术？](#27734-什么是双缓冲技术)
    - [27.4 muduo库的双缓冲技术原理是？](#274-muduo库的双缓冲技术原理是)
      - [27.4.2 muduo日志模块的运行时一共有哪几种情况？](#2742-muduo日志模块的运行时一共有哪几种情况)
      - [27.4.3 除了上面的4个缓冲区的做法，还有什么其他改进方案吗？这个改进方案有何优缺点？](#2743-除了上面的4个缓冲区的做法还有什么其他改进方案吗这个改进方案有何优缺点)
- [作者在第六章抛出的几个问题，看完muduo库以后再回来看](#作者在第六章抛出的几个问题看完muduo库以后再回来看)
- [28. 性能测试](#28-性能测试)
  - [28.1 `ping pong`协议](#281-ping-pong协议)
    - [28.1.1 什么是`ping pong`协议？](#2811-什么是ping-pong协议)
    - [28.1.2 `ping pong`协议 的作用是？](#2812-ping-pong协议-的作用是)
    - [28.1.3 `ping pong`协议 拆包吗？](#2813-ping-pong协议-拆包吗)
  - [28.2 为什么要在同一台机器上测吞吐量？](#282-为什么要在同一台机器上测吞吐量)
  - [28.3 击鼓传花](#283-击鼓传花)
  - [28.3.1 什么是击鼓传花？](#2831-什么是击鼓传花)
  - [28.3.2 这个击鼓传花主要用来测试什么？](#2832-这个击鼓传花主要用来测试什么)
  - [28.3.3 用击鼓传花进行性能测试时需要注意什么？](#2833-用击鼓传花进行性能测试时需要注意什么)
- [29. 对于一个proxy， 既要把连接a收到的数据发给连接b， 又要把从b收到的数据发给a， 那么到底读哪个？如何解决呢？](#29-对于一个proxy-既要把连接a收到的数据发给连接b-又要把从b收到的数据发给a-那么到底读哪个如何解决呢)
- [参考文献](#参考文献)





# 1. RAII
## 用在什么地方了？
加锁TODO:





&emsp;
&emsp; 
# 2. local copy






&emsp;
&emsp; 
# 3.  对象池、弱回调
TODO: 后面自己实现一遍





&emsp;
&emsp; 
# 4. 分布式系统中，一般推荐用什么 进程通信 方法? 为什么？
## 4.1  用什么？
用 消息传递(message passing)。

## 4.2  为什么？
&emsp;&emsp; 共享内存只能用于同一个机器上的进程间通信，而消息传递不但可以用于同一机器中的进程间通信，还可以用于不同机器间的通信。在单机上， 我们也可以照搬message passing作为多个进程的并发模型。 这样整个分布式系统的架构的一致性很强， 扩容（scale out） 起来也较容易。





&emsp;
&emsp; 
# 5. muduo库中为什么不推荐使用 递归mutex?
## 5.1 因为 非递归mutex性能好？
&emsp;&emsp; 不是为了性能，而是为了体现设计意图。non-recursive和recursive的性能差别其实不大， 因为少用一个计数器，非递归mutex略快一点点而已。

## 5.2 那是为什么呢？
&emsp;&emsp; 毫无疑问recursive mutex使用起来要方便一些， 因为不用考虑一个线程会自己把自己给锁死了， 我猜这也是Java和Windows默认提供recursive mutex的原因。 
&emsp;&emsp; 正因为它方便，recursive mutex可能会隐藏代码里的一些问题。典型情况是你以为拿到一个锁就能修改对象了，没想到外层代码已经拿到了锁，正在修改（或读取） 同一个对象呢。 来看一个具体的例子：
```cpp
class Foo{
public:
    void doit() const;
};

MutexLock mutex;
std::vector<Foo> foos;

void post(const Foo& f){
    MutexLockGuard lock(mutex);
    foos.push_back(f); // 
}

void traverse()
{
    MutexLockGuard lock(mutex);
    for (std::vector<Foo>::const_iterator it = foos.begin(); it != foos.end(); ++it){
        it->doit();
    }
}

void Foo::doit() const{
    Foo f;
    post(f);
}

int main()
{
    Foo f;
    post(f);
    traverse();
}
```
`post()`加锁， 然后修改`foos`对象； `traverse()`加锁， 然后遍历`foos`向量。 这些都是正确的。
将来有一天， `Foo::doit()`间接调用了`post()`， 那么会很有戏剧性的结果：
> (1) mutex是非递归的， 于是死锁了。
> (2) mutex是递归的， 由于`push_back()`可能（但不总是）导致`vector迭代器`失效（`push_back()`导致内存重新分配后，迭代器会失效），程序偶尔会crash。
> 
这时候就能体现non-recursive的优越性：把程序的逻辑错误暴露出来。死锁比较容易debug，把各个线程的调用栈打出来，只要每个函数不是特别长，很容易看出来是怎么死的， 见§2.1.2的例子 9。或者可以用`PTHREAD_MUTEX_ERRORCHECK`一下子就能找到错误（前提是MutexLock带debug选项）。程序反正要死，不如死得有意义一点，留个“全尸”，让验尸（post-mortem）更容易些。
&emsp;&emsp; **换句话来说就是：使用非递归mutex可以尽早的将问题暴露出来，这样不容易在生产环境出问题。**

## 5.3 如何解决上面代码中的死锁问题？
可以，用`shared_ptr`实现copy-on-write，这样就能解决，代码如下：
```cpp
class Foo
{
public:
    void doit() const;
};

typedef std::vector<Foo> FooList;
typedef boost::shared_ptr<FooList> FooListPtr;
FooListPtr g_foos;
MutexLock mutex;

void post(const Foo& f)
{
    printf("post\n");
    MutexLockGuard lock(mutex);

    // 理论上 全局变量 g_foos 应该只有一个拥有者，如果不只一个，
    // 那么应该就是在 traverse()中正在遍历
    if (!g_foos.unique())
    {
        // 因为有其它线程在遍历 g_foos，为了不破坏 g_foos的迭代器
        // 直接拷贝一份
        g_foos.reset(new FooList(*g_foos));
        printf("copy the whole list\n");
    }

    // 对拷贝后的 g_foos 进行修改，不会影响 traverse()
    assert(g_foos.unique());
    g_foos->push_back(f);
}

void traverse()
{
    FooListPtr foos;
    {
        // 锁是加在这个局部作用域的，出了花括号的范围就解锁了，
        // 后面的遍历是没有锁的
        MutexLockGuard lock(mutex);
        foos = g_foos;
        assert(!g_foos.unique());
    }

    // assert(!foos.unique()); this may not hold

    for (std::vector<Foo>::const_iterator it = foos->begin();
        it != foos->end(); ++it)
    {
        it->doit();
    }
}

void Foo::doit() const
{
    Foo f;
    post(f);
}

int main()
{
    g_foos.reset(new FooList);
    Foo f;
    post(f);
    traverse();
}
```






&emsp;
&emsp; 
# 6. 死锁
TODO: 只是自己实现了一下第一个程序，后面的调试涉及到`gdb`，后面学了之后再回来！
```cpp
pthread_mutex_t mtx;
pthread_mutexattr_t mtxAttr;

class Request{
public:
    void process(){
        int ret = pthread_mutex_lock(&mtx);
        print();
        cout << "process() : " << ret << endl;
        ret = pthread_mutex_unlock(&mtx);
    }

    void print(){
        int ret = pthread_mutex_lock(&mtx);
        ret = pthread_mutex_unlock(&mtx);
        cout << "The end of print() :" << ret << endl;
    }
};


int main()
{
    int s = pthread_mutexattr_init(&mtxAttr);
    // 不依赖默认属性，直接指定mutex类型为 PTHREAD_MUTEX_NORMAL
    s = pthread_mutexattr_settype(&mtxAttr, PTHREAD_MUTEX_NORMAL);
    s = pthread_mutex_init(&mtx, &mtxAttr);
    Request obj;
    obj.process();
}
```

## 6.2 自己写的时候犯的错误
(1) 编译的时候忘记加 `-pthread`选项
正确的应该是：
```shell
g++ -o test.o test.cpp --std=c++11 -pthread
```





&emsp;
&emsp; 
# 7 条件变量
## 7.1 使用建议
**对于wait端：**
> (1) 必须与mutex一起使用， 该布尔表达式的读写需受此mutex保护。
> (2) 在mutex已上锁的时候才能调用wait()。
> (3) 把判断布尔条件和wait()放到while循环中。
>
对于signal/broadcast端：
> (1) 不一定要在mutex已上锁的情况下调用signal（理论上） 。
> (2) 在signal之前一般要修改布尔表达式。
> (3) 修改布尔表达式通常要用mutex保护（至少用作full memory barrier） 。
> (4) 注意区分signal与broadcast：“broadcast通常用于表明状态变化， signal通常用于表示资源可用。
> 

## 7.2 如何确定，`pthread_cond_signal()`和 `pthread_cond_broadcast()`应该使用哪个？
(1) 下面情况适合用`pthread_cond_broadcast`：
> 一个生产者多消费者，生产者能一次产生多个产品的情况。
> 多生产者多消费者
> 读写锁实现（写入之后，通知所有读者）
> 
(2) 下面情况适合`pthread_cond_signal`的情况
> 单一生产者，生产者一次生产一个产品的情况，最好一个消费者
> 
**在muduo库2.2小节中，有这样一段话：**
> 注意区分signal与broadcast： “broadcast通常用于表明状态变化， signal通常用于表示资源可用。（broadcast should generally be used to indicate state change rather than resource availability。 ）
> 
对于这段话其实可以这么理解：
> 资源被释放时，一般只能同时供一个线程使用，因此用signal通知一个线程即
>






&emsp;
&emsp; 
# 8. 不要用 读写锁 和 信号量
## 8.1 为什么不要用 读写锁？
&emsp;&emsp; 
**从正确性方面来说**，一种典型的易犯错误是在持有read lock的时候修改了共享数据。 这通常发生在程序的维护阶段， 为了新增功能， 程序员不小心在原来read lock保护的函数中调用了会修改状态的函数。 这种错误的后果跟无保护并发读写共享数据是一样的。
**从性能方面来说**，读写锁不见得比普通mutex更高效。 无论如何reader lock加锁的开销不会比mutex lock小，因为它要更新当前reader的数目。如果临界区很小，锁竞争不激烈，那么mutex往往会更快。

## 8.2 为什么不要用 信号量？
&emsp;&emsp; 笔者说：我没有遇到过需要使用信号量的情况，无从谈及个人经验。我认为信号量不是必备的同步原语，因为条件变量配合互斥器可以完全替代其功能，而且更不易用错。除了[RWC]指出的“semaphore has no notion of ownership”之外，信号量的另一个问题在于它有自己的计数值，而通常我们自己的数据结构也有长度值，这就造成了同样的信息存了两份，需要时刻保持一致，这增加了程序员的负担和出错的可能。如果要控制并发度，可以考虑用`muduo::ThreadPool`。






&emsp;
&emsp; 
# 9 用RAII封装`mutex`类
有几点要注意：
(1) 不应该使用系统默认的互斥锁类型（即`PTHREAD_MUTEX_DEFAULT`），而应该用`mutexattr`来显示指定mutex的类型（这里是`PTHREAD_MUTEX_NORMAL`）。（在本文前面关于 [死锁]()的介绍中就是这么做的 ）






&emsp;
&emsp; 
# 9 封装`Condition`类
TODO: 代码分析

## 9.2 同时使用`mutex`和`condition`时需要注意什么？
&emsp;&emsp; 注意它们的声明顺序和初始化顺序， mutex_应先于condition_构造， 并作为后者的构造参数。
&emsp;&emsp; 因为成员变量在类中的声明次序 就是 其在初始化列表中的初始化顺序，与其在初始化列表中的先后次序无关。






&emsp;
&emsp; 
# 10. 线程安全的`Singleton`实现







&emsp;
&emsp; 
# 11. sleep(3) 不是同步原语
&emsp;&emsp; `sleep()/usleep()/nanosleep()`只能出现在测试代码中 比如写单元测试的时候；或者用于有意延长临界区，加速复现死锁的情况。







&emsp;
&emsp; 
# 12. 借`shared_ptr`实现`copy-on-write (COW)`
## 12.1 如何用 `shared_ptr`实现`copy-on-write`？
概括的来说，**COW技术的精髓是**：
> (1) 如果你是数据的唯一拥有者，那么你可以直接修改数据。
> (2) 如果你不是数据的唯一拥有者，那么你拷贝它之后再修改。
> 
用`shared_ptr`来实现COW时，主要考虑两点：
> (1) 读数据
> (2) 写数据
> 
&emsp;&emsp; `shared_ptr`拥有对对象的引用计数，在对对象进行读写操作时，这个计数是`1`，当读数据时，我们 创建一个新的智能指针 指向 原智能指针指向的对象，这个时候引用计数加`1`（变为`2`），这么做其实就是为了提醒其它线程，这个对象还有其他人持有，你别乱改。
&emsp;&emsp; **概括的来说**，就是通过新建一个 局部`shared_ptr` 指向共享的那个 全局`shared_ptr`所指向的对象，让其引用计数增加，这样其它线程就能知道你在使用该对象，其它线程就会 “自觉地” 拷贝后再修改：
```cpp
//假设g_ptr是一个全局的shared_ptr<Foo>并且已经初始化。
void read()
{
    shared_ptr<Foo> tmpptr; // 新建一个 局部shared_ptr
    {
        lock(); // 加锁
        /* 此时引用计数为2，这样写端就能知道有其它线程持有了g_ptr，它就不会直接修改g_ptr了
         而是会复制一份，然后修改复制的那一份，具体可以看后面 写端的实现*/
        // 引用计数加1是为了提醒其它线程，这个对象还有其他人持有，你别乱改
        tmpptr=g_ptr; // 局部智能指针tmpptr 和 全局智能指针g_ptr指向同一对象，它俩的引用计数都是2
    }
    // 注意 tmpptr指向的对象的引用计数还是为2哦，因为tmpptr还没被释放
    //访问tmpptr
    //...

} // 离开read()，tmpptr被释放，那么引用计数就减1
```
这部分是shared_ptr最基本的用法，还是很好理解的，`read()`函数调用结束，`tmpptr`作为栈上变量离开作用域，自然析构，原数据对象的引用计数也变为1。
&emsp;&emsp; 写数据就复杂一些。根据COW的准则，当你是唯一拥有者（对应对象的引用计数是1）时，那么你直接修改数据，这样没有问题，当你不是唯一拥有者，则需要拷贝数据再去修改，这就需要用到一些shared_ptr的编程技法了：
```cpp
void write()
{
    // 写端要在一开始就加锁，要不然多个线程并发write()时会导致，某些线程写入失效。
    lock() 
    if(!g_ptr.unique())
    {
        g_ptr.reset(new Foo(*g_ptr));
    }
    assert(g_ptr.unique());
    // write
    // 
}
```
解释一下代码：
`shared_ptr::unique()`：当引用计数为1时返回true，否则false。
那么当引用计数不为1的时候，说明有别的线程正在读，受shread_ptr::reset()中example的误导，一直以为，reset后，原对象被析构，这样不就会影响正在读的线程了吗？
**解释一下`write()`中的`g_ptr.reset(new Foo(*g_ptr));`**
&emsp;&emsp; 假设在同一时刻，有一个读线程A，一个写线程B，当写线程B进入到`if`循环中时，原对象的引用计数为2（写线程A也持有了`g_ptr`指向的数据，导致引用计数加1），分别为`tmpptr`（线程B）和`g_ptr`，此时`reset()`函数将原对象的引用计数减1，并且`g_ptr`已经指向了新的对象（用原对象构造），这样就完成了数据的拷贝，并且原对象还在，只是引用计数变成了1，只有在读端线程A释放了`tmpptr`后才会导致旧的数据被释放。

## 12.2 用`shared_ptr`实现COW时要注意什么呢？
read端：
> 锁只需在拷贝的时候加，最好使用local copy，或者新建一个函数来完成拷贝也是可以的（就像后面的`CustomerData::query`的第一行代码那样）。
> 
write端：
> 锁需要在一开始就加上，因为我们是需要修改共享的资源的，所以我们需要对整个函数加锁。
> 

## 12.3 copy-on-write 应用：用普通`mutex`替换读写锁的一个例子
**场景**： 一个多线程的C++程序，24h x 5.5d运行。有几个工作线程ThreadWorker{0, 1, 2, 3}， 处理客户发过来的交易请求； 另外有一个背景线程ThreadBackground， 不定期更新程序内部的参考数据。 这些线程都跟一个hash表打交道， 工作线程只读， 背景线程读写， 必然要用到一些同步机制， 防止数据损坏。 这里的示例代码用`std::map`代替`hash`表， 意思是一样的：
```cpp
typedef std::pair<string, int> Entry;
typedef std::vector<Entry> EntryList;
typedef std::map<string, EntryList> Map;
```
`Map`的`key`是用户名， `value`是一个`vector`， 里边存的是不同`stock`的最小交易间隔， `vector`已经排好序， 可以用二分查找。
&emsp;&emsp; 我们的系统要求工作线程的延迟尽可能小，可以容忍背景线程的延迟略大。一天之内，背景线程对数据更新的次数屈指可数，最多一小时一次，更新的数据来自于网络，所以对更新的及时性不敏感。Map的数据量也不大，大约一千多条数据。
&emsp;&emsp; 最简单的同步办法是用读写锁：工作线程加读锁，背景线程加写锁。但是读写锁的开销比普通mutex要大，而且是写锁优先，会阻塞后面的读锁。如果工作线程能用最普通的非重入`mutex`实现同步，就不必用读写锁，这能降低工作线程延迟。我们借助`shared_ptr`做到了这一点： 
```cpp
class CustomerData : boost::noncopyable
{
public:
    CustomerData()
        : data_(new Map)
    { }

    int query(const string& customer, const string& stock) const;

private:
    typedef std::pair<string, int> Entry;
    typedef std::vector<Entry> EntryList;
    typedef std::map<string, EntryList> Map;
    typedef boost::shared_ptr<Map> MapPtr;
    void update(const string& customer, const EntryList& entries);
    void update(const string& message);

    static int findEntry(const EntryList& entries, const string& stock);
    static MapPtr parseData(const string& message);

    MapPtr getData() const
    {
        muduo::MutexLockGuard lock(mutex_);
        return data_;
    }

    mutable muduo::MutexLock mutex_;
    MapPtr data_;
};
```
`CustomerData::query()`相当于读端，用了前面说的引用计数加`1`的办法， 用局部`MapPtr data`变量来持有`Map`，防止并发修改：
```cpp
int CustomerData::query(const string& customer, const string& stock) const
{
    // getData() 返回 data_的拷贝，这样管理data_的shared_ptr的引用计数就
    // 不再为1，而是2，如果写端(这里是背景线程)同时在操作data_，
    // 它就能知道有人持有 _data，它会复制一份再修改。
    MapPtr data = getData();

    Map::const_iterator entries = data->find(customer);
    if (entries != data->end())
        return findEntry(entries->second, stock);
    else
        return -1;
}
```
&emsp;&emsp; 关键看`CustomerData::update()`（写端）怎么写。既然要更新数据，那肯定得加锁，如果这时候其他线程正在读， 那么不能在原来的数据上修改， 得创建一个副本，在副本上修改，修改完了再替换。如果没有用户在读， 那么就能直接修改，节约一次Map拷贝。
&emsp;&emsp; 注意其中用了`shared_ptr::unique()`来判断是不是有人在读， 如果有人在读， 那么我们不能直接修改， 因为`query()`并没有全程加锁， 只在`getData()`内部有锁。`shared_ptr::swap()`把`data_`替换为新副本， 而且我们还在锁里，不会有别的线程来读， 可以放心地更新。如果别的reader线程已经刚刚通过`getData()`拿到了`MapPtr`，它会读到稍旧的数据。这不是问题，因为数据更新来自网络，如果网络稍有延迟， 反正reader线程也会读到旧的数据。
```cpp
// 每次收到一个 customer 的数据更新
void CustomerData::update(const string& customer, const EntryList& entries)
{
    // 注意，写端要在最外层加锁，要不然多个线程同时update()会导致某些线程修改失败
    muduo::MutexLockGuard lock(mutex_); //
    if (!data_.unique()) // 如果本线程不是 data_ 的唯一持有者，那就复制一份再修改
    {
        // 复制一份
        MapPtr newData(new Map(*data_)); 
        data_.swap(newData);
    }
    assert(data_.unique());
    (*data_)[customer] = entries;
}

void CustomerData::update(const string& message)
{
    MapPtr newData = parseData(message);
    if (newData)
    {
        muduo::MutexLockGuard lock(mutex_);
        data_.swap(newData);
    }
}
```





&emsp;
&emsp;
&emsp; 
# 13. one loop per thread 
这是muduo库使用的 C++多线程服务端编程模式， `one (event) loop per thread `+ `thread pool`
> `event loop`（也叫IO loop） 用作IO multiplexing，配合 non-blocking IO和定时器。
> `thread pool` 用来做计算， 具体可以是任务队列或生产者消费者队列。
> 

.TODO:不太明白这到底是是指啥，学了源码再回来看






&emsp;
&emsp;
&emsp; 
# 14 如何确定线程池的大小？
&emsp;&emsp; 程序里具体用几个loop、线程池的大小等参数需要根据应用来设定，基本的原则是“**阻抗匹配**”，使得CPU和IO都能高效地运作。
&emsp;&emsp; 如果池中线程在执行任务时，密集计算所占的时间比重为`P（0＜P≤1）`，而系统一共有`C`个CPU，为了让这`C`个CPU跑满而又不过载，线程池大小的经验公式`T＝C/P`。`T`是个hint，考虑到`P`值的估计不是很准确，`T`的最佳值可以上下浮动50％.这个经验公式的原理很简单，`T`个线程，每个线程占用`P`的CPU时间， 如果刚好占满`C`个CPU，那么必有`T×P＝C`。下面验证一下边界条件的正确性。
&emsp;&emsp; 假设`C＝8， P＝1.0`，线程池的任务完全是密集计算，那么`T＝8`。只要8个活动线程就能让8个CPU饱和，再多也没用，因为CPU资源已经耗光了。
&emsp;&emsp; 假设`C＝8， P＝0.5`，线程池的任务有一半是计算，有一半等在IO上，那么`T＝16`。考虑操作系统能灵活、合理地调度sleeping/writing/running线程，那么大概16个“50％繁忙的线程”能让8个CPU忙个不停。启动更多的线程并不能提高吞吐量，反而因为增加上下文切换的开销而降低性能。
&emsp;&emsp; 如果`P＜0.2`，这个公式就不适用了，`T`可以取一个固定值，比如`5×C`。另外，公式里的`C`不一定是CPU总数，可以是“分配给这项任务的CPU数目”，比如在8核机器上分出4个核来做一项任务，那么`C＝4`。
&emsp;&emsp; 此外，程序里或许还有个别执行特殊任务的线程，比如logging，这对应用程序来说基本是不可见的，但是在分配资源（CPU和IO）的时候要算进去，以免高估了系统的容量。






&emsp;
&emsp;
&emsp; 
# 15. 进程间通信方式的选择
## 15.1 IPC工具
&emsp;&emsp; 进程间通信我首选socket（主要指TCP），其最大的好处在于：可以跨主机，具有伸缩性。反正都是多进程了，如果一台机器的处理能力不够，很自然地就能用多台机器来处理。把进程分散到同一局域网的多台机器上，程序改改`host:port`配置就能继续用。相反，其他IPC都不能跨机器，这就限制了scalability。

## 15.2 通信协议
使用TCP协议，它有如下好处：
> ① 没有资源泄露的风险：TCP port由一个进程独占，且操作系统会自动回收（listening port和已建立连接的TCP socket都是文件描述符， 在进程结束时操作系统会关闭所有文件描述符） 。 这说明， 即使程序意外退出， 也不会给系统留下垃圾， 程序重启之后能比较容易地恢复， 而不需要重启操作系统（用跨进程的mutex就有这个风险）
> ② 既然port是独占的，那么可以防止程序重复启动，后面那个进程抢不到port，自然就没法初始化了，避免造成意料之外的结果。
> ③ 方便分析：tcpdump和Wireshark是解决两个进程间协议和状态争端的好帮手， 也是性能（吞吐量、 延迟） 分析的利器。 我们可以借此编写分布式程序的自动化回归测试。 也可以用tcpcopy 14之类的工具进行压力测试
> ④ TCP还能跨语言， 服务端和客户端不必使用同一种语言。 试想如果用共享内存作为IPC， C++程序如何与Java通信， 难道用JNI吗？
> 

## 15.3 长连接还是短连接？
建议使用长连接，它有如下好处：
> ① 容易定位分布式系统中的服务之间的依赖关系。 只要在机器上运`行netstat -tpna | grep :port`就能立刻列出用到某服务的客户端地址（Foreign列） ， 然后在客户端的机器上用`netstat`或`lsof`命令找出是哪个进程发起的连接。 这样在迁移服务的时候能有效地防止出现outage。 TCP短连接和UDP则不具备这一特性。 
> ② 通过接收和发送队列的长度也较容易定位网络或程序故障。在正常运行的时候，`netstat`打印的`Recv-Q`和`Send-Q`都应该接近0，或者在0附近摆动。 如果`Recv-Q`保持不变或持续增加，则通常意味着服务进程的处理速度变慢，可能发生了死锁或阻塞。如果`Send-Q`保持不变或持续增加，有可能是对方服务器太忙、来不及处理，也有可能是网络中间某个路由器或交换机故障造成丢包，甚至对方服务器掉线，这些因素都可能表现为数据发送不出去。通过持续监控`Recv-Q`和`Send-Q`就能及早预警性能或可用性故障。 
> 






&emsp;
&emsp;
&emsp; 
# 16. 多线程服务器的适用场合
## 16.1 多线程可用的模式 和 优缺点
如果要在一台多核机器上提供一种服务或执行一个任务，可用的模式有： 
> (1) 运行 一个 单线程的进程；（启动一个进程A，该进程只含有一个线程）
> (2) 运行 一个 多线程的进程；（启动一个进程A，该进程中包含多个线程）
> (3) 运行 多个 单线程的进程；(启动多个进程，A B C...，这些进程各自都只包含一个线程)
> &emsp;&emsp; a 简单地把模式(1)中的进程运行多份 
> &emsp;&emsp; b 主进程+woker进程，如果必须绑定到一个TCP port，比如httpd+fastcgi
> (4) 运行 多个 多线程的进程。(启动多个进程，A B C...，而且这些进程都包含多个线程)
> 
模式(1)是不可伸缩的（scalable） ， 不能发挥多核机器的计算能力。
模式(2)是被很多人所鄙视的， 认为多线程程序难写， 而且与模式(3)相比并没有什么优势。
模式(3)是目前公认的主流模式。 它有以下两种子模式：
> a 简单地把模式(1)中的进程运行多份 
> b 主进程+woker进程， 如果必须绑定到一个TCP port， 比如httpd+fastcgi
> 
模式(4)更是千夫所指， 它不但没有结合2和3的优点， 反而汇聚了二者的缺点

## 16.2 为什么说 多线程 并不是 万灵丹（silver bullet） ？
&emsp;&emsp; Paul E. McKenney在《Is Parallel Programming Hard, And, If So, What Can You Do About It?》 17第3.5节指出：
> As a rough rule of thumb, use the simplest tool that will get the job done.
> rule of thumb：(根据实际经验的)粗略估算;经验之谈; 经验法则
> 
比方说，使用速率为50MB/s的数据压缩库、在进程创建销毁的开销是800μs、线程创建销毁的开销是50μs的前提下， 考虑如何执行压缩任务：
* &emsp;&emsp; 如果要偶尔压缩1GB的文本文件，预计运行时间是20s，那么起一个进程去做是合理的，因为进程启动和销毁的开销远远小于实际任务的耗时。
* &emsp;&emsp; 如果要经常压缩500kB的文本数据，预计运行时间是10ms，那么每次都起进程似乎有点浪费了，可以每次单独起一个线程去做。
* &emsp;&emsp; 如果要频繁压缩10kB的文本数据，预计运行时间是200μs，那么每次起线程似乎也很浪费，不如直接在当前线程搞定。也可以用一个线程池，每次把压缩任务交给线程池，避免阻塞当前线程（特别要避免阻塞IO线程）。

由此可见，多线程并不是万灵丹（silver bullet），它有适用的场合，也有不适合的场合。

## 16.3 那么究竟什么时候该用 多线程？什么时候该用单线程？
### 16.3.1 何时使用 单线程
#### 16.3.2.1 必须使用 单线程的场合
一般来说，有两种场合必须使用单线程：
> (1) 程序可能会`fork()`,多线程程序不是不能调用fork(2)， 而是这么做会遇到很多麻烦；
> (2) 限制程序的CPU占用率。
> 
<span style="color:red; font-size:18px; font-weight:bold"> (1) 程序可能会fork() </span>

多线程程序不是不能调用`fork()`， 而是这么做会遇到很多麻烦；一个程序`fork()`之后一般有两种行为：
> ① 立刻执行`exec()`，变身为另一个程序。例如`shell`和`inetd`；又比如`lighttpd` `fork()`出子进程， 然后运行`fastcgi`程序。 或者集群中运行在计算节点上的负责启动`job`的守护进程（即我所谓的“看门狗进程”） 。
> ② 不调用`exec()`，继续运行当前程序。要么通过共享的文件描述符与父进程通信， 协同完成任务；要么接过父进程传来的文件描述符，独立完成工作，例如20世纪80年代的Web服务器`NCSA httpd`。
>
这些行为中，我认为只有“看门狗进程”必须坚持单线程，其他的均可替换为多线程程序（从功能上讲）。
&emsp;
<span style="color:red; font-size:18px; font-weight:bold"> (2) 限制程序的CPU占用率 </span>

&emsp;&emsp; 这个很容易理解，比如在一个8核的服务器上，一个单线程程序即便发生busy-wait（无论是因为bug，还是因为overload），占满1个core，其CPU使用率也只有`12.5％`。在这种最坏的情况下，系统还是有`87.5％`的计算资源可供其他服务进程使用。
&emsp;&emsp; 因此对于一些辅助性的程序，如果它必须和主要服务进程运行在同一台机器的话（比如它要监控其他服务进程的状态），那么**做成单线程的能限制该程序的CPU占用频率，避免其过分抢夺系统的计算资源**。比方说：
> &emsp;&emsp; 如果要把生产服务器上的日志文件压缩后备份到NFS上，那么应该使用普通单线程压缩工具（gzip/bzip2）。它们对系统造成的影响较小，在8核服务器上最多占满1个core。如果有人为了“提高速度”，开启了多线程压缩或者同时起多个进程来压缩多个日志文件， 有可能造成的结果是非关键任务耗尽了CPU资源，正常客户的请求响应变慢。这是我们不愿意看到的。

#### 16.3.2.2 单线程程序的优缺点
<span style="color:red; font-size:18px; font-weight:bold"> 优点： </span>

&emsp;&emsp; 从编码的角度，单线程程序的优势无须赘言： 编写起来简单。比如说event loop

<span style="color:red; font-size:18px; font-weight:bold"> 缺点： </span>

&emsp;&emsp; event loop 有一个明显的缺点，它是非抢占的（non-preemptive）。假设事件a的优先级高于事件b，处理事件a需要1ms，处理事件b需要10ms。如果事件b稍早于a发生，那么当事件a到来时，程序已经离开了`poll()`调用，并开始处理事件b。事件a要等上10ms才有机会被处理，总的响应时间为11ms。这等于发生了优先级反转。这个缺点可以用多线程来克服，这也是多线程的主要优势。

### 16.3.2 何时使用 多线程？
#### 16.3.2.1 多线程程序是否有性能优势？
&emsp;&emsp; 无论是`IO bound`还是`CPU bound`的服务，多线程都 没有绝对意义上的 性能优势。这句话是说：
> **如果用很少的CPU负载就能让IO跑满，或者用很少的IO流量就能让CPU跑满，那么多线程没啥用处。**
> 
举例来说：
* &emsp;&emsp; 对于静态Web服务器，或者FTP服务器，CPU的负载较轻，主要瓶颈在磁盘IO和网络IO方面。这时候往往一个单线程的程序（模式1） 就能撑满IO。 用多线程并不能提高吞吐量， 因为IO硬件容量已经饱和了。 同理， 这时增加CPU数目也不能提高吞吐量。
* &emsp;&emsp; CPU跑满的情况比较少见，这里我只好虚构一个例子。假设有一个服务，它的输入是n个整数，问能否从中选出m个整数，使其和为0（这里n＜100, m＞0）。这是著名的subset sum问题，是NP-Complete的。对于这样一个“服务”，哪怕很小的n值也会让CPU算死。比如n＝30，一次的输入不过200字节（32-bit整数），CPU的运算时间却能长达几分钟。 对于这种应用， 模式3a是最适合的，能发挥多核的优势， 程序也简单。
* 
也就是说， 无论任何一方早早地先到达瓶颈， 多线程程序都没啥优势。

#### 16.3.2.2 适用多线程程序的场景
&emsp;&emsp; **多线程的适用场景是**：提高响应速度，让IO和“计算”相互重叠，降低latency。虽然多线程不能提高绝对性能，但能提高平均响应性能。
一个程序要做成多线程的， 大致要满足：
  * (1) 有多个CPU可用。单核机器上多线程没有性能优势（但或许能简化并发业务逻辑的实现） 。
  * (2) 线程间有共享数据，即内存中的全局状态。如果没有共享数据，用模型`(3)b`就行。 虽然我们应该把线程间的共享数据降到最低，但不代表没有。
  * (3) 共享的数据是可以修改的， 而不是静态的常量表。 如果数据不能修改， 那么可以在进程间用shared memory， 模式(3)就能胜任。
  * (4) 提供非均质的服务。即，事件的响应有优先级差异，我们可以用专门的线程来处理优先级高的事件。防止优先级反转。
  * (4) latency和throughput同样重要，不是逻辑简单的IO bound或CPU bound程序。换言之，程序要有相当的计算量。
  * (5) 利用异步操作。比如logging。无论往磁盘写log file，还是往log server发送消息都不应该阻塞critical path。
  * (6) 能scale up。一个好的多线程程序应该能享受增加CPU数目带来的好处，目前主流是8核，很快就会用到16核的机器了。
  * (7) 具有可预测的性能。随着负载增加，性能缓慢下降，超过某个临界点之后会急速下降。线程数目一般不随负载变化。
  * (8) 多线程能有效地划分责任与功能，让每个线程的逻辑比较简单，任务单一，便于编码。而不是把所有逻辑都塞到一个event loop里，不同类别的事件之间相互影响。
  
上面这些条件比较抽象，这里举两个具体的（虽然是虚构的） 例子
<span style="color:red;  font-weight:bold"> 例一：Linux服务器机群管理程序 </span>

&emsp;&emsp; 假设要管理一个Linux服务器机群， 这个机群里有8个计算节点，1个控制节点。机器的配置都是一样的：双路四核CPU，千兆网互联。现在需要编写一个简单的机群管理软件（参考LLNL的SLURM 20），这个软件由3个程序组成：
> (1) 运行在控制节点上的master，这个程序监视并控制整个机群的状态。
> (2) 运行在每个计算节点上的slave，负责启动和终止job，并监控本机的资源。 
> (3) 供最终用户使用的client命令行工具，用于提交job。
>
根据前面的分析，slave是个“看门狗进程”，它会启动别的job进程，因此必须是个单线程程序。另外它不应该占用太多的CPU资源，这也适合单线程模型。master应该是个模式(2)的多线程程序：
> (1)  它独占一台8核的机器， 如果用模型(1)，等于浪费了87.5％的CPU资源。
> (2)  整个机群的状态应该能完全放在内存中，这些状态是共享且可变的。如果用模式(3)，那么进程之间的状态同步会成大问题。而如果大量使用共享内存，则等于是掩耳盗铃，是披着多进程外衣的多线程程序。因为一个进程一旦在临界区内阻塞或crash， 其他进程会全部死锁。
> (3)  master的主要性能指标不是throughput， 而是latency， 即尽快地响应各种事件。 它几乎不会出现把IO或CPU跑满的情况。
> (4)  master监控的事件有优先级区别，一个程序正常运行结束和异常崩溃的处理优先级不同， 计算节点的磁盘满了和机箱温度过高这两种报警条件的优先级也不同。 如果用单线程， 则可能会出现优先级反转。
> (4)  假设master和每个slave之间用一个TCP连接，那么master采用2个或4个IO线程来处理8个TCPconnections能有效地降低延迟。
> (5)  master要异步地往本地硬盘写log，这要求logging library有自己的IO线程。
> (6)  master有可能要读写数据库，那么数据库连接这个第三方library可能有自己的线程， 并回调master的代码。
> (7)  master要服务于多个clients，用多线程也能降低客户响应时间。也就是说它可以再用2个IO线程专门处理和clients的通信。
> (8)  master还可以提供一个monitor接口， 用来广播推送（pushing）机群的状态，这样用户不用主动轮询（polling）。这个功能如果用单独的线程来做，会比较容易实现， 不会搞乱其他主要功能。
> 
综上所述，master一共开了10个线程：
> 4个用于和slaves通信的IO线程。
> 1个logging线程。
> 1个数据库IO线程。
> 2个和clients通信的IO线程。
> 1个主线程， 用于做些背景工作， 比如job调度。
> 1个pushing线程， 用于主动广播机群的状态。
> 
虽然线程数目略多于core数目，但是这些线程很多时候都是空闲的，可以依赖OS的进程调度来保证可控的延迟。

<span style="color:red; font-weight:bold"> 例二：TCP聊天服务器 </span>

&emsp;&emsp; 注意，这里的“聊天”不完全指人与人聊天，也可能是机器与机器“聊天”。这种服务的特点是并发连接之间有数据交换（如果多个连接分属不同的进程，则数据交互会很不方便），从一个连接收到的数据要转发给其他多个连接。因此我们不能按模式(3)的做法，把多个连接分到多个进程中分别处理（因为并发连接之间有数据交换，这会带来复杂的进程间通信），而只能用模式(1)或者模式(2)。如果纯粹只有数据交换，那么我想模式(1)也能工作得很好，因为现在的CPU足够快， 单线程应付几百个连接不在话下。
&emsp;&emsp; 如果功能进一步复杂化，加上关键字过滤、黑名单、防灌水等等功能，甚至要给聊天内容自动加上相关连接，每一项功能都会占用CPU资源。这时就要考虑模式(2)了，因为单个CPU的处理能力显得捉襟见肘，顺序处理导致消息转发的延迟增加。这时我们考虑把空闲的多个CPU利用起来，自然的做法是把连接分散
到多个线程上，例如按round-robin的方式把1000个客户连接分配到4个IO线程上。这样充分利用多核加速。具体的例子见 *书中6.6* 的方案9，以及*书中7.12* 的实现。
**上面都是书中原话，总结一下就是：**
> 模式(1)（即，运行 一个 单线程的进程） 不适合是因为，聊天程序可能需要处理成千上万的用户连接，因为单线程只能占用一个核，无法使用多核加速，这样不能充分使用CPU资源，会造成消息转发有明显的延时。
> 模式(3) （即，运行 多个 单线程的进程）可以充分发挥多核性能，但是因为该模式运行了多个进程，而在聊天程序中，不同的连接需要进行数据交互（毕竟是多个用户(一个用户可以看做一个连接)，而多进程之间信息交换比较麻烦，因此也不合适
> 模式(2)(即，运行 一个 多线程的进程) 充分发挥多核性(因为有多个线程)，而且通信很方便(这些线程都属于一个线程)，因此使用这个模式最合理。
> 
#### 16.3.2.3 总结
&emsp;&emsp; 首先要明确的是，使用多线程程序的目的是为了提高程序性能，如何多线程可以提高程序性能，那么就该使用多线程，比如前面的TCP聊天服务器的例子。
&emsp;&emsp; 但有的时候，多线程程序并不一定有性能优势，比如：
* &emsp;&emsp; 对于静态Web服务器，或者FTP服务器，CPU的负载较轻，主要瓶颈在磁盘IO和网络IO方面。这时候往往一个单线程的程序（模式1） 就能撑满IO。 用多线程并不能提高吞吐量， 因为IO硬件容量已经饱和了。 同理， 这时增加CPU数目也不能提高吞吐量。
* &emsp;&emsp; CPU跑满的情况比较少见，这里我只好虚构一个例子。假设有一个服务，它的输入是n个整数，问能否从中选出m个整数，使其和为0（这里n＜100, m＞0）。这是著名的subset sum问题，是NP-Complete的。对于这样一个“服务”，哪怕很小的n值也会让CPU算死。比如n＝30，一次的输入不过200字节（32-bit整数），CPU的运算时间却能长达几分钟。 对于这种应用， 模式3a是最适合的，能发挥多核的优势， 程序也简单。

&emsp;&emsp; **总的来说**，就是如果多线程能提高程序性能，那就使用多线程；如果多线程不能提高程序性能，那就别用，因为做了也没啥效果，反而增加程序的复杂程度。

## 16.4 一个多线程程序中，它的线程可以分为哪几类？
据作者的经验， 一个多线程服务程序中的线程大致可分为3类：
**(1) IO线程**
&emsp;&emsp; 这类线程的主循环是IO multiplexing，阻塞地等在`select/poll/epoll_wait`系统调用上。这类线程也处理定时事件。当然它的功能不止IO， 有些简单计算也可以放入其中， 比如消息的编码或解码。
**(2) 计算线程**
&emsp;&emsp; 这类线程的主循环是blockingqueue，阻塞地等在`conditionvariable`上。 这类线程一般位于`thread pool`中。 这种线程通常不涉及IO， 一般要避免任何阻塞操作。
**(3) 第三方库所用的线程**
&emsp;&emsp; 比如`logging`， 又比如`database connection`。

## 16.5 为什么说，在这个时代，多线程是不可避免的？
&emsp;&emsp; 因为现在是多核时代，要想充分发挥CPU性能，多线程编程是不可避免的。






&emsp;
&emsp;
&emsp; 
# 17. Linux上的线程标识
## 17.1 Linux有哪些方法可以用来表示一个线程？
一共有两种方法：
&emsp; (1) Pthreads 自带的函数`pthread_self()`的返回值；
&emsp; (2) 系统调用 `gettid()`的返回值；

## 17.2 书中建议如何标识一个线程？为什么？
### 17.2.1 建议的做法
&emsp;&emsp; 书中建议使用系统调用 `gettid()`的返回值来表示一个线程；
### 17.2.2 原因
<span style="color:red; font-size:18px; font-weight:bold"> (1) 为什么不使用`pthread_self()`的返回值 </span>

之所以不使用`pthread_self()`的返回值 来标识一个线程，主要有两个原因：
**原因一：`pthread_self()`的返回值 的类型不够明确：**
&emsp;&emsp; 因为 `pthread_t` 不一定是一个数值类型（整数或指针），也有可能是一个结构体，因此`Pthreads`专门提供了`pthread_equal()`函数用于对比两个线程标识符是否相等。 这就带来一系列问题， 包括：
> ① 无法打印输出`pthread_t`， 因为不知道其确切类型。 也就没法在日志中用它表示当前线程的id。
> ② 无法比较`pthread_t`的大小或计算其hash值， 因此无法用作关联容器的key。
> ③ 无法定义一个非法的`pthread_t`值，用来表示绝对不可能存在的线程id，因此`MutexLock class`没有办法有效判断当前线程是否已经持有本锁。
> ④ pthread_t值只在进程内有意义，与操作系统的任务调度之间无法建立有效关联。比方说在/proc文件系统中找不到pthread_t对应的task。

**原因二：`pthread_self()`返回的线程ID并不绝对唯一**
&emsp;&emsp; `glibc`的`Pthreads`实现实际上把`pthread_t`用作一个结构体指针（它的类型是`unsigned long`），指向一块动态分配的内存，而且这块内存是反复使用的。这就造成`pthread_t`的值很容易重复。 `Pthreads`只保证同一进程之内，同一时刻的各个线程的id不同；不能保证同一进程先后多个线程具有不同的id， 更不要说一台机器上多个进程之间的id唯一性了。
&emsp;&emsp; 例如下面这段代码中先后两个线程的标识符是相同的：
```cpp
void* thread_func(void *arg) {   }

int main()
{
    pthread_t t1, t2;
    pthread_create(&t1, NULL, thread_func, NULL);
    cout << t1 << endl;
    pthread_join(t1, NULL);

    pthread_create(&t2, NULL, thread_func, NULL);
    cout << t2 << endl; 
    pthread_join(t2, NULL);
}
```
运行结果如下：
```
140553174914816
140553174914816
```
<span style="color:green; font-weight:bold"> 综上所述，`pthread_t`并不适合用作程序中对线程的标识符 </span>

<span style="color:red; font-size:18px; font-weight:bold"> (2) 为什么使用 系统调用`gettid()`的返回值 </span>

使用`gettid()`系统调用的返回值作为线程id， 这么做的好处有：
> ① 它的类型是`pid_t`， 其值通常是一个小整数，便于在日志中输出。
> ② 在现代Linux中， 它直接表示内核的任务调度`id`，因此在`/proc`文件系统中可以轻易找到对应项： `/proc/tid`或`/prod/pid/task/tid`。
> ③ 方便用Linux的其他系统工具定位到具体某一个线程，例如：在`top()中`我们可以按线程列出任务， 然后找出CPU使用率最高的线程id，再根据程序日志判断到底哪一个线程在耗用CPU
> ④ 任何时刻都是全局唯一的，并且由于Linux分配新`pid`采用递增轮回办法，短时间内启动的多个线程也会具有不同的线程`id`。
> ⑤ `0`是非法值， 因为操作系统第一个进程init的pid是`1`
> 

### 17.2.3 如何保证性能？
&emsp;&emsp; 每次都执行一次系统调用`gettid()`似乎有些浪费，如何才能做到更高效呢？
> 用`__thread变量`来缓存`gettid()`的返回值，这样只有在本线程第一次调用的时候才进行系统调用， 以后都是直接从`thread local`缓存的线程id拿到结果，效率无忧。多线程程序在打日志的时候可以在每一条日志消息中包含当前线程的id， 不必担心有效率损失。
> 






&emsp;
&emsp;
&emsp; 
# 18. 线程的创建与销毁的守
## 18.1 线程的创建
### 18.1.1 线程的创建需要遵守的原则是？为什么？
> ① 程序库不应该在未提前告知的情况下创建自己的“背景线程”。
> ② 尽量用相同的方式创建线程， 例如`muduo::Thread`。
> ③ 在进入`main()`函数之前不应该启动线程。
> ④ 程序中线程的创建最好能在初始化阶段全部完成。
> 
下面来解释一下原因：
<span style="color:green; font-weight:bold"> ① 程序库不应该在未提前告知的情况下创建自己的“背景线程”</span>

&emsp;&emsp; 线程是稀缺资源，一个进程可以创建的并发线程数目受限于地址空间的大小和内核参数，一台机器可以同时并行运行的线程数目受限于CPU的数目。因此我们在设计一个服务端程序的时候要精心规划线程的数目，特别是根据机器的CPU数目来设置工作线程的数目，并为关键任务保留足够的计算资源。如果程序库在背地里使用了额外的线程来执行任务， 我们这种资源规划就漏算了。可能会导致高估系统的可用资源，结果处理关键任务不及时，达不到预设的性能指标。
&emsp;&emsp; 还有一个重要原因是，一旦程序中有不止一个线程，就很难安全地`fork()`了。因此“库”不能偷偷创建线程。如果确实有必要使用背景线程，至少应该让使用者知道。另外，如果有可能，可以让使用者在初始化库的时候传入线程池或event loop对象，这样程序可以统筹线程的数目和用途，避免低优先级的任务独占某个线程。

<span style="color:green; font-weight:bold"> ② 尽量用相同的方式创建线程， 例如`muduo::Thread` </span>

&emsp;&emsp; 理想情况下，程序里的线程都是用同一个`class`创建的（如`muduo::Thread`），这样容易在线程的启动和销毁阶段做一些统一的簿记（bookkeeping）工作。比如说调用一次`muduo::CurrentThread::tid()`把当前线程id缓
存起来，以后再取线程id就不会陷入内核了。也可以统计当前有多少活动线程，进程一共创建了多少线程，每个线程的用途分别是什么。C/C++的线程不像Java线程那样有名字，但是我们可以通过Thread class实现类似的效果。 如果每个线程都是通过`muduo::Thread`启动的， 这些都不难做到。必要的话可以写一个`ThreadManager singleton class`，用它来记录当前活动线程，可以方便调试与监控。
&emsp;&emsp; 但是这不是总能做到的，有些第三方库（C语言库） 会自己启动线程，这样的“野生”线程就没有纳入全局的`ThreadManager`管理之中。`muduo::CurrentThread::tid()`必须要考虑被这种“野生”线程调用的可能， 因此它必须每次都检查缓存的线程id是否有效，而不能假定在线程启动阶段已经缓存好了id，直接返回缓存值就行了。 如果库提供异步回调，一定要明确说明会在哪个（哪些）线程调用用户提供的回调函数，这样用户可以知道在回调函数中能不能执行耗时的操作，会不会阻塞其他任务的执行。

<span style="color:green; font-weight:bold"> ③ 在进入`main()`函数之前不应该启动线程 </span>

&emsp;&emsp; 在`main()`函数之前不应该启动线程，因为这会影响全局对象的安全构造。我们知道，C++保证在进入
`main()`之前完成全局对象的构造。同时，各个编译单元之间的对象构造顺序是不确定的，我们也有一些办法来影响初始化顺序，保证在初始化某个全局对象时使用到的其他全局对象都是构造完成的。但无论如何这些全局对象的构造是依次进行的，都在主线程中完成，无须考虑并发与线程安全。如果其中一个全局对象创建了线程，那就危险了。因为这破坏了初始化全局对象的基本假设。万一将来代码改动之后造成该线程访问了未经初始化的全局对象，那么这种隐晦错误查起来就很费劲了。或许你想用锁来保证全局对象初始化完成，但是怎么保证这个全局的锁对象的构造能在线程启动之前完成呢？因此，全局对象不能创建线程。如果一个库需要创建线程，那么应该进入`main()`函数之后再调用库的初始化函数去做。

<span style="color:green; font-weight:bold"> ④ 程序中线程的创建最好能在初始化阶段全部完成 </span>

&emsp;&emsp; 不要为了每个计算任务，每次请求去创建线程。一般也不会为每个网络连接创建线程，除非并发连接数与CPU数相近。一个服务程序的线程数目应该与当前负载无关，而应该与机器的CPU数目有关，即load average有比较小（最好不大于CPU数目） 的上限。 这样尽量避免出现thrashing，不会因为负载急剧增加而导致机器失去正常响应。 这么做的重要原因是，在机器失去响应期间，我们无法探查它究竟在做什么，也没办法立刻终止有问题的进程，防止损害进一步扩大。如果有实时性方面的要求，线程数目不应该超过CPU数目，这样可以基本保证新任务总能及时得到执行， 因为总有CPU是空闲的。 最好在程序的初始化阶段创建全部工作线程， 在程序运行期间不再创建或销毁线程。 借助`muduo::ThreadPool` 和 `muduo::EventLoop`， 我们很容易就能把计算任务和IO任务分配到已有的线程， 代价只有新建线程的几分之一。

## 18.1 线程的销毁
### 18.1.1 线程的销毁有几种方式？
> ① **自然死亡：** 从线程主函数返回， 线程正常退出
> ② **非正常死亡：** 从线程主函数抛出异常或线程触发`segfault`信号等非法操作
> ③ **自杀：** 在线程中调用`pthread_exit()`来立刻退出线程
> ④ **他杀：** 其他线程调用`pthread_cancel()`来强制终止某个线程
> 
&emsp;&emsp; 线程正常退出的方式只有一种，即自然死亡。任何从外部强行终止线程的做法和想法都是错的。 佐证有： Java的Thread class把stop()、 suspend()、 destroy()等函数都废弃（deprecated） 了， Boost.Threads根本就不提供thread::cancel()成员函数。因为强行终止线程的话（无论是自杀还是他杀），它没有机会清理资源。也没有机会释放已经持有的锁，其他线程如果再想对同一个mutex加锁，那么就会立刻死锁。因此我认为不用去研究cancellation point这种“鸡肋”概念。
&emsp;&emsp; 最后，我认为如果能做到前面提到的“程序中线程的创建最好能在初始化阶段全部完成”，则线程是不必销毁的，伴随进程一直运行，彻底避开了线程安全退出可能面临的各种困难，包括Thread对象生命期管理、 资源释放等等。

### 18.1.2 `exit()`是否线程安全？为什么？
&emsp;&emsp; `exit()`在C++中不是线程安全的。它的作用除了终止进程，还会析构全局对象和已经构造完的函数静态对象。这有潜在的死锁可能：
```cpp
void someFunctionMayCallExit()
{
    exit(1);
}

class GlobalObject
{
public:
    void doit(){
        MutexLockGuard lock(mutex_);
        someFunctionMayCallExit();
    }

    ~GlobalObject(){
        printf("GlobalObject:~GlobalObject\n");
        MutexLockGuard g(mutex_);
        // clean up
        printf("GlobalObject:~GlobalObject cleanning\n");
    }
private:
    MutexLock mutex_;
    };

GlobalObject g_obj;

int main()
{
    g_obj.doit();
}
```
`GlobalObject::doit()`函数辗转调用了`exit()`， 从而触发了全局对象`g_obj`的析构。` GlobalObject`的析构函数会试图加锁`mutex`_， 而此时mutex_已经被`GlobalObject::doit()`锁住了， 于是造成了死锁。






&emsp;
&emsp;
&emsp; 
# 19. 善用`__thread`关键字
## 19.1 `__thread`是什么？
&emsp;&emsp; `__thread`是GCC内置的线程局部存储，但凡带有这种说明符的变量，每个线程都拥有一份对变量的拷贝。具体见[multi threads.md]()中的笔记。

## 19.2 线程局部存储有何优势？
&emsp;&emsp; 使用起来方便。






&emsp;
&emsp;
&emsp; 
# 20. 多线程与IO
## 20.1 能否多个线程同时读写同一个socket文件描述符？
不建议这么做，因为这么做会遇到两个问题：
> ① 关闭文件描述符会遇到race condition
> ② 无法解决解决消息收发的顺序性问题
> 

<span style="color:purple; font-weight:bold"> ① 关闭文件描述符会遇到race condition </span>

&emsp;&emsp; 首先，操作文件描述符的系统调用本身是线程安全的，我们不用担心多个线程同时操作文件描述符会造成进程崩溃或内核崩溃。
&emsp;&emsp; 但是，多个线程同时操作同一个socket文件描述符确实很麻烦，我认为是得不偿失的。需要考虑的情况如下：
> ① 如果一个线程正在阻塞地`read()`某个socket，而另一个线程`close()`了此socket。
> ② 如果一个线程正在阻塞地`accept()`某个listening socket，而另一个线程`close()`了此socket。
> ③ 更糟糕的是，一个线程正准备`read()`某个socket 而另一个线程`close()`了此socket；第三个线程又恰好`open()`了另一个文件描述符， 其`fd`号码正好与前面的socket相同。 这样程序的逻辑就混乱了。
> 

<span style="color:purple; font-weight:bold"> ② 无法解决解决消息收发的顺序性问题 </span>

现在假设不考虑关闭文件描述符，只考虑读和写，情况也不见得多好。因为socket读写的特点是不保证完整性，读100字节有可能只返回20字节， 写操作也是一样的：
> ① 如果两个线程同时`read`同一个TCP socket，两个线程几乎同时各自收到一部分数据，如何把数据拼成完整的消息？如何知道哪部分数据先到达？
> ② 如果两个线程同时`write`同一个TCP socket，每个线程都只发出去半条消息，那接收方收到数据如何处理？
> ③ 如果给每个TCP socket配一把锁，让同时只能有一个线程读或写此socket，似乎可以“解决”问题，但这样还不如直接始终让同一个线程来操作此socket来得简单。
> ④ 对于非阻塞IO，情况是一样的，而且收发消息的完整性与原子性几乎不可能用锁来保证，因为这样会阻塞其他IO线程。
> 

<span style="color:purple; font-weight:bold"> read和write分到两个线程行不行呢？ </span>
如此看来，理论上只有`read`和`write`可以分到两个线程去， 因为TCP socket是双向IO。 问题是真的值得把`read`和`write`拆开成两个线程吗？
&emsp;&emsp; 以上讨论的都是网络IO，那么多线程可以加速磁盘IO吗 首先要避免`lseek(2)`/ `read(2)`的race condition。做到这一点之后，据我看，用多个线程read或write同一个文件也不会提速。不仅如此，多个线程分别`read`或`write`同一个磁盘上的多个文件也不见得能提速。因为每块磁盘都有一个操作队列，多个线程的读写请求到了内核是排队执行的。只有在内核缓存了大部分数据的情况下， 多线程读这些热数据才可能比单线程快。多线程磁盘IO的一个思路是每个磁盘配一个线程，把所有针对此磁盘的IO都挪到同一个线程， 这样或许能避免或减少内核中的锁争用。 我认为应该用“显然是正确”的方式来编写程序， 一个文件只由一个进程中的一个线程来读写， 这种做法显然是正确的。

## 20.2 对于多线程程序，应该如何操作文件描述符呢？
&emsp;&emsp; 为了简单起见，我认为多线程程序应该遵循的原则是： 
> ① 每个文件描述符只由一个线程操作，从而轻松解决消息收发的顺序性问题，也避免了关闭文件描述符的各种race condition。
> ② 一个线程可以操作多个文件描述符，但一个线程不能操作别的线程拥有的文件描述符。这一点不难做到，muduo网络库已经把这些细节封装了。
> 

## 20.3 对于多线程程序，应该如何操作`epoll`?
&emsp;&emsp; `epoll`也遵循相同的原则。 Linux文档并没有说明： 当一个线程正阻塞在`epoll_wait()`上时， 另一个线程往此`epoll fd`添加一个新的监视`fd`会发生什么。新`fd`上的事件会不会在此次`epoll_wait()`调用中返回？ 为了稳妥起见， 我们应该把对同一个`epoll fd`的操作（添加、删除、修改、等待）都放到同一个线程中执行， 这正是我们需要`muduo::EventLoop::wakeup()`的原因。
&emsp;&emsp; 当然， 一般的程序不会直接使用`epoll、 read、 write`，这些底层操作都由网络库代劳了。

## 20.4 多线程IO的几个例外
&emsp;&emsp; 这条规则有两个例外：对于磁盘文件，在必要的时候多个线程可以同时调用`pread(2)/pwrite(2)`来读写同一个文件；对于`UDP`，由于协议本身保证消息的原子性，在适当的条件下（比如消息之间彼此独立）可以多个线程同时读写同一个`UDP`文件描述符。






&emsp;
&emsp;
&emsp; 
# 21. 在多线程程序中管理文件描述符
## 21.1 为什么说 多线程读写 文件描述符 容易“串话”？
&emsp;&emsp; Linux的文件描述符是小整数，在程序刚刚启动的时候，`0`是标准输入，`1`是标准输出，`2`是标准错误。这时如果我们新打开一个文件，它的文件描述符会是`3`，因为POSIX标准要求每次新打开文件（含socket） 的时候必须使用当前最小可用的文件描述符号码。POSIX这种分配文件描述符的方式稍不注意就会造成串话：
> **情况一：** 一个线程正准备`read(2)`某个socket， 而第二个线程几乎同时`close(2)`了此socket； 第三个线程又恰好`open(2)`了另一个文件描述符，其号码正好与前面的socket相同（因为比它小的号码都被占用了） 。 这时第一个线程可能会读到不属于它的数据，不仅如此，还把第三个线程的功能也破坏了，因为第一个线程把数据读走了（TCP连接的数据只能读一次， 磁盘文件会移动当前位置） 。 
>**另外一种情况：** 一个线程从`fd＝8`收到了比较耗时的请求，它开始处理这个请求，并记住要把响应结果发给`fd＝8`。 但是在处理过程中， `fd＝8`断开连接， 被关闭了，又有新的连接到来，碰巧使用了相同的`fd＝8`。 当线程完成响应的计算， 把结果发给`fd＝8`时，接收方已经物是人非，后果难以预料。
> 

## 21.2 如何解决上面说的“串话”的问题？
可以用RAII包装文件描述符：
> &emsp;&emsp; 用Socket对象包装文件描述符，所有对此文件描述符的读写操作都通过此对象进行，在对象的析构函数里关闭文件描述符。这样一来，只要Socket对象还活着，就不会有其他Socket对象跟它有一样的文件描述符，也就不可能串话。剩下的问题就是做好多线程中的对象生命期管理，这在第1章已经完美解决了，那就是使用智能指针来管理该对象。

## 21.3 服务端是否应该关闭 标准输出（fd＝1） 和标准错误（fd＝2）？ 不是不应该，正确做法是怎样的？
&emsp;&emsp; **不应该**，因为有些第三方库在特殊紧急情况下会往`stdout`或`stderr`打印出错信息， 如果我们的程序关闭了标准输出（fd＝1） 和标准错误（fd＝2），这两个文件描述符有可能被网络连接占用，结果造成对方收到莫名其妙的数据。 
&emsp;&emsp; **正确的做法是** 把`stdout`或`stderr`重定向到磁盘文件（最好不要是`/dev/null`），这样我们不至于丢失关键的诊断信息。当然，这应该由启动服务程序的看门狗进程完成，对服务程序本身是透明的。

## 21.4 多线程程序中应该如何管理 并发连接？
在非阻塞网络编程中， 我们常常要面临这样一种场景： 
> 从某个TCP连接A收到了一个request，程序开始处理这个request；处理可能要花一定的时间，为了避免耽误（阻塞）处理其他request，程序记住了发来request的TCP连接，在某个线程池中处理这个请求；在处理完之后，会把response发回TCP连接A。
> 但是，在处理request的过程中， 客户端断开了TCP连接A， 而另一个客户端刚好创建了新连接B。
> 
&emsp;&emsp; 我们的程序不能只记住TCP连接A的文件描述符，而应该持有封装socket连接的TcpConnection对象，保证在处理request期间TCP连接A的文件描述符不会被关闭。或者持有TcpConnection对象的弱引用（weak_ptr），这样能知道socket连接在处理request期间是否已经关闭了，该文件描述符到底是“前世”还是“今生”。
&emsp;&emsp; 否则的话， 旧的TCP连接A一断开，TcpConnection对象销毁，关闭了旧的文件描述符（RAII） ， 而且新连接B的socket文件描述符有可能等于之前断开的TCP连接（这是完全可能的， POSIX要求每次新建文件
描述符时选取当前最小的可用的整数）。当程序处理完旧连接的request时，就有可能把response发给新的TCP连接B，造成串话。
&emsp;&emsp; **为了应对这种情况**，防止访问失效的对象或者发生网络串话，muduo使用`shared_ptr`来管理
TcpConnection的生命期。这是唯一一个采用引用计数方式管理生命期的对象。如果不用s`hared_ptr`，我想不出其他安全且高效的办法来管理多线程网络服务端程序中的并发连接。

## 21.5 总结
&emsp;&emsp; 为了防止串话，最好使用RAII来将 socket文件描述符 封装一下（如封装成`TcpConnection`类），每个连接都新建一个`TcpConnection`对象，然后用智能指针来管理这个`TcpConnection`对象，这样就能避免串话了。






&emsp;
&emsp;
&emsp; 
# 22. RAII与`fork()`
&emsp;&emsp; 在编写C++程序的时候，我们总是设法保证对象的构造和析构是成对出现的，否则就几乎一定会有内存泄漏。 在现代C++中，这一点不难做到（书中§1.7）。利用这一特性，我们可以用对象来包装资源，把资源管理与对象生命期管理统一起来（RAII）。
&emsp;&emsp; 但是，假如程序会`fork()`，这一假设就会被破坏了。考虑下面这个例子，Foo对象构造了一次，但是析构了两次。
```cpp
int main()
{
    Foo foo;    //调用构造函数
    fork();     //fork为两个进程
    foo.doit(); //在父子进程中都使用foo
 
    //析构函数会被调用两次，父进程和子进程各一次
}
```
如果`Foo class`封装了某种资源，而这个资源没有被子进程继承，那么`Foo::doit()`的功能在子进程中是错乱的。 而我们没有办法自动预防这一点 总不能每次申请一个资源就去调用一次`pthread_atfork()`吧？
&emsp;&emsp; `fork()`之后， 子进程继承了父进程的几乎全部状态，但也有少数例外。子进程会继承地址空间和文件描述符， 因此用于管理动态内存和文件描述符的`RAII class`都能正常工作。 但是子进程不会继承：
> ·父进程的内存锁， `mlock(2)`、 `mlockall(2)`。
> ·父进程的文件锁， `fcntl(2)`。
> ·父进程的某些定时器， `setitimer(2)`、 `alarm(2)`、 `timer_create(2)`等等。
> ·其他， 见`man 2 fork`。
> 
通常我们会用RAII手法来管理以上种类的资源（加锁解锁、 创建销毁定时器等等），但是在fork()出来的子进程中不一定正常工作， 因为资源在fork()时已经被释放了。 比方说：
> 用RAII技法封装`timer_create()/timer_delete()`，在子进程中析构函数调用`timer_delete()`可能会出错， 因为试图释放一个不存在的资源。或者更糟糕地把其他对象持有的`timer`给释放了（如果碰巧新建的`timer_t`与之重复的话） 。
&emsp;&emsp; 因此， 我们在编写服务端程序的时候， “是否允许`fork()`”是在一开始就应该慎重考虑的问题， 在一个没有为`fork()`做好准备的程序中使用`fork()`，会遇到难以预料的问题。






&emsp;
&emsp;
&emsp; 
# 23. 多线程与fork()
## 23.1 多线程和fork()的协作性如何？为什么？
&emsp;&emsp; 多线程与`fork()`的协作性很差。 这是POSIX系列操作系统的历史包袱。因为长期以来程序都是单线程的，`fork()`运转正常。当20世纪90年代初期引入多线程之后，`fork()`的适用范围大为缩减。

## 23.2 当多线程进程 调用`fork()`会发生什么？会引发什么问题？
### 23.2.1 会发生什么？
&emsp;&emsp; 当多线程进程调用 fork()时，仅会将发起调用的线程复制到子进程中。（子进程中该线程的线程 ID 与父进程中发起 fork()调用线程的线程 ID 相一致。）其他线程均在子进程中消失，也不会为这些线程调用清理函数以及针对线程特有数据的解构函数。

### 23.2.2 会引发什么问题？
这将导致如下一些问题：
> &emsp;&emsp; **问题一：** 虽然只将发起调用的线程复制到子进程中，但全局变量的状态以及所有的 Pthreads 对象（如互斥量、 条件变量等） 都会在子进程中得以保留。（因为在父进程中为这些 Pthreads对象分配了内存，而子进程则获得了该内存的一份拷贝。）这会导致很棘手的问题。例如，假设在调用 fork()时，另一线程已然锁定了某一互斥量，且对某一全局数据结构的更新也做到了一半。此时，子进程中的该线程无法解锁这一互斥量（因为其并非该互斥量的属主），如果试图获取这一互斥量，线程会遭阻塞。此外， 子进程中的全局数据结构拷贝可能也处于不一致状态，因为对其进行更新的线程在执行到一半时消失了。
> &emsp;&emsp; **问题二：** 因为并未执行清理函数和针对线程特有数据的解构函数，多线程程序的 fork()调用会导致子进程的内存泄漏。另外，子进程中的线程很可能无法访问（父进程中）由其他线程所创建的线程特有数据项，因为（子进程）没有相应的引用指针。
> 

## 23.3 鉴于多线程进程中调用`fork()`会引发这么多问题，我们应该怎么做呢？
**方法一： `fork()`后立即调用`exec()`**
&emsp;&emsp; 由于这些问题，推荐在多线程程序中调用 `fork()`的唯一情况是：其后紧跟对 `exec()`的调用。因为新程序会覆盖原有内存， `exec()`将导致子进程的所有 Pthreads 对象消失。
**方法二：调用`pthread_atfork()`来创建 fork 处理函数**






&emsp;
&emsp;
&emsp; 
# 24. 多线程与signal
TODO:







&emsp;
&emsp;
&emsp; 
# 26. 编写多线程C++程序需遵循的原则
&emsp;&emsp; ① 线程是宝贵的， 一个程序可以使用几个或十几个线程。 一台机器上不应该同时运行几百个、 几千个用户线程，因为这会大大增加内核scheduler的负担， 降低整体性能。
&emsp;&emsp; ② 线程的创建和销毁是有代价的， 一个程序最好在一开始创建所需的线程， 并一直反复使用。 不要在运行期间反复创建、 销毁线程， 如果必须这么做， 其频度最好能降到1分钟1次（或更低） 。
&emsp;&emsp; ③ 每个线程应该有明确的职责， 例如IO线程（运行EventLoop::loop()， 处理IO事件） 、 计算线程（位于ThreadPool中， 负责计算） 等等。
&emsp;&emsp; ④ 线程之间的交互应该尽量简单，理想情况下，线程之间只用消息传递（例如BlockingQueue） 方式交互。 如果必须用锁， 那么最好避免一个线程同时持有两把或更多的锁， 这样可彻底防止死锁。
&emsp;&emsp; ⑤ 要预先考虑清楚一个mutable shared对象将会暴露给哪些线程 每个线程是读还是写，读写有无可能并发进行。






&emsp;
&emsp;
&emsp; 
# 27. 日志(logging)
## 27.1 在服务端编程中， 日志一般要记录哪些内容？为什么要记录这些内容？
(1) 时间戳（精确到微秒）
> 每条日志都有时间戳， 这样就能完整追踪分布式系统中一个事件的来龙去脉。 也只有这样才能查清楚发生故障时究竟发生了什么， 比如业务处理流程卡在了哪一步。
> 
(2) 线程ID
> 便于分析多线程程序的时序， 也可以检测死锁
> 
(3) 级别（level）
> 日志消息有多种级别 ，如：TRACE、 DEBUG、 INFO、 WARN、 ERROR、 FATAL等，在线查错的时候先看看有无ERROR日志， 通常可加速定位问题。
> 
(4) 日志的正文
> 收到的每条内部消息的id（还可以包括关键字段、 长度、 hash等） ；
> 收到的每条外部消息的全文；
> 发出的每条消息的全文，每条消息都有全局唯一的id；
> 关键内部状态的变更， 等等。
> 
(5) 源文件和行号
> 修复bug的时候不至于搞错对象
> 

## 27.2 日志级别有什么用？
(1) 在线查错的时候先看看有无ERROR日志，通常可加速定位问题.
(2) 日志的输出级别在运行时可调， 这样同一个可执行文件可以分别在QA测试环境的时候输出DEBUG级别的日志， 在生产环境输出INFO级别的日志 15 。 在必要的时候也可以临时在线调整日志的输出级别。 例如某台机器的消息量过大、 日志文件太多、 磁盘空间紧张， 那么可以临时调整为WARNING级别输出， 减少日志数目。 又比如某个新上线的进程的行为略显古怪， 则可以临时调整为DEBUG级别输出， 打印更细节的日志消息以便分析查错。 

## 27.3 日志的归档应该由谁来完成？为什么？
&emsp;&emsp; 日志文件压缩与归档 16 （archive） 不是日志库应有的功能， 而应该交给专门的脚本去做。因为这样的话C++和Java的服务程序 可以共享这一基础设施。 如果想更换日志压缩算法或归档策略也不必动业务程序， 改改周边配套脚本就行了。

## 27.4 磁盘空间满了之后是否应该将之前的日志清空吗？
&emsp;&emsp; 不应该，如果出现程序死循环拼命写日志的异常情况，**那么往往是开头的几条日志最关键**，它往往反映了引发异常（busy-loop）的原因（例如收到某条非法消息），后面都是无用的垃圾日志。如果日志库具备重复利用空间的“功能”，只会帮倒忙。磁盘写入的带宽按100MB/s计算，写满一个100GB的磁盘分区需要16分钟， 这足够监控系统报警并人工干预了。

## 27.5 在程序奔溃时，如何保证最后几条日志不会丢失？
&emsp;&emsp; 往文件写日志的一个常见问题是， 万一程序崩溃，那么最后若干条日志往往就丢失了，因为日志库不能每条消息都flush硬盘，更不能每条日志都`open/close`文件， 这样性能开销太大。
&emsp;&emsp; muduo日志库用两个办法来应对这一点：
> 其一是定期（默认3秒） 将缓冲区内的日志消息flush到硬盘； 
> 其二是每条内存中的日志消息都带有cookie（或者叫哨兵值/sentry），其值为某个函数的地址， 这样通过在core dump文件中查找cookie 就能找到尚未来得及写入磁盘的消息。
> 

## 27.6 日志的格式应该满足什么要求？
**(1) 格式应该保持不变**
&emsp;&emsp; 日志消息的格式最好是固定的，方便用脚本分析。 因为我们经常会为不同目的编写parse日志的脚本， 既要解析最近几天的日志文件， 也要和几个月之前， 甚至一年之前的日志文件的同类数据做对比。 如果在此期间日志格式变了， 势必会增加很多无谓的工作量。 
**(2) 尽量每条日志占一行** 
&emsp;&emsp; 这样很容易用awk、 sed、 grep等命令行工具来快速联机分析日志， 比方说要查看`2012-06-03 08:02:00”至“2012-06-03 08:02:59`这1分钟内每秒打印日志的条数（直方图） ，可以运行
```shell
grep -o '^20120603 08:02:..' | sort | uniq -c
```
**(3) 时间戳精确到微秒**
&emsp;&emsp; 每条消息都通过`gettimeofday(2)`获得当前时间，这么做不会有什么性能损失。 因为在x86-64 Linux上， `gettimeofday(2)`不是系统调用，不会陷入内核。
**(4) 始终使用GMT时区（Z）**
&emsp;&emsp; 对于跨洲的分布式系统而言，可省去本地时区转换的麻烦（别忘了主要西方国家大多实行夏令时），更易于追查事件的顺序。
**(5) 避免出现正则表达式的元字符**
&emsp;&emsp; 应该避免在日志格式（特别是消息id）中出现正则表达式的元字符（meta character），例如'['和']'等等，这样在用`less(1)`查看日志文件的时候查找字符串更加便捷。
**(6) 打印线程id**
&emsp;&emsp; 便于分析多线程程序的时序，也可以检测死锁。这里的线程id是指调用`LOG_INFO <<`的线程。
**(7) 打印日志级别**
&emsp;&emsp; 在线查错的时候先看看有无`ERROR`日志， 通常可加速定位问题。
**(8) 打印源文件名和行号**
&emsp;&emsp; 修复bug的时候不至于搞错对象。

## 27.7 日志的性能
### 27.7.1 日志的性能重要吗？
&emsp;&emsp; 重要，因为只有日志库足够高效，程序员才敢在代码中输出足够多的诊断信息，减小运维难度，提升效率。如果日志的性能太低，那程序员肯定不敢打印太多的日志。

### 27.7.2 日志的高效性
#### 27.7.2.1 日志的高效性体现在哪几个方面？
日志服务的高效性体现在几方面：
&emsp; (1) 每秒写几千上万条日志的时候没有明显的性能损失。
&emsp; (2) 能应对一个进程产生大量日志数据的场景，例如1GB/min。
&emsp; (3) 不阻塞正常的执行流程。
&emsp; (4) 在多线程程序中，不造成争用（contention）。这里列举一些具体的性能指标，考虑往普通7200rpm SATA硬盘写日志文件的情况：
> &emsp;&emsp; ① 磁盘带宽约是110MB/s， 日志库应该能瞬时写满这个带宽（不必持续太久）。
> &emsp;&emsp; ② 假如每条日志消息的平均长度是110字节， 这意味着1秒要写100万条日志。
> 
以上是“高性能”日志库的最低指标。 如果磁盘带宽更高， 那么日志库的预期性能指标也会相应提高。反过来说， 在磁盘带宽确定的情况下， 日志库的性能只要“足够好”就行了。

#### 27.7.2.2 为什么需要保证程序可以 1秒写100万条 日志消息呢？ 
&emsp;&emsp; 换一个角度其实很容易想明白，如果一个程序耗尽全部CPU资源和磁盘带宽可以做到1秒写100万条日志消息，那么当只需要1秒写10万条日志的时候，立刻就能腾出90％的资源来干正事（处理业务）。相反，如果一个日志库在满负荷的情况下只能1秒写10万条日志，真正用到生产环境，恐怕就只能1秒写1万条日志才不会影响正常业务处理，这其实钳制了服务器的吞吐量。
&emsp;&emsp; **总的来说，就是为了不影响正常业务的处理。**

#### 27.7.2.3 每次打印日志时都要打印时间戳，如何提高性能呢？
&emsp;&emsp; 时间戳字符串中的日期和时间两部分是缓存的，一秒之内的多条日志只需重新格式化微秒部分。 例如此处 出现的3条日志消息中：

<div align="center"> <img src="./pic/muduo的几条日志消息.png"> </div>

`20120603 08:02:46`是复用的，每条日志只需要格式化微秒部分（`.125770Z`）

### 27.7.3 多线程 异步 日志
#### 27.7.3.1 多线程日志程序如何处理 不同线程的日志消息会出现交织 的情况？
有三种方法可以用：
&emsp;&emsp; 方法一：用一个全局mutex保护IO，但这会造成全部线程抢一个锁。
&emsp;&emsp; 方法二：每个线程单独写一个日志文件，这可能会让业务线程阻塞在写磁盘操作上，而且多线日志文件会增加分析日志的困难。
&emsp;&emsp; 方法三：使用 异步日志。
#### 27.7.3.2 什么是异步日志？它有何优势？
&emsp;&emsp; 用一个背景线程负责收集日志消息， 并写入日志文件， 其他业务线程只管往这个“日志线程”发送日志消息， 这称为“异步日志”，也称为 非阻塞日志。
&emsp;&emsp; 在多线程服务程序中，异步日志（叫“非阻塞日志”似乎更准确）是必需的，因为如果在网络IO线程或业务线程中直接往磁盘写数据的话，写操作偶尔可能阻塞长达数秒之久（原因很复杂，可能是磁盘或磁盘控制器复位）。这可能导致请求方超时，或者耽误发送心跳消息，在分布式系统中更可能造成多米诺骨牌效应，例如误报死锁引发自动failover等。因此，在正常的实时业务处理流程中应该彻底避免磁盘IO， 这在使用one loop per thread模型的非阻塞服务端程序中尤为重要，因为线程是复用的，阻塞线程意味着影响多个客户连接。
&emsp;&emsp; 异步日志的好处是 可以解放工作线程，使其不会阻塞于磁盘IO。
#### 27.7.3.3 异步日志 如何保证 消息不会出现交织？
&emsp;&emsp; 可以用一个“队列”来将日志前端的数据传送到后端（日志线程），而且不是每次产生一条日志消息都通知（notify()）接收方。

#### 27.7.3.4 什么是双缓冲技术？
&emsp;&emsp; 双缓冲（double buffering）技术的基本思路是 准备两块buffer：A和B，前端负责往buffer A填数据（日志消息），后端负责将buffer B的数据写入文件。当buffer A写满之后，交换A和B，让后端将buffer A的数据写入文件，而前端则往buffer B填入新的日志消息，如此往复。
&emsp;&emsp; 用两个buffer的好处是在新建日志消息的时候不必等待磁盘文件操作，也避免每条新日志消息都触发（唤醒）后端日志线程。换言之，前端不是将一条条日志消息分别传送给后端，而是将多条日志消息拼成一个大的buffer传送给后端，相当于批处理，减少了线程唤醒的频度，降低开销。另外，为了及时将日志消息写入文件，即便buffer A未满，日志库也会每3秒执行一次上述交换写入操作。

### 27.4 muduo库的双缓冲技术原理是？
muduo库实际实现采用了四个缓冲区，这样可以进一步减少或避免日志前端的等待。数据结构如下：
```cpp
typedef boost::ptr_vector<LargeBuffer> BufferVector;
typedef BufferVector::auto_type             BufferPtr;

muduo::MutexLock mutex_;
muduo::Condition cond_;

BufferPtr        currentBuffer_; //当前缓冲
BufferPtr        nextBuffer_;    //预备缓冲
BufferVector     buffers_;       //待写入文件的已填满的缓冲
```
其中，`LargeBuffer` 类型是 `FixedBuffer classtemplate` 的一份具体实现，其大小为4MB，可以存至少1000条日志消息。`boost::ptr_vector<T>::auto_type`类型类似C++11中的`std::unique_ptr`， 具备移动语
义，而且能自动管理对象生命期。 mutex_用于保护后面的四个数据成员。 buffers_存放
的是供后端写入的buffer。
先来看发送方代码：
```cpp
void AsyncLogging::append(const char* logline, int len)
{
    muduo::MutexLockGuard lock(mutex_);
    if (currentBuffer_->avail() > len){
        // most common case: buffer is not full,copy data here
        currentBuffer_->append(logline, len);
    }else{// buffer is full, push it, and find next spare buffer 
        buffers_.push_back(std::move(currentBuffer_));

        if (nextBuffer_){// is there is one already, use it 
            currentBuffer_ = std::move(nextBuffer_); // 移动
        }
        else{ // allocate a new one
            currentBuffer_.reset(new Buffer); // Rarely happens
        }
        currentBuffer_->append(logline, len);
        
        // 在这个分支中，已经有一个buffer被写满了，
        // 因此需要在完成移动后通知后端将 已写满的buffer 中的内容写到日志文件中去。
        cond_.notify();
    }
}
```
**为什么`if (currentBuffer_->avail() > len)`分支中不需要`cond_.notify()`？**
&emsp;&emsp; 因为此时`currentBuffer_`还没有被写满，因此不需要通知后端将缓存写到日志文件中。

```cpp
void AsyncLogging::threadFunc()
{
    BufferPtr newBuffer1(new Buffer);
    BufferPtr newBuffer2(new Buffer);
    BufferVector buffersToWrite;
    while (running_)
    {
        assert(newBuffer1 && newBuffer1->length() == 0);
        assert(newBuffer2 && newBuffer2->length() == 0);
        assert(buffersToWrite.empty());

    {
        muduo::MutexLockGuard lock(mutex_);
        if (buffers_.empty())  // 注意，这里没有用while循环
        {
            cond_.waitForSeconds(flushInterval_);
        }
        buffers_.push_back(std::move(currentBuffer_));  // 将当前buffer移动到buffers_里
        currentBuffer_ = std::move(newBuffer1);         // 移动
        buffersToWrite.swap(buffers_);                  // 指针交换，而不是赋值
        if (!nextBuffer_)
        {
            nextBuffer_ = std::move(newBuffer2);
        }
    }

    // (1) 将 buffersToWrite 中的内容写到log文件中

    // (2) 重新填充 newBuffer1和newBuffer2， 这样下一次执行的时候还有两个空闲buffer可用于替换前端的当前缓冲和预备缓冲

    // (3) 刷新缓冲区，将系统缓冲区中的内容写到log文件
}
```
函数解释：
* 首先准备好两块空闲的buffer，以备在临界区内交换（代码段8、9处）
* 在临界区内，等待条件触发（代码段10处）。这里的条件有两个：
  * 其一是超时，其二是前端写满了一个或多个buffer
  * 注意这里是非常规的condition variable用法，它没有使用while循环，而且等待时间有 上限。
* 当“条件”满足时，先将当前缓冲（currentBuffer_）移入 buffers_（代码段11处），并立刻将空闲的newBuffer1移为当前缓冲（代码段12处）。注意这整段代码位于临界区之内，因此不会有任何race condition
* 接下来将buffers_与buffersToWrite交换（代码段13处），后面的代码可以在临界区之外安全地访问buffersToWrite，将其中的日志数据写入文件（代码段15处）
* 临界区里最后干的一件事情是用newBuffer2替换nextBuffer_（代码段14处），这样前端始终有一个预备buffer可供调配。nextBuffer_可以减少前端临界区分配内存的概率，缩短前端临界区长度
注意到后端临界区内也没有耗时的操作，运行时间为常数
代码段16处会将buffersToWrite内的buffer重新填充newBuffer1和 newBuffer2，这样下一次执行的时候还有两个空闲buffer可用于替换前端的当前缓冲和预备缓冲。最后，这四个缓冲在程序启动的时候会全部填充为0，这样可以避免程序热身时page fault引发性能不稳定

#### 27.4.2 muduo日志模块的运行时一共有哪几种情况？
&emsp;&emsp; 详见书中`P80`的运行图示。

#### 27.4.3 除了上面的4个缓冲区的做法，还有什么其他改进方案吗？这个改进方案有何优缺点？
&emsp;&emsp; 前面我们一共准备了四块缓冲， 应该足以应付日常的需求。 如果需要进一步增加buffer数目， 可以改用下面的数据结构：
```cpp
BufferPtr currentBuffer_;   // 当前缓冲
BufferVector emptyBuffers;  // 空闲的缓冲
BufferVector fullBuffers；  // 已写满的缓冲
```
初始化时在`emptyBuffers_`中放入足够多空闲`buffer`， 这样前端几乎不会遇到需要在临界区内新分配`buffer`的情况， 这是一种空间换时间的做法。 为了避免短时突发写大量日志造成新分配的buffer占用过多内存， 后端代码应该保证`emptyBuffers_`和`fullBuffers_`的长度之和不超过某个定值。 `buffer`在前端和后端之间流动， 形成一个循环：

<div align='center'> <img src="./pic/双缓冲的改进.png"> </div>

**优缺点**
&emsp;&emsp; 上面的改进方案其实就是以空间换时间。



TODO
# 作者在第六章抛出的几个问题，看完muduo库以后再回来看
TODO: 
6.4.1 TCP网络编程本质论
这其中有很多难点， 也有很多细节需要注意， 比方说：
如果要主动关闭连接， 如何保证对方已经收到全部数据？ 如果应用层有缓冲（这在非阻塞网络编程中
是必需的， 见下文） ， 那么如何保证先发送完缓冲区中的数据， 然后再断开连接？ 直接调用close(2)恐怕是
不行的。
如果主动发起连接， 但是对方主动拒绝， 如何定期（带back-off地） 重试？
非阻塞网络编程该用边沿触发（edge trigger） 还是电平触发（level trigger） ？ 12如果是电平触发， 那么
什么时候关注EPOLLOUT事件？ 会不会造成busy-loop？ 如果是边沿触发， 如何防止漏读造成的饥饿？
epoll(4)一定比poll(2)快吗？
在非阻塞网络编程中， 为什么要使用应用层发送缓冲区？ 假设应用程序需要发送40kB数据， 但是操作
系统的TCP发送缓冲区只有25kB剩余空间， 那么剩下的15kB数据怎么办？ 如果等待OS缓冲区可用， 会阻塞
当前线程， 因为不知道对方什么时候收到并读取数据。 因此网络库应该把这15kB数据缓存起来， 放到这个
TCP链接的应用层发送缓冲区中， 等socket变得可写的时候立刻发送数据， 这样“发送”操作不会阻塞。 如果
应用程序随后又要发送50kB数据， 而此时发送缓冲区中尚有未发送的数据（若干kB） ， 那么网络库应该将
这50kB数据追加到发送缓冲区的末尾， 而不能立刻尝试write()， 因为这样有可能打乱数据的顺序。
在非阻塞网络编程中， 为什么要使用应用层接收缓冲区？ 假如一次读到的数据不够一个完整的数据
包， 那么这些已经读到的数据是不是应该先暂存在某个地方， 等剩余的数据收到之后再一并处理？ 见
lighttpd关于\r\n\r\n分包的bug13。 假如数据是一个字节一个字节地到达， 间隔10ms， 每个字节触发一次文件
描述符可读（readable） 事件， 程序是否还能正常工作？ lighttpd在这个问题上出过安全漏洞14。
在非阻塞网络编程中， 如何设计并使用缓冲区？ 一方面我们希望减少系统调用， 一次读的数据越多越
划算， 那么似乎应该准备一个大的缓冲区。 另一方面， 我们希望减少内存占用。 如果有10000个并发连接，
每个连接一建立就分配各50kB的读写缓冲区(s)的话， 将占用1GB内存， 而大多数时候这些缓冲区的使用率
很低。 muduo用readv(2)结合栈上空间巧妙地解决了这个问题。
如果使用发送缓冲区， 万一接收方处理缓慢， 数据会不会一直堆积在发送方， 造成内存暴涨？ 如何做
应用层的流量控制？
如何设计并实现定时器？ 并使之与网络IO共用一个线程， 以避免锁。
这些问题在muduo的代码中可以找到答案。






&emsp;
&emsp;
&emsp; 
# 28. 性能测试
## 28.1 `ping pong`协议
### 28.1.1 什么是`ping pong`协议？
&emsp;&emsp; 简单地说， `ping pong`协议是客户端和服务器都实现echo协议。 当TCP连接建立时， 客户端向服务器发送一些数据， 服务器会echo回这些数据， 然后客户端再echo回服务器。 这些数据就会像乒乓球一样在客户端和服务器之间来回传送， 直到有一方断开连接为止。

### 28.1.2 `ping pong`协议 的作用是？
&emsp;&emsp; `ping pong`协议 是用来测试 **吞吐量** 的常用办法。 

### 28.1.3 `ping pong`协议 拆包吗？
&emsp;&emsp; `ping pong`协议的数据是无格式的， 双方都是收到多少数据就反射回去多少数据， 并不拆包， 这与后面的`ZeroMQ`延迟测试不同。

## 28.2 为什么要在同一台机器上测吞吐量？
&emsp;&emsp; 因为现在的CPU很快， 即便是单线程单TCP连接也能把千兆以太网的带宽跑满。 如果用两台机器， 所有的吞吐量测试结果都将是110MiB/s， 失去了对比的意义。 （用Python也能跑出同样的吞吐量， 或许可以对比哪个库占的CPU少。 ）
&emsp;&emsp; 在同一台机器上测试， 可以在CPU资源相同的情况下， 单纯对比网络库的效率。 也就是说在单线程下， 服务端和客户端各占满1个CPU， 比较哪个库的吞吐量高。

## 28.3 击鼓传花
## 28.3.1 什么是击鼓传花？
&emsp;&emsp; 测试的场景是： 有1000个人围成一圈， 玩击鼓传花的游戏， 一开始第1个人手里有花， 他把花传给右手边的人， 那个人再继续把花传给右手边的人， 当花转手100次之后游戏停止， 记录从开始到结束的时间。
&emsp;&emsp; 用程序表达是， 有1000个网络连接（`socketpair(2)`或`pipe(2)`） ， 数据在这些连接中顺次传递， 一开始往第1个连接里写1个字节， 然后从这个连接的另一头读出这1个字节， 再写入第2个连接， 然后读出来继续写到第3个连接， 直到一共写了100次之后程序停止， 记录所用的时间。

## 28.3.2 这个击鼓传花主要用来测试什么？
&emsp;&emsp; 用来测试一个库在高并发情况下的IO事件处理效率。

## 28.3.3 用击鼓传花进行性能测试时需要注意什么？
&emsp;&emsp; 以上是只有一个活动连接的场景， 我们实际测试的是100个或1000个活动连接（即100朵花或1000朵
花， 均匀分散在人群手中），而连接总数（即并发数） 从100～100000（10万）。**注意每个连接是两个文件描述符， 为了运行测试， 要调高每个进程能打开的文件数， 如设为256000。**






&emsp;
&emsp;
&emsp; 
# 29. 对于一个proxy， 既要把连接a收到的数据发给连接b， 又要把从b收到的数据发给a， 那么到底读哪个？如何解决呢？









&emsp;
&emsp;
&emsp; 
# 参考文献
1. [理解 shared_ptr实现copy-on-write（COW）](https://blog.csdn.net/zhangxiao93/article/details/52792888)