# 1. RAII
## 用在什么地方了？
加锁TODO:





&emsp;
&emsp; 
# 2. local copy






&emsp;
&emsp; 
# 3.  对象池、弱回调
TODO: 后面自己实现一遍





&emsp;
&emsp; 
# 4. 分布式系统中，一般推荐用什么 进程通信 方法? 为什么？
## 4.1  用什么？
用 消息传递(message passing)。

## 4.2  为什么？
&emsp;&emsp; 共享内存只能用于同一个机器上的进程间通信，而消息传递不但可以用于同一机器中的进程间通信，还可以用于不同机器间的通信。在单机上， 我们也可以照搬message passing作为多个进程的并发模型。 这样整个分布式系统的架构的一致性很强， 扩容（scale out） 起来也较容易。





&emsp;
&emsp; 
# 5. muduo库中为什么不推荐使用 递归mutex?
## 5.1 因为 非递归mutex性能好？
&emsp;&emsp; 不是为了性能，而是为了体现设计意图。non-recursive和recursive的性能差别其实不大， 因为少用一个计数器，前者略快一点点而已。

## 5.2 那是为什么呢？
&emsp;&emsp; 毫无疑问recursive mutex使用起来要方便一些， 因为不用考虑一个线程会自己把自己给锁死了， 我猜这也是Java和Windows默认提供recursive mutex的原因。 
&emsp;&emsp; 正因为它方便，recursive mutex可能会隐藏代码里的一些问题。典型情况是你以为拿到一个锁就能修改对象了，没想到外层代码已经拿到了锁，正在修改（或读取） 同一个对象呢。 来看一个具体的例子：
```cpp
class Foo{
public:
    void doit() const;
};

MutexLock mutex;
std::vector<Foo> foos;

void post(const Foo& f){
    MutexLockGuard lock(mutex);
    foos.push_back(f); // 
}

void traverse()
{
    MutexLockGuard lock(mutex);
    for (std::vector<Foo>::const_iterator it = foos.begin(); it != foos.end(); ++it){
        it->doit();
    }
}

void Foo::doit() const{
    Foo f;
    post(f);
}

int main()
{
    Foo f;
    post(f);
    traverse();
}
```
`post()`加锁， 然后修改`foos`对象； `traverse()`加锁， 然后遍历`foos`向量。 这些都是正确的。
将来有一天， `Foo::doit()`间接调用了`post()`， 那么会很有戏剧性的结果：
> (1) mutex是非递归的， 于是死锁了。
> (2) mutex是递归的， 由于`push_back()`可能（但不总是）导致`vector迭代器`失效（`push_back()`导致内存重新分配后，迭代器会失效），程序偶尔会crash。
> 
这时候就能体现non-recursive的优越性：把程序的逻辑错误暴露出来。死锁比较容易debug，把各个线程的调用栈打出来，只要每个函数不是特别长，很容易看出来是怎么死的， 见§2.1.2的例子 9。或者可以用`PTHREAD_MUTEX_ERRORCHECK`一下子就能找到错误（前提是MutexLock带debug选项）。程序反正要死，不如死得有意义一点，留个“全尸”，让验尸（post-mortem）更容易些。
&emsp;&emsp; **换句话来说就是：使用非递归mutex可以尽早的将问题暴露出来，这样不容易在生产环境出问题。**

## 5.3 如何解决上面代码中的死锁问题？
可以，用`shared_ptr`实现copy-on-write，这样就能解决，代码如下：
```cpp
class Foo
{
public:
    void doit() const;
};

typedef std::vector<Foo> FooList;
typedef boost::shared_ptr<FooList> FooListPtr;
FooListPtr g_foos;
MutexLock mutex;

void post(const Foo& f)
{
    printf("post\n");
    MutexLockGuard lock(mutex);

    // 理论上 全局变量 g_foos 应该只有一个拥有者，如果不只一个，
    // 那么应该就是在 traverse()中正在遍历
    if (!g_foos.unique())
    {
        // 因为有其它线程在遍历 g_foos，为了不破坏 g_foos的迭代器
        // 直接拷贝一份
        g_foos.reset(new FooList(*g_foos));
        printf("copy the whole list\n");
    }

    // 对拷贝后的 g_foos 进行修改，不会影响 traverse()
    assert(g_foos.unique());
    g_foos->push_back(f);
}

void traverse()
{
    FooListPtr foos;
    {
        // 锁是加在这个局部作用域的，出了花括号的范围就解锁了，
        // 后面的遍历是没有锁的
        MutexLockGuard lock(mutex);
        foos = g_foos;
        assert(!g_foos.unique());
    }

    // assert(!foos.unique()); this may not hold

    for (std::vector<Foo>::const_iterator it = foos->begin();
        it != foos->end(); ++it)
    {
        it->doit();
    }
}

void Foo::doit() const
{
    Foo f;
    post(f);
}

int main()
{
    g_foos.reset(new FooList);
    Foo f;
    post(f);
    traverse();
}
```






&emsp;
&emsp; 
# 6. 死锁
TODO: 只是自己实现了一下第一个程序，后面的调试涉及到`gdb`，后面学了之后再回来！
```cpp
pthread_mutex_t mtx;
pthread_mutexattr_t mtxAttr;

class Request{
public:
    void process(){
        int ret = pthread_mutex_lock(&mtx);
        print();
        cout << "process() : " << ret << endl;
        ret = pthread_mutex_unlock(&mtx);
    }

    void print(){
        int ret = pthread_mutex_lock(&mtx);
        ret = pthread_mutex_unlock(&mtx);
        cout << "The end of print() :" << ret << endl;
    }
};


int main()
{
    int s = pthread_mutexattr_init(&mtxAttr);
    // 不依赖默认属性，直接指定mutex类型为 PTHREAD_MUTEX_NORMAL
    s = pthread_mutexattr_settype(&mtxAttr, PTHREAD_MUTEX_NORMAL);
    s = pthread_mutex_init(&mtx, &mtxAttr);
    Request obj;
    obj.process();
}
```

## 6.2 自己写的时候犯的错误
(1) 编译的时候忘记加 `-pthread`选项
正确的应该是：
```shell
g++ -o test.o test.cpp --std=c++11 -pthread
```





&emsp;
&emsp; 
# 7 条件变量
## 7.1 使用建议
**对于wait端：**
> (1) 必须与mutex一起使用， 该布尔表达式的读写需受此mutex保护。
> (2) 在mutex已上锁的时候才能调用wait()。
> (3) 把判断布尔条件和wait()放到while循环中。
>
对于signal/broadcast端：
> (1) 不一定要在mutex已上锁的情况下调用signal（理论上） 。
> (2) 在signal之前一般要修改布尔表达式。
> (3) 修改布尔表达式通常要用mutex保护（至少用作full memory barrier） 。
> (4) 注意区分signal与broadcast：“broadcast通常用于表明状态变化， signal通常用于表示资源可用。
> 

## 7.2 如何确定，`pthread_cond_signal()`和 `pthread_cond_broadcast()`应该使用哪个？
(1) 下面情况适合用`pthread_cond_broadcast`：
> 一个生产者多消费者，生产者能一次产生多个产品的情况。
> 多生产者多消费者
> 读写锁实现（写入之后，通知所有读者）
> 
(2) 下面情况适合`pthread_cond_signal`的情况
> 单一生产者，生产者一次生产一个产品的情况，最好一个消费者
> 
**在muduo库2.2小节中，有这样一段话：**
> 注意区分signal与broadcast： “broadcast通常用于表明状态变化， signal通常用于表示资源可用。（broadcast should generally be used to indicate state change rather than resource availability。 ）
> 
对于这段话其实可以这么理解：
> 资源被释放时，一般只能同时供一个线程使用，因此用signal通知一个线程即
>






&emsp;
&emsp; 
# 8. 不要用 读写锁 和 信号量
## 8.1 为什么不要用 读写锁？
&emsp;&emsp; 
**从正确性方面来说**，一种典型的易犯错误是在持有read lock的时候修改了共享数据。 这通常发生在程序的维护阶段， 为了新增功能， 程序员不小心在原来read lock保护的函数中调用了会修改状态的函数。 这种错误的后果跟无保护并发读写共享数据是一样的。
**从性能方面来说**，读写锁不见得比普通mutex更高效。 无论如何reader lock加锁的开销不会比mutex lock小，因为它要更新当前reader的数目。如果临界区很小，锁竞争不激烈，那么mutex往往会更快。

## 8.2 为什么不要用 信号量？
&emsp;&emsp; 笔者说：我没有遇到过需要使用信号量的情况，无从谈及个人经验。我认为信号量不是必备的同步原语，因为条件变量配合互斥器可以完全替代其功能，而且更不易用错。除了[RWC]指出的“semaphore has no notion of ownership”之外，信号量的另一个问题在于它有自己的计数值，而通常我们自己的数据结构也有长度值，这就造成了同样的信息存了两份，需要时刻保持一致，这增加了程序员的负担和出错的可能。如果要控制并发度，可以考虑用`muduo::ThreadPool`。






&emsp;
&emsp; 
# 9 用RAII封装`mutex`类
有几点要注意：
(1) 不应该使用系统默认的互斥锁类型（即`PTHREAD_MUTEX_DEFAULT`），而应该用`mutexattr`来显示指定mutex的类型（这里是`PTHREAD_MUTEX_NORMAL`）。（在本文前面关于 [死锁]()的介绍中就是这么做的 ）






&emsp;
&emsp; 
# 9 封装`Condition`类
TODO: 代码分析

## 9.2 同时使用`mutex`和`condition`时需要注意什么？
&emsp;&emsp; 注意它们的声明顺序和初始化顺序， mutex_应先于condition_构造， 并作为后者的构造参数。
&emsp;&emsp; 因为成员变量在类中的声明次序 就是 其在初始化列表中的初始化顺序，与其在初始化列表中的先后次序无关。






&emsp;
&emsp; 
# 10. 线程安全的`Singleton`实现







&emsp;
&emsp; 
# 11. sleep(3) 不是同步原语
&emsp;&emsp; `sleep()/usleep()/nanosleep()`只能出现在测试代码中 比如写单元测试的时候；或者用于有意延长临界区，加速复现死锁的情况。







&emsp;
&emsp; 
# 12. 借`shared_ptr`实现`copy-on-write (COW)`
## 12.1 如何用 `shared_ptr`实现`copy-on-write`？
概括的来说，**COW技术的精髓是**：
> (1) 如果你是数据的唯一拥有者，那么你可以直接修改数据。
> (2) 如果你不是数据的唯一拥有者，那么你拷贝它之后再修改。
> 
用`shared_ptr`来实现COW时，主要考虑两点：
> (1) 读数据
> (2) 写数据
> 
&emsp;&emsp; `shared_ptr`拥有对对象的引用计数，在对对象进行读写操作时，这个计数是`1`，当读数据时，我们 创建一个新的智能指针 指向 原智能指针指向的对象，这个时候引用计数加`1`（变为`2`），这么做其实就是为了提醒其它线程，这个对象还有其他人持有，你别乱改。
&emsp;&emsp; **概括的来说**，就是通过新建一个 局部`shared_ptr` 指向共享的那个 全局`shared_ptr`所指向的对象，让其引用计数增加，这样其它线程就能知道你在使用该对象，其它线程就会 “自觉地” 拷贝后再修改：
```cpp
//假设g_ptr是一个全局的shared_ptr<Foo>并且已经初始化。
void read()
{
    shared_ptr<Foo> tmpptr; // 新建一个 局部shared_ptr
    {
        lock(); // 加锁
        /* 此时引用计数为2，这样写端就能知道有其它线程持有了g_ptr，它就不会直接修改g_ptr了
         而是会复制一份，然后修改复制的那一份，具体可以看后面 写端的实现*/
        // 引用计数加1是为了提醒其它线程，这个对象还有其他人持有，你别乱改
        tmpptr=g_ptr; // 局部智能指针tmpptr 和 全局智能指针g_ptr指向同一对象，它俩的引用计数都是2
    }
    // 注意 tmpptr指向的对象的引用计数还是为2哦，因为tmpptr还没被释放
    //访问tmpptr
    //...

} // 离开read()，tmpptr被释放，那么引用计数就减1
```
这部分是shared_ptr最基本的用法，还是很好理解的，`read()`函数调用结束，`tmpptr`作为栈上变量离开作用域，自然析构，原数据对象的引用计数也变为1。
&emsp;&emsp; 写数据就复杂一些。根据COW的准则，当你是唯一拥有者（对应对象的引用计数是1）时，那么你直接修改数据，这样没有问题，当你不是唯一拥有者，则需要拷贝数据再去修改，这就需要用到一些shared_ptr的编程技法了：
```cpp
void write()
{
    // 写端要在一开始就加锁，要不然多个线程并发write()时会导致，某些线程写入失效。
    lock() 
    if(!g_ptr.unique())
    {
        g_ptr.reset(new Foo(*g_ptr));
    }
    assert(g_ptr.unique());
    // write
    // 
}
```
解释一下代码：
`shared_ptr::unique()`：当引用计数为1时返回true，否则false。
那么当引用计数不为1的时候，说明有别的线程正在读，受shread_ptr::reset()中example的误导，一直以为，reset后，原对象被析构，这样不就会影响正在读的线程了吗？
**解释一下`write()`中的`g_ptr.reset(new Foo(*g_ptr));`**
&emsp;&emsp; 假设在同一时刻，有一个读线程A，一个写线程B，当写线程B进入到`if`循环中时，原对象的引用计数为2（写线程A也持有了`g_ptr`指向的数据，导致引用计数加1），分别为`tmpptr`（线程B）和`g_ptr`，此时`reset()`函数将原对象的引用计数减1，并且`g_ptr`已经指向了新的对象（用原对象构造），这样就完成了数据的拷贝，并且原对象还在，只是引用计数变成了1，只有在读端线程A释放了`tmpptr`后才会导致旧的数据被释放。

## 12.2 用`shared_ptr`实现COW时要注意什么呢？
read端：
> 锁只需在拷贝的时候加，最好使用local copy，或者新建一个函数来完成拷贝也是可以的（就像后面的`CustomerData::query`的第一行代码那样）。
> 
write端：
> 锁需要在一开始就加上，因为我们是需要修改共享的资源的，所以我们需要对整个函数加锁。
> 

## 12.3 copy-on-write 应用：用普通`mutex`替换读写锁的一个例子
**场景**： 一个多线程的C++程序，24h x 5.5d运行。有几个工作线程ThreadWorker{0, 1, 2, 3}， 处理客户发过来的交易请求； 另外有一个背景线程ThreadBackground， 不定期更新程序内部的参考数据。 这些线程都跟一个hash表打交道， 工作线程只读， 背景线程读写， 必然要用到一些同步机制， 防止数据损坏。 这里的示例代码用`std::map`代替`hash`表， 意思是一样的：
```cpp
typedef std::pair<string, int> Entry;
typedef std::vector<Entry> EntryList;
typedef std::map<string, EntryList> Map;
```
`Map`的`key`是用户名， `value`是一个`vector`， 里边存的是不同`stock`的最小交易间隔， `vector`已经排好序， 可以用二分查找。
&emsp;&emsp; 我们的系统要求工作线程的延迟尽可能小，可以容忍背景线程的延迟略大。一天之内，背景线程对数据更新的次数屈指可数，最多一小时一次，更新的数据来自于网络，所以对更新的及时性不敏感。Map的数据量也不大，大约一千多条数据。
&emsp;&emsp; 最简单的同步办法是用读写锁：工作线程加读锁，背景线程加写锁。但是读写锁的开销比普通mutex要大，而且是写锁优先，会阻塞后面的读锁。如果工作线程能用最普通的非重入`mutex`实现同步，就不必用读写锁，这能降低工作线程延迟。我们借助`shared_ptr`做到了这一点： 
```cpp
class CustomerData : boost::noncopyable
{
public:
    CustomerData()
        : data_(new Map)
    { }

    int query(const string& customer, const string& stock) const;

private:
    typedef std::pair<string, int> Entry;
    typedef std::vector<Entry> EntryList;
    typedef std::map<string, EntryList> Map;
    typedef boost::shared_ptr<Map> MapPtr;
    void update(const string& customer, const EntryList& entries);
    void update(const string& message);

    static int findEntry(const EntryList& entries, const string& stock);
    static MapPtr parseData(const string& message);

    MapPtr getData() const
    {
        muduo::MutexLockGuard lock(mutex_);
        return data_;
    }

    mutable muduo::MutexLock mutex_;
    MapPtr data_;
};
```
`CustomerData::query()`相当于读端，用了前面说的引用计数加`1`的办法， 用局部`MapPtr data`变量来持有`Map`，防止并发修改：
```cpp
int CustomerData::query(const string& customer, const string& stock) const
{
    // getData() 返回 data_的拷贝，这样管理data_的shared_ptr的引用计数就
    // 不再为1，而是2，如果写端(这里是背景线程)同时在操作data_，
    // 它就能知道有人持有 _data，它会复制一份再修改。
    MapPtr data = getData();

    Map::const_iterator entries = data->find(customer);
    if (entries != data->end())
        return findEntry(entries->second, stock);
    else
        return -1;
}
```
&emsp;&emsp; 关键看`CustomerData::update()`（写端）怎么写。既然要更新数据，那肯定得加锁，如果这时候其他线程正在读， 那么不能在原来的数据上修改， 得创建一个副本，在副本上修改，修改完了再替换。如果没有用户在读， 那么就能直接修改，节约一次Map拷贝。
&emsp;&emsp; 注意其中用了`shared_ptr::unique()`来判断是不是有人在读， 如果有人在读， 那么我们不能直接修改， 因为`query()`并没有全程加锁， 只在`getData()`内部有锁。`shared_ptr::swap()`把`data_`替换为新副本， 而且我们还在锁里，不会有别的线程来读， 可以放心地更新。如果别的reader线程已经刚刚通过`getData()`拿到了`MapPtr`，它会读到稍旧的数据。这不是问题，因为数据更新来自网络，如果网络稍有延迟， 反正reader线程也会读到旧的数据。
```cpp
// 每次收到一个 customer 的数据更新
void CustomerData::update(const string& customer, const EntryList& entries)
{
    // 注意，写端要在最外层加锁，要不然多个线程同时update()会导致某些线程修改失败
    muduo::MutexLockGuard lock(mutex_); //
    if (!data_.unique()) // 如果本线程不是 data_ 的唯一持有者，那就复制一份再修改
    {
        // 复制一份
        MapPtr newData(new Map(*data_)); 
        data_.swap(newData);
    }
    assert(data_.unique());
    (*data_)[customer] = entries;
}

void CustomerData::update(const string& message)
{
    MapPtr newData = parseData(message);
    if (newData)
    {
        muduo::MutexLockGuard lock(mutex_);
        data_.swap(newData);
    }
}
```





&emsp;
&emsp;
&emsp; 
# 13. one loop per thread 
这是muduo库使用的 C++多线程服务端编程模式， `one (event) loop per thread `+ `thread pool`
> `event loop`（也叫IO loop） 用作IO multiplexing，配合 non-blocking IO和定时器。
> `thread pool` 用来做计算， 具体可以是任务队列或生产者消费者队列。
> 

.TODO:不太明白这到底是是指啥，学了源码再回来看






&emsp;
&emsp;
&emsp; 
# 14 如何确定线程池的大小？
&emsp;&emsp; 程序里具体用几个loop、线程池的大小等参数需要根据应用来设定，基本的原则是“**阻抗匹配**”，使得CPU和IO都能高效地运作。
&emsp;&emsp; 如果池中线程在执行任务时，密集计算所占的时间比重为`P（0＜P≤1）`，而系统一共有`C`个CPU，为了让这`C`个CPU跑满而又不过载，线程池大小的经验公式`T＝C/P`。`T`是个hint，考虑到`P`值的估计不是很准确，`T`的最佳值可以上下浮动50％.这个经验公式的原理很简单，`T`个线程，每个线程占用`P`的CPU时间， 如果刚好占满`C`个CPU，那么必有`T×P＝C`。下面验证一下边界条件的正确性。
&emsp;&emsp; 假设`C＝8， P＝1.0`，线程池的任务完全是密集计算，那么`T＝8`。只要8个活动线程就能让8个CPU饱和，再多也没用，因为CPU资源已经耗光了。
&emsp;&emsp; 假设`C＝8， P＝0.5`，线程池的任务有一半是计算，有一半等在IO上，那么`T＝16`。考虑操作系统能灵活、合理地调度sleeping/writing/running线程，那么大概16个“50％繁忙的线程”能让8个CPU忙个不停。启动更多的线程并不能提高吞吐量，反而因为增加上下文切换的开销而降低性能。
&emsp;&emsp; 如果`P＜0.2`，这个公式就不适用了，`T`可以取一个固定值，比如`5×C`。另外，公式里的`C`不一定是CPU总数，可以是“分配给这项任务的CPU数目”，比如在8核机器上分出4个核来做一项任务，那么`C＝4`。
&emsp;&emsp; 此外，程序里或许还有个别执行特殊任务的线程，比如logging，这对应用程序来说基本是不可见的，但是在分配资源（CPU和IO）的时候要算进去，以免高估了系统的容量。






&emsp;
&emsp;
&emsp; 
# 15. 进程间通信方式的选择
## 15.1 IPC工具
&emsp;&emsp; 进程间通信我首选socket（主要指TCP），其最大的好处在于：可以跨主机，具有伸缩性。反正都是多进程了，如果一台机器的处理能力不够，很自然地就能用多台机器来处理。把进程分散到同一局域网的多台机器上，程序改改`host:port`配置就能继续用。相反，其他IPC都不能跨机器，这就限制了scalability。

## 15.2 通信协议
使用TCP协议，它有如下好处：
> ① 没有资源泄露的风险：TCP port由一个进程独占，且操作系统会自动回收（listening port和已建立连接的TCP socket都是文件描述符， 在进程结束时操作系统会关闭所有文件描述符） 。 这说明， 即使程序意外退出， 也不会给系统留下垃圾， 程序重启之后能比较容易地恢复， 而不需要重启操作系统（用跨进程的mutex就有这个风险）
> ② 既然port是独占的，那么可以防止程序重复启动，后面那个进程抢不到port，自然就没法初始化了，避免造成意料之外的结果。
> ③ 方便分析：tcpdump和Wireshark是解决两个进程间协议和状态争端的好帮手， 也是性能（吞吐量、 延迟） 分析的利器。 我们可以借此编写分布式程序的自动化回归测试。 也可以用tcpcopy 14之类的工具进行压力测试
> ④ TCP还能跨语言， 服务端和客户端不必使用同一种语言。 试想如果用共享内存作为IPC， C++程序如何与Java通信， 难道用JNI吗？
> 

## 15.3 长连接还是短连接？
建议使用长连接，它有如下好处：
> ① 容易定位分布式系统中的服务之间的依赖关系。 只要在机器上运`行netstat -tpna | grep :port`就能立刻列出用到某服务的客户端地址（Foreign列） ， 然后在客户端的机器上用`netstat`或`lsof`命令找出是哪个进程发起的连接。 这样在迁移服务的时候能有效地防止出现outage。 TCP短连接和UDP则不具备这一特性。 
> ② 通过接收和发送队列的长度也较容易定位网络或程序故障。在正常运行的时候，`netstat`打印的`Recv-Q`和`Send-Q`都应该接近0，或者在0附近摆动。 如果`Recv-Q`保持不变或持续增加，则通常意味着服务进程的处理速度变慢，可能发生了死锁或阻塞。如果`Send-Q`保持不变或持续增加，有可能是对方服务器太忙、来不及处理，也有可能是网络中间某个路由器或交换机故障造成丢包，甚至对方服务器掉线，这些因素都可能表现为数据发送不出去。通过持续监控`Recv-Q`和`Send-Q`就能及早预警性能或可用性故障。 
> 






&emsp;
&emsp;
&emsp; 
# 16. 多线程服务器的适用场合
## 16.1 多线程可用的模式 和 优缺点
如果要在一台多核机器上提供一种服务或执行一个任务，可用的模式有： 
> (1) 运行 一个 单线程的进程；
> (2) 运行 一个 多线程的进程；
> (3) 运行 多个 单线程的进程；
> (4) 运行 多个 多线程的进程。
> 
模式(1)是不可伸缩的（scalable） ， 不能发挥多核机器的计算能力。
模式(2)是被很多人所鄙视的， 认为多线程程序难写， 而且与模式(3)相比并没有什么优势。
模式(3)是目前公认的主流模式。 它有以下两种子模式：
> a 简单地把模式(1)中的进程运行多份 
> b 主进程+woker进程， 如果必须绑定到一个TCP port， 比如httpd+fastcgi
> 
模式(4)更是千夫所指， 它不但没有结合2和3的优点， 反而汇聚了二者的缺点

## 16.2 为什么说 多线程 并不是 万灵丹（silver bullet） ？
&emsp;&emsp; Paul E. McKenney在《Is Parallel Programming Hard, And, If So, What Can You Do About It?》 17第3.5节指出：
> As a rough rule of thumb, use the simplest tool that will get the job done.
> rule of thumb：(根据实际经验的)粗略估算;经验之谈; 经验法则
> 
比方说，使用速率为50MB/s的数据压缩库、在进程创建销毁的开销是800μs、线程创建销毁的开销是50μs的前提下， 考虑如何执行压缩任务：
* &emsp;&emsp; 如果要偶尔压缩1GB的文本文件，预计运行时间是20s，那么起一个进程去做是合理的，因为进程启动和销毁的开销远远小于实际任务的耗时。
* &emsp;&emsp; 如果要经常压缩500kB的文本数据，预计运行时间是10ms，那么每次都起进程似乎有点浪费了，可以每次单独起一个线程去做。
* &emsp;&emsp; 如果要频繁压缩10kB的文本数据，预计运行时间是200μs，那么每次起线程似乎也很浪费，不如直接在当前线程搞定。也可以用一个线程池，每次把压缩任务交给线程池，避免阻塞当前线程（特别要避免阻塞IO线程）。
* 
由此可见，多线程并不是万灵丹（silver bullet），它有适用的场合，也有不适合的场合。

## 16.3 那么究竟什么时候该用 多线程？什么时候该用单线程？
### 16.3.1 何时使用 单线程
#### 16.3.2.1 必须使用 单线程的场合
一般来说，有两种场合必须使用单线程：
> (1) 程序可能会`fork()`,多线程程序不是不能调用fork(2)， 而是这么做会遇到很多麻烦；
> (2) 限制程序的CPU占用率。
> 
<span style="color:red; font-size:18px; font-weight:bold"> (1) 程序可能会fork() </span>

多线程程序不是不能调用`fork()`， 而是这么做会遇到很多麻烦；一个程序`fork()`之后一般有两种行为：
> ① 立刻执行`exec()`，变身为另一个程序。例如`shell`和`inetd`；又比如`lighttpd` `fork()`出子进程， 然后运行`fastcgi`程序。 或者集群中运行在计算节点上的负责启动`job`的守护进程（即我所谓的“看门狗进程”） 。
> ② 不调用`exec()`，继续运行当前程序。要么通过共享的文件描述符与父进程通信， 协同完成任务；要么接过父进程传来的文件描述符，独立完成工作，例如20世纪80年代的Web服务器`NCSA httpd`。
>
这些行为中，我认为只有“看门狗进程”必须坚持单线程，其他的均可替换为多线程程序（从功能上讲）。
&emsp;
<span style="color:red; font-size:18px; font-weight:bold"> (2) 限制程序的CPU占用率 </span>

&emsp;&emsp; 这个很容易理解，比如在一个8核的服务器上，一个单线程程序即便发生busy-wait（无论是因为bug，还是因为overload），占满1个core，其CPU使用率也只有`12.5％`。在这种最坏的情况下，系统还是有`87.5％`的计算资源可供其他服务进程使用。
&emsp;&emsp; 因此对于一些辅助性的程序，如果它必须和主要服务进程运行在同一台机器的话（比如它要监控其他服务进程的状态），那么**做成单线程的能避免过分抢夺系统的计算资源**。比方说：
> &emsp;&emsp; 如果要把生产服务器上的日志文件压缩后备份到NFS上，那么应该使用普通单线程压缩工具（gzip/bzip2）。它们对系统造成的影响较小，在8核服务器上最多占满1个core。如果有人为了“提高速度”，开启了多线程压缩或者同时起多个进程来压缩多个日志文件， 有可能造成的结果是非关键任务耗尽了CPU资源，正常客户的请求响应变慢。这是我们不愿意看到的。

#### 16.3.2.2 单线程程序的优缺点
<span style="color:red; font-size:18px; font-weight:bold"> 优点： </span>

&emsp;&emsp; 从编码的角度，单线程程序的优势无须赘言： 编写起来简单。比如说event loop

<span style="color:red; font-size:18px; font-weight:bold"> 缺点： </span>

&emsp;&emsp; event loop 有一个明显的缺点，它是非抢占的（non-preemptive）。假设事件a的优先级高于事件b，处理事件a需要1ms，处理事件b需要10ms。如果事件b稍早于a发生，那么当事件a到来时，程序已经离开了`poll()`调用，并开始处理事件b。事件a要等上10ms才有机会被处理，总的响应时间为11ms。这等于发生了优先级反转。这个缺点可以用多线程来克服，这也是多线程的主要优势。

### 16.3.2 何时使用 多线程？
#### 16.3.2.1 多线程程序是否有性能优势？
&emsp;&emsp; 无论是`IO bound`还是`CPU bound`的服务，多线程都没有什么绝对意义上的性能优势。这句话是说：
> **如果用很少的CPU负载就能让IO跑满，或者用很少的IO流量就能让CPU跑满，那么多线程没啥用处。**
> 
举例来说：
* &emsp;&emsp; 对于静态Web服务器，或者FTP服务器，CPU的负载较轻，主要瓶颈在磁盘IO和网络IO方面。这时候往往一个单线程的程序（模式1） 就能撑满IO。 用多线程并不能提高吞吐量， 因为IO硬件容量已经饱和了。 同理， 这时增加CPU数目也不能提高吞吐量。
* &emsp;&emsp; CPU跑满的情况比较少见，这里我只好虚构一个例子。假设有一个服务，它的输入是n个整数，问能否从中选出m个整数，使其和为0（这里n＜100, m＞0）。这是著名的subset sum问题，是NP-Complete的。对于这样一个“服务”，哪怕很小的n值也会让CPU算死。比如n＝30，一次的输入不过200字节（32-bit整数），CPU的运算时间却能长达几分钟。 对于这种应用， 模式3a是最适合的，能发挥多核的优势， 程序也简单。
* 
也就是说， 无论任何一方早早地先到达瓶颈， 多线程程序都没啥优势。

#### 16.3.2.2 适用多线程程序的场景
&emsp;&emsp; **多线程的适用场景是**：提高响应速度，让IO和“计算”相互重叠，降低latency。虽然多线程不能提高绝对性能，但能提高平均响应性能。
一个程序要做成多线程的， 大致要满足：
> (1) 有多个CPU可用。单核机器上多线程没有性能优势（但或许能简化并发业务逻辑的实现） 。
> (2) 线程间有共享数据，即内存中的全局状态。如果没有共享数据，用模型`(3)b`就行。 虽然我们应该把线程间的共享数据降到最低，但不代表没有。
> (3) 共享的数据是可以修改的， 而不是静态的常量表。 如果数据不能修改， 那么可以在进程间用shared memory， 模式(3)就能胜任。
> (4) 提供非均质的服务。即，事件的响应有优先级差异，我们可以用专门的线程来处理优先级高的事件。防止优先级反转。
> (4) latency和throughput同样重要，不是逻辑简单的IO bound或CPU bound程序。换言之，程序要有相当的计算量。
> (5) 利用异步操作。比如logging。无论往磁盘写log file，还是往log server发送消息都不应该阻塞critical path。
> (6) 能scale up。一个好的多线程程序应该能享受增加CPU数目带来的好处，目前主流是8核，很快就会用到16核的机器了。
> (7) 具有可预测的性能。随着负载增加，性能缓慢下降，超过某个临界点之后会急速下降。线程数目一般不随负载变化。
> (8) 多线程能有效地划分责任与功能，让每个线程的逻辑比较简单，任务单一，便于编码。而不是把所有逻辑都塞到一个event loop里，不同类别的事件之间相互影响。
> 

https://blog.csdn.net/Solstice/article/details/5334243






&emsp;
&emsp;
&emsp; 
# 参考文献
1. [理解 shared_ptr实现copy-on-write（COW）](https://blog.csdn.net/zhangxiao93/article/details/52792888)